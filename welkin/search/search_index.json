{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":""},{"location":"#innovate-at-speed-in-regulated-industries","title":"Innovate at speed in regulated industries","text":"<p>Welcome to Welkin, the Kubernetes platform for software critical to our society!</p> <p>Welkin enables organizations across Europe to accelerate innovation through open source cloud-native technology, while ensuring security and regulatory compliance.</p>        For application developers             Learn how Welkin helps you deploy and observe your application in production.      Get started        For CISOs and DPOs             Learn how Welkin helps you comply with European regulations, such as NIS2 and GDPR.      Help me comply        For platform administrators             Learn how to set up Welkin on any cloud provider or on-prem.      Set up Welkin"},{"location":"#what-is-welkin","title":"What is Welkin?","text":""},{"location":"#benefits-of-welkin","title":"Benefits of Welkin","text":"The platform you would build yourself               Built with CNCF projects,         public Architectural Decision Records,         as well as great documentation for application developers.             Loved\u2764\ufe0f by CISOs\ud83d\udc6e and DPOs\ud83e\uddd1\u200d\u2696\ufe0f               Built around controls to achieve EU regulatory compliance with:         GDPR,         ISO 27001,         NIS2 (BSI IT-Grundschutz).             Cloud agnostic, running in production in 10+ clouds               Runs on many EU clouds or on-prem."},{"location":"#welkin-is-trusted-by-industry-leaders","title":"Welkin is trusted by industry leaders","text":""},{"location":"#power-member-of-the-cloud-native-community","title":"Power member of the cloud native community","text":"<ul> <li>          Maintained by Elastisys, proud CNCF silver member     </li> <li>          Runs in production hosting critical applications     </li> <li>          Our platform is a CNCF Certified Kubernetes\u00ae Distribution     </li> </ul>"},{"location":"#commercial-offering","title":"Commercial offering","text":"Welkin Enterprise             Leverage Welkin on-prem with implementation and continuous support.      Go to Welkin Enterprise        Welkin Managed             A secure and fully managed Kubernetes platform for organizations that build and operate applications critical to our society.      Go to Welkin Managed        Consulting             Extend your team with our cloud native experts. Develop and deploy apps faster and with more confidence in a DevSecOps fashion.      Go to Consulting        Training             Level up your team's skills with our wide range of courses, both tailor-made and official Kubernetes ones from the Linux Foundation.      Go to Training"},{"location":"architecture/","title":"Architecture","text":"<p>Below we present the architecture of Welkin, using the C4 model.</p> <p>For the nitty-gritty details, see Architectural Decision Records.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-1-system-context","title":"Level 1: System Context","text":"<p>Let us start with the system context.</p> <p></p> <p>Compliance imposes restrictions on all levels of the tech stack. Your compliance focus should mostly lie on your application. Welkin ensures that the platform hosting your application is compliant. Finally, you need the whole software stack on a hardware that is managed in a compliant way, either via an ISO 27001-certified Infrastructure Provider or using on-prem hardware.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-2-clusters","title":"Level 2: Clusters","text":"<p>Most regulations require logging to a tamper-proof environment. This is usually interpreted as an attacker gaining access to your application should not be able to delete logs showing their attack and the harm caused by their attack.</p> <p>To achieve this, Welkin is implemented as two Kubernetes Clusters</p> <ul> <li>A Workload Cluster, which hosts your application, and</li> <li>A Management Cluster, which hosts services for monitoring, logging and vulnerability management.</li> </ul> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-individual-components","title":"Level 3: Individual Components","text":"<p>Click on the diagram below to see the nuts-and-bolts of Welkin.</p> <p></p> <p>Note</p> <p>Due to technical limitations, some compliance-related components still need to run in the Workload Cluster. These are visible when inspecting the Workload Cluster, for example, via the Kubernetes API. Currently, these components are:</p> <ul> <li>Falco, for intrusion detection;</li> <li>Prometheus, for collecting metrics;</li> <li>Fluentd, for collecting logs;</li> <li>OpenPolicyAgent, for enforcing Kubernetes API policies.</li> </ul> <p>Note that, the logs, metrics and alerts produced by these components are immediately pushed into the tamper-proof logging environment, hence this technical limitation does not weaken compliance.</p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-authentication","title":"Level 3: Authentication","text":"<p>Click on the diagram below to see the nuts-and-bolts of Welkin authentication.</p> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-backup","title":"Level 3: Backup","text":"<p>Click on the diagram below to see the nuts-and-bolts of Welkin backup.</p> <p></p>","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-metrics-and-metrics-based-alerting","title":"Level 3: Metrics and Metrics-based Alerting","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-logs-and-log-based-alerting","title":"Level 3: Logs and Log-based Alerting","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"architecture/#level-3-access-control","title":"Level 3: Access Control","text":"","tags":["MSBFS 2020:7 2 kap. 4 \u00a7","NIST SP 800-171 3.1.11","NIST SP 800-171 3.12.4"]},{"location":"glossary/","title":"Glossary","text":"<p>There are only two hard things in Computer Science: cache invalidation and naming things.</p> <p>\u2014 Phil Karlton</p> <p>This page introduces terminology used in the Welkin project. We borrow terminology from:</p> <ul> <li>Kubernetes Glossary;</li> <li>The Cluster API Book: Concepts.</li> </ul> <p>You may want to familiarize yourself with that terminology first.</p> <p>When naming things, we stick to Inclusive Naming.</p> <p>Please capitalize these terms, i.e., treat them as proper nouns.</p>"},{"location":"glossary/#air-gapped-network","title":"Air-gapped Network","text":"<p>From Wikipedia:</p> <p>An [air-gapped network] is a network security measure employed on one or more computers to ensure that a secure computer network is physically isolated from unsecured networks, such as the public Internet or an unsecured local area network. It means a computer or network has no network interface controllers connected to other networks, with a physical or conceptual air gap, analogous to the air gap used in plumbing to maintain water quality.</p> <p>Usage notes:</p> <ul> <li>Please avoid \"air-gapped environment\" to avoid confusion with Environment.</li> <li>Please avoid synonymous expressions, like \"disconnected network\" or \"offline environment\".</li> </ul> <p>See also:</p> <ul> <li>Air gap (networking) on Wikipedia</li> </ul>"},{"location":"glossary/#application-developer","title":"Application Developer","text":"<p>A person who writes an application that runs in a Kubernetes Cluster.</p> <p>Usage notes:</p> <ul> <li>It's okay to use \"developer\", if it's clear from the context that we refer to an Application Developer.</li> <li>If you need more precision, use:<ul> <li>\"Application Developers who are Grafana administrators\" (see Grafana Roles)</li> <li>\"Application Developers who are Harbor system administrators\" (see Harbor Managing Users)</li> <li>\"Application Developers who are Kubernetes admins\" (see Kubernetes user-facing roles)</li> <li>\"Application Developers with Kubernetes edit permissions\" (see Kubernetes user-facing roles)</li> </ul> </li> </ul> <ul> <li>Do NOT use \"Super Application Developer\", \"user-admin\", \"user-view\", \"app dev\", \"dev\", etc.</li> </ul> <p>See also:</p> <ul> <li>Application Developer on Kubernetes Glossary</li> <li>Certified Kubernetes Application Developer (CKAD)</li> </ul>"},{"location":"glossary/#apps-layer-or-welkin-layer","title":"Apps layer (or Welkin layer)","text":"<p>Denotes the Welkin components installed on top of a Kubernetes Cluster.</p> <p>Usage notes:</p> <ul> <li>This term is likely to be known and understood only by Platform Administrators and Contributors. Use only when addressing these two audiences.</li> </ul> <p>See also:</p> <ul> <li>Architecture Diagram Level 3: Individual Components</li> <li>Apps layer source code</li> </ul>"},{"location":"glossary/#cluster","title":"Cluster","text":"<p>Can refer to a Kubernetes Cluster, a PostgreSQL Cluster, a Redis Cluster, a RabbitMQ Cluster, an OpenSearch Cluster, etc.</p> <p>Usage notes:</p> <ul> <li>If it's not clear from the context what kind of Cluster you refer to, please spell it out. E.g., \"The PostgreSQL Cluster runs inside the Workload Cluster.\" instead of \"The Cluster runs inside the Workload Cluster.\"</li> </ul> <p>See also:</p> <ul> <li>Architecture Diagram Level 2: Clusters</li> <li>Cluster on Kubernetes Glossary</li> <li>PostgreSQL Database Cluster</li> <li>Redis Cluster</li> <li>RabbitMQ Clustering Guide</li> </ul>"},{"location":"glossary/#contributor","title":"Contributor","text":"<p>Someone who makes Welkin better by providing code, documentation, feedback. Contributors make their work visible by raising issues and creating pull requests.</p> <p>See also:</p> <ul> <li>Contributor Guide</li> </ul>"},{"location":"glossary/#critical-entity","title":"Critical Entity","text":"<p>To quote the EU Critical Entities Resilience (CER) Directive:</p> <p>Critical entities, as providers of essential services, play an indispensable role in the maintenance of vital societal functions or economic activities in the internal market in an increasingly interdependent Union economy.</p> <p>In particular, they all need to take various measures related to physical and staff security.</p> <p>However, there is no single clear definition for Critical Entities. Instead, EU Member States must implement a process for identifying critical entities based on categories of entities published in EU CER Directive.</p> <p>All entities identified as critical under CER are considered essential entities under the EU NIS2 Directive.</p> <p>See also:</p> <ul> <li>EU Critical Entities Resilience (CER) Directive: Categories of Entities</li> </ul>"},{"location":"glossary/#customer","title":"Customer","text":"<p>Someone who benefits from Welkin via a commercial agreement.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Customer\" to refer figuratively to Application Developer.   Although we are big fans of a customer-driven mindset, there are several way to deliver Welkin commercially. Hence, this usage of the word \"Customer\" is confusing.</li> <li>Do NOT use \"Customer\" to refer figuratively to End User.   Although we are big fans of a customer-driven mindset, there are several way to deliver Welkin commercially. Hence, this usage of the word \"Customer\" is confusing.</li> <li>Do NOT use \"Data Controller\", \"Data Processor\" or \"Data Sub-processor\". Determining which entity fulfills these GDPR concepts is usually done via a Data Protection Agreement (DPA). See EDPB Guidelines 07/2020 on the concepts of controller and processor in the GDPR.</li> </ul> <p>See also:</p> <ul> <li>Customer on Wikipedia.</li> </ul>"},{"location":"glossary/#end-user","title":"End User","text":"<p>Ultimate user of the Application deployed on top of Kubernetes.</p> <p>Usage notes:</p> <ul> <li>Spell \"End User\" when used as noun, \"end-user\" when used as adjective. E.g., \"good end-user experience\" versus \"good experience to the End User\".</li> <li>Do NOT use \"Application User\" to refer to the End User.</li> <li>Platform Services, like Grafana, Harbor and OpenSearch, are meant for Application Developers and not End Users.</li> </ul> <p>See also:</p> <ul> <li>End User on Wikipedia</li> </ul>"},{"location":"glossary/#environment","title":"Environment","text":"<p>One instance of a Welkin Deployment. One Environment is composed of two Kubernetes Clusters, the Management Cluster and Workload Cluster.</p> <p>Usage notes:</p> <ul> <li>Make sure to distinguish between Environment and Cluster.</li> </ul>"},{"location":"glossary/#essential-entity","title":"Essential Entity","text":"<p>Essential Entities are organizations which are considered to provide essential services to society and have obligations according to the EU NIS2 Directive. In particular, they need to take certain measures related to information security and cybersecurity.</p> <p>There is no clear definition for Essential Entities. Instead, EU Member States must implement a process for identifying essential entities based on a list of sectors of high criticality published in NIS2.</p> <p>This process is currently under development in most EU Member States. As an example on how this process could look like, please refer to the NIS-era MSBFS 2024:4 rule.</p> <p>Usage notes:</p> <ul> <li>The EU NIS2 Directive also introduces \"important entities\". These organizations have somewhat lower obligations under NIS2 and are subject to lower maximum fines.</li> </ul> <p>See also:</p> <ul> <li>EU NIS2 Directive: Sectors of High Criticality</li> <li>Swedish MSBFS 2024:4 Rules on identification of providers of essential services</li> </ul>"},{"location":"glossary/#kubernetes-cluster","title":"Kubernetes Cluster","text":"<p>A set of worker machines, called Nodes, that run containerized applications. Every Cluster has at least one worker Node.</p> <p>Usage notes:</p> <ul> <li>Prefer Workload Cluster or Management Cluster to avoid confusion.</li> </ul> <p>See also:</p> <ul> <li>Architecture Diagram Level 2: Clusters</li> <li>Cluster on Kubernetes Glossary</li> </ul>"},{"location":"glossary/#identity-provider","title":"Identity Provider","text":"<p>An Identity Provider (IdP) is a system that offers user authentication as a service. Examples include:</p> <ul> <li>Keycloak</li> <li>Microsoft Entra ID previously known as Azure Active Directory</li> <li>Google Identity</li> <li>JumpCloud</li> </ul> <p>Usage notes:</p> <ul> <li>Do NOT use \"Authentication Provider\"</li> <li>Dex is a \"Federated OpenID Connect Provider\". Hence, it is okay to call it a \"Federated Identity Provider\".</li> </ul> <p>See also:</p> <ul> <li>Identity provider on Wikipedia</li> </ul>"},{"location":"glossary/#infrastructure-provider","title":"Infrastructure Provider","text":"<p>A supplier of Virtual or Bare-metal Machines, networks, load balancers, block storage and object storage.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Data Processor\" or \"Data Sub-processor\". Determining which entity fulfills these GDPR concepts is usually done via a Data Protection Agreement (DPA). See EDPB Guidelines 07/2020 on the concepts of controller and processor in the GDPR.</li> <li>Do NOT use \"Cloud Provider\", as this is easily confused with \"Platform-as-a-Service Cloud Provider\".</li> </ul> <p>See also:</p> <ul> <li>Architecture Diagram Level 1: System Context</li> <li>Infrastructure provider on The Cluster API Book</li> </ul>"},{"location":"glossary/#management-cluster","title":"Management Cluster","text":"<p>A Kubernetes Cluster hosting some platform components.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Service Cluster\". That terms is poorly recognized and hereby deprecated.</li> <li><code>SC</code> and <code>sc</code> may be used to preserve backwards compatibility. Acceptable usage includes code and command-line tools. Unacceptable usage include documentation.</li> </ul> <p>See also:</p> <ul> <li>Management Cluster on The Cluster API Book</li> <li>Architecture Diagram Level 2: Clusters</li> </ul>"},{"location":"glossary/#maintainer","title":"Maintainer","text":"<p>\"Those contributors who lead an open source project.\" Elastisys is Maintainer of Welkin.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Creators\" nor \"Community Leaders\".</li> </ul> <p>See also:</p> <ul> <li>Open source Maintainers on Linux Foundation</li> </ul>"},{"location":"glossary/#personal-data-controller","title":"Personal Data Controller","text":"<p>Defined in Art. 4 GDPR as:</p> <p>the natural or legal person, public authority, agency or other body which, alone or jointly with others, determines the purposes and means of the processing of personal data; where the purposes and means of such processing are determined by Union or Member State law, the controller or the specific criteria for its nomination may be provided for by Union or Member State law;</p> <p>In brief, this is the organization which decides or influences what goes in the privacy policy.</p> <p>Usage notes:</p> <ul> <li>\"Controller\" can also refer to the Controller pattern in Kubernetes. Only use \"controller\" (without \"personal data\" or \"Kubernetes\") if the reader can understand from the context which one you refer to.</li> </ul> <p>See also:</p> <ul> <li>Art. 4 GDPR</li> </ul>"},{"location":"glossary/#personal-data-processor","title":"Personal Data Processor","text":"<p>Defined in Art. 4 GDPR as:</p> <p>a natural or legal person, public authority, agency or other body which processes personal data on behalf of the controller;</p> <p>In brief, this is the organization that receives instructions from the data controller and -- with few exceptions -- can only process personal data as instructed.</p> <p>Usage notes:</p> <ul> <li>The GDPR does not define the concept of \"sub-processor\".   However, the European Data Protection Board (EDPB) encourages using the term \"sub-processor\" to denote an organization which acts under the instructions of the processor.</li> </ul> <p>See also:</p> <ul> <li>Art. 4 GDPR</li> <li>What is a sub-processor? Data Protection Guide for Small Business by the EDPB</li> </ul>"},{"location":"glossary/#platform-administrator","title":"Platform Administrator","text":"<p>The people who operate Welkin and Additional Platform Services.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Operator\" to refer to \"Platform Administrator\". Such usage is confusing due to the Operator pattern.</li> <li>It's okay to use \"admin\" or \"administrator\", if it's clear from the context that we refer to the Platform Administrator.</li> </ul> <p>See also:</p> <ul> <li>Certified Kubernetes Administrator (CKA)</li> </ul>"},{"location":"glossary/#service-endpoint","title":"Service Endpoint","text":"<p>Interface exposed via the network for accessing Welkin functionality. Endpoints include Harbor, OpenSearch, Grafana, Dex and the Workload Cluster Kubernetes API.</p> <p>Usage notes:</p> <ul> <li>Do NOT use \"Web portals\" or \"Service Access Points\".</li> </ul> <p>See also:</p> <ul> <li>API on Wikipedia</li> </ul>"},{"location":"glossary/#workload-cluster","title":"Workload Cluster","text":"<p>A Kubernetes Cluster hosting the Application which is used by the End User.</p> <p>See also:</p> <ul> <li>Workload Cluster on The Cluster API Book</li> <li>Architecture Diagram Level 2: Clusters</li> </ul>"},{"location":"mission-and-vision/","title":"Mission and Vision","text":"","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#purpose","title":"Purpose","text":"<p>The Welkin Mission and Vision statements support Elastisys' Mission and Vision. They provide a \"northern star\" to orient development.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#statements","title":"Statements","text":"<ul> <li> <p>Mission: The best application developer and administrator experience in and of a cloud-native platform that cost-effectively ensures security and regulatory compliance.</p> </li> <li> <p>Vision: Securing Europe\u2019s digital future for services critical to society.</p> </li> </ul>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#what-does-the-vision-and-mission-mean","title":"What does the vision and mission mean?","text":"<p>Below we explain what we mean with each word in the mission and vision.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#best-application-developer-experience","title":"Best application developer experience","text":"<ul> <li>Closes the DevOps loop:<ul> <li>It should be easy for application developer to deploy their code.</li> <li>It should be easy for application developers to observe their code via the three pillars of observability: logs, metrics and traces.</li> </ul> </li> <li>As self-service as possible:<ul> <li>Application developers should be able to reach their goals with as little platform administrator involvement as possible.</li> <li>This should not be interpreted as \u201cclick to deploy\u201d, rather \"as a platform administrator, I can point application developers to documentation to quickly achieve their goals\".</li> <li>\"Click to deploy\" can be part of the solution, but it doesn't have to be.</li> </ul> </li> <li>Good documentation:<ul> <li>Documentation should help application developers throughout their journey of getting started and making the best use of the platform.</li> <li>Documentation should be searchable, for example, application developers should be able to copy-paste an error message to quickly recover from it.</li> </ul> </li> <li>Makes security and compliance easy:<ul> <li>The platform should make it hard for application developers to do the wrong thing by employing guardrails and secure defaults.</li> </ul> </li> </ul>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#best-administrator-experience","title":"Best administrator experience","text":"<ul> <li>Good documentation:<ul> <li>It should be easy for the platform administrator to get an overview of the platform, its components and the interaction between its components.</li> <li>The platform administrator should be provided with:<ul> <li>checklists, for rare and hard-to-automate tasks;</li> <li>runbooks;</li> <li>scripts.</li> </ul> </li> </ul> </li> <li>Good scalability:<ul> <li>A team of platform administrators should be able to administer many environments.</li> <li>Cognitive load on platform administrators should be limited, for example, by:<ul> <li>limiting the scope (tech stack) of the platform;</li> <li>limiting divergence between environments.</li> </ul> </li> </ul> </li> <li>Minimise number of tickets from application developers:<ul> <li>Ideally, the platform administrator should be able to point application developers to documentation which enables the latter to reach their goals by themselves.</li> </ul> </li> <li>Minimise amount of manual actions, \"thinking\" and troubleshooting<ul> <li>The platform should perform frequent actions by itself, such as related to patching, updates and capacity management.</li> <li>The platform should obey configuration-by-code. Note that, 100% GitOps -- i.e., performing all operations via <code>git commit</code> -- is a non-goal.</li> </ul> </li> <li>Minimise likelihood and impact of incidents:<ul> <li>The platform should alert when an environment needs human attention.</li> </ul> </li> <li>Minimise number of alerts, especially off-hours ones:<ul> <li>All alerts should be actionable.</li> <li>The platform should reduce the priority of alerts via resilience and automation.</li> </ul> </li> <li>Minimize time spent on billing:<ul> <li>It should be easy to understand how to bill for the services provided by the platform.</li> </ul> </li> <li>Minimize time spent on verifying environment specification:<ul> <li>It should be easy for platform administrators to retrieve and verify the configuration of a large number of environments to ensure that important details, such as backup retention, log retention, are configured as expected by application developers.</li> </ul> </li> </ul>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#security","title":"Security","text":"<p>Welkin should be built and should make it easy for the application to be built with the following security principles in mind.</p> <ul> <li>Security-By-Design:<ul> <li>The entire system and its individual components are designed from the ground up with security in mind.</li> <li>Deliberate attacks and unauthorized actions are explicitly considered, and the impact of security incidents is minimized through system design.</li> </ul> </li> <li>Security-By-Default:<ul> <li>Security is enabled by default, requiring no additional configuration to ensure a secure state.</li> <li>Security features may be disabled only if:<ul> <li>their benefits strongly outweighs the limitations they impose;</li> <li>there are sufficient compensatory measures;</li> <li>the residual risk is clearly understood and documented.</li> </ul> </li> </ul> </li> <li>Minimal-Need-To-Know Principle:<ul> <li>Each component and user is granted only the permissions necessary to perform a specific action.</li> </ul> </li> <li>Defence-In-Depth Principle:<ul> <li>Security risks are not addressed by individual protective measures alone but are mitigated through the implementation of layered, multi-level, and complementary security measures.</li> </ul> </li> <li>Redundancy Principle:<ul> <li>The overall system is designed in such a way that the failure of individual components does not compromise security-relevant functions.</li> <li>The system design reduces both the likelihood and impact of issues arising from unrestricted resource consumption, such as memory or network bandwidth, which could lead to resource exhaustion or denial-of-service (DoS) attacks.</li> </ul> </li> <li>Free From Known Exploitable Vulnerabilities:<ul> <li>Exploitable vulnerability means a vulnerability that has the potential to be effectively used by an adversary under practical operational conditions. This definition is extracted from the EU Cyberresilience Act (CRA).</li> <li>The platform makes it easy to detect vulnerabilities and apply security patches, both at the level of the application and the platform itself.</li> </ul> </li> </ul> <p>The platform should support an organization's information security certification efforts, e.g., with ISO 27001:2022.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#regulatory-compliance","title":"Regulatory compliance","text":"<p>The platform should help an organization comply with various regulatory requirements, such as:</p> <ul> <li>EU GDPR</li> <li>EU NIS2</li> <li>EU Critical Entities Resilience Directive (CER)</li> <li>EU Medical Device Regulation (MDR)</li> <li>Swedish Patientdatalagen (PDL)</li> </ul>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#cost-effectively","title":"Cost effectively","text":"<ul> <li>The platform should leave headroom, as required for stability and security, but not waste resources.</li> <li>Application developers should be able to understand what they get for every CPU, GB and Gbps.</li> <li>The platform should help close the FinOps loop.</li> </ul>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#supporting-pillars","title":"Supporting Pillars","text":"<p>In order to support the mission and vision, Welkin needs the following supporting pillars.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#quality-assurance","title":"Quality Assurance","text":"<p>It should be easy to release a new version of Welkin which meets the target quality standards.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"mission-and-vision/#contributor-experience","title":"Contributor Experience","text":"<p>It should be easy to contribute improvements and new features to Welkin.</p>","tags":["ISO 27001 Annex A 5.1 Policies for Information Security","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"quality-criteria/","title":"Quality Criteria","text":"<p>Welkin provides a stable and secure platform for containerized applications. To achieve this, quality assurance is an integral part of development. When we say \"quality\", we really refer to the following quality criteria.</p> <p>Feature can be delivered ...</p> <ul> <li> <p>... at scale</p> <ul> <li>Feature has good developer-facing documentation. The documentation includes:<ul> <li>the happy path;</li> <li>a running example based on the user demo, if applicable;</li> <li>limitations, if applicable;</li> <li>further reading.</li> </ul> </li> <li>Feature is self-serviced</li> <li>Feature is well-understood and aligned in marketing, sales, product and operations</li> <li>Feature is clearly covered by ToS</li> <li>Feature is implemented using a stable upstream API</li> <li>Feature is used by at least 2 Application Developers</li> <li>Feature generates a manageable number of service tickets, whether questions or change orders</li> <li>Feature has well understood packaging and pricing</li> <li>Feature can be billed easily</li> <li>Feature integrates well with application developer observability (alerting, logging, metrics)</li> <li>Feature integrates well with application developer authentication</li> </ul> </li> <li> <p>... without ruining the platform administrator's life</p> <ul> <li>At least 2 admins have required training</li> <li>Feature has good admin-facing documentation (2nd day ops, all processes in place and documented, etc.)</li> <li>Feature triggers a manageable number of P1 alerts</li> <li>Feature triggers a manageable number of P2 alerts</li> <li>Feature has good upstream support</li> <li>All information security risks related to feature have been identified</li> <li>(In case of a new supplier) Supplier collaborates directly with Elastisys admins</li> <li>Feature is covered by QA</li> <li>Feature is sufficiently redundant to be able to operate in degraded state upon faults</li> <li>Feature integrates well with Ops observability (alerting, logging, metrics)</li> </ul> </li> <li> <p>... without compromising our security posture</p> <ul> <li>Feature has good and well-understood access control towards Application Developer</li> <li>Feature does not expose platform to additional risk (needs escalated privileges that were not analyzed, etc.)</li> <li>Feature has good and well-understood security patching</li> <li>Feature has good and well-understood upgrades</li> <li>Feature has good and well-understood business continuity, i.e., high availability or self-healing</li> <li>Feature has good and well-understood disaster recovery</li> <li>Feature does not impair ability to upgrade underlying infrastructure and base OS</li> <li>(In case of a new supplier) Supplier provides sufficient security for our needs</li> <li>Feature has good and well understood way of measuring SLA fulfillment</li> </ul> </li> </ul> <p>These criteria should be taken as a direction, not a \"task list\". For some features, some of these criteria won't apply. For other features, we might accept that some of these criteria cannot be fully satisfied. It is the role of our Quality Assurance Engineer (QAE) to decide how to apply these criteria to each feature.</p>","tags":["ISO 27001 Annex A 8.29 Security Testing in Development and Acceptance"]},{"location":"quality-criteria/#quality-assurance-qa","title":"Quality Assurance (QA)","text":"<p>Each release of Welkin is quality assured. Some, but not all, of the quality assurance steps are public. Please find them linked below:</p> <ul> <li>Welkin Apps Release Checklist</li> <li>Welkin Kubespray Release Checklist</li> </ul>","tags":["ISO 27001 Annex A 8.29 Security Testing in Development and Acceptance"]},{"location":"request-a-feature/","title":"Request a Feature","text":"<p>For Welkin Managed Customers</p> <p>You can request a new feature by filing a service ticket under \"Request a Feature\".</p> <p>For Welkin Enterprise Customers</p> <p>You can request a new feature by filing a service ticket under \"Request a Feature\".</p>"},{"location":"adr/","title":"Architectural Decision Log","text":"","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.14.1.1 \"Information security requirements analysis and specification\"</li> <li>A.14.2.4 \"Restrictions on Changes to Software Packages\"</li> </ul>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#what-are-architectural-decisions","title":"What are architectural decisions?","text":"<p>Architectural decisions are high-level technical decisions that affect most stakeholders, in particular Welkin developers, administrators and users. A non-exhaustive list of architectural decisions is as follows:</p> <ul> <li>adding or removing tools;</li> <li>adding or removing components;</li> <li>changing what component talks to what other component;</li> <li>major (in the SemVer sense) component upgrades.</li> </ul> <p>Architectural decisions should be taken as directions to follow for future development and not issues to be fixed immediately.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#what-triggers-an-architectural-decision","title":"What triggers an architectural decision?","text":"<p>An architectural decision generally starts with one of the following:</p> <ul> <li>A new features was requested by product management.</li> <li>An improvement was requested by engineering management.</li> <li>A new risk was discovered, usually by the architect, but also by any stakeholder.</li> <li>A new technology was discovered, that may help with a new feature, an improvement or to mitigate a risk.</li> </ul>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#how-are-architectural-decisions-captured","title":"How are architectural decisions captured?","text":"<p>Architectural decisions are captured via Architectural Decision Records or the tech radar. Both are stored in Git, hence a decision log is also captured as part of the Git commit messages.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#how-are-architectural-decisions-taken","title":"How are architectural decisions taken?","text":"<p>Architectural decisions need to mitigate the following information security risks:</p> <ul> <li>a component might not fulfill advertised expectations;</li> <li>a component might be abandoned;</li> <li>a component might change direction and deviate from expectations;</li> <li>a component might require a lot of (initial or ongoing) training;</li> <li>a component might not take security seriously;</li> <li>a component might change its license, prohibiting its reuse or making its use expensive.</li> </ul> <p>The Welkin architect is overall responsible for this risk.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#how-are-these-risks-mitigated","title":"How are these risks mitigated?","text":"<p>Before taking in any new component to Welkin, we investigate and evaluate them. We prefer components that are:</p> <ul> <li>community-driven open-source projects, to reduce the risk of a component becoming abandoned, changing its license or changing direction in the interest of a single entity; as far as possible, we choose CNCF projects (preferably graduated ones) or projects which are governed by at least 3 different entities;</li> <li>projects with a good security track record, to avoid unexpected security vulnerabilities or delays in fixing security vulnerabilities; as far as possible, we choose projects with a clear security disclosure process and a clear security announcement process;</li> <li>projects that are popular, both from a usage and contribution perspective; as far as possible, we choose projects featuring well-known users and many Maintainers;</li> <li>projects that rely on technologies that our team is already trained on, to reduce the risk of requiring a lot of (initial or ongoing) training; as far as possible, we choose projects that overlap with the projects already on our tech radar;</li> <li>projects that are simple to install and manage, to reduce required training and burden on administrators.</li> </ul> <p>Often, it is not possible to fulfill the above criteria. In that case, we take the following mitigations:</p> <ul> <li>Architectural Decision Records include recommendations on training to be taken by administrators.</li> <li>Closed-source or \"as-a-Service\" alternatives are used, if they are easy to replace thanks to broad API compatibility or standardization.</li> </ul> <p>These mitigations may be relaxed for components that are part of alpha or beta features, as these features -- and required components -- can be removed at our discretion.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#adrs","title":"ADRs","text":"<p>This log lists the architectural decisions for Welkin.</p> <ul> <li>ADR-0000 - Use Markdown Architectural Decision Records</li> <li>ADR-0001 - Use Rook for Storage Orchestrator</li> <li>ADR-0002 - Use Kubespray for Cluster Life-cycle</li> <li>ADR-0003 - [Superseded by ADR-0019] Push Metrics via InfluxDB</li> <li>ADR-0004 - Plan for Usage without Wrapper Scripts</li> <li>ADR-0005 - Use Individual SSH Keys</li> <li>ADR-0006 - Use Standard Kubeconfig Mechanisms</li> <li>ADR-0007 - Make Monitoring Forwarders Storage Independent</li> <li>ADR-0008 - Use HostNetwork or LoadBalancer for Ingress</li> <li>ADR-0009 - Use ClusterIssuers for Let's Encrypt</li> <li>ADR-0010 - Run managed services in Workload Cluster</li> <li>ADR-0011 - [Superseded by ADR-0046] Let upstream projects handle CRDs</li> <li>ADR-0012 - [Superseded by ADR-0017] Do not persist Dex</li> <li>ADR-0013 - Configure Alerts in On-call Management Tool (e.g., Opsgenie)</li> <li>ADR-0014 - Use bats for testing bash wrappers</li> <li>ADR-0015 - We believe in community-driven open source</li> <li>ADR-0016 - [Superseded by ADR-0040] gid=0 is okay, but not by default</li> <li>ADR-0017 - Persist Dex</li> <li>ADR-0018 - Use Probe to Measure Uptime of Internal Welkin Services</li> <li>ADR-0019 - Push Metrics via Thanos</li> <li>ADR-0020 - Filter by Cluster label then data source</li> <li>ADR-0021 - Default to TLS for performance-insensitive additional services</li> <li>ADR-0022 - Use Dedicated Nodes for Additional Services</li> <li>ADR-0023 - [Superseded by ADR-0056] Only allow Ingress Configuration Snippet Annotations after Proper Risk Acceptance</li> <li>ADR-0024 - Allow a Harbor robot account that can create other robot accounts with full privileges</li> <li>ADR-0025 - Use local-volume-provisioner for Managed Services that requires high-speed disks</li> <li>ADR-0026 - Use <code>environment-name</code> as the default root of Hierarchical Namespace Controller (HNC)</li> <li>ADR-0027 - PostgreSQL - Enable external replication</li> <li>ADR-0028 - Harder Pod eviction when Nodes are going OOM</li> <li>ADR-0029 - Expose Jaeger UI in WC</li> <li>ADR-0030 - Run ArgoCD on the Elastisys Nodes</li> <li>ADR-0031 - Run csi-cinder-controllerplugin on the Elastisys Nodes</li> <li>ADR-0032 - Boot disk size on Nodes</li> <li>ADR-0033 - Run Cluster API controllers on Management Cluster</li> <li>ADR-0034 - How to run multiple AMS packages of the same type in the same environment</li> <li>ADR-0035 - Run Tekton on Management Cluster</li> <li>ADR-0036 - Run Ingress-NGINX as a DaemonSet</li> <li>ADR-0037 - Enforce TTL on Jobs</li> <li>ADR-0038 - Replace the starboard-operator with the trivy-operator</li> <li>ADR-0039 - Application developer privilege elevation</li> <li>ADR-0040 - Allow running containers with primary and supplementary group id 0</li> <li>ADR-0041 - Rely on Infrastructure Provider for encryption-at-rest</li> <li>ADR-0042 - ArgoCD with dynamic HNC namespaces</li> <li>ADR-0043 - Rclone and Encryption adheres Cryptography Policy</li> <li>ADR-0044 - ArgoCD is not allowed to manage its own namespace</li> <li>ADR-0045 - Use specialised prebuilt images</li> <li>ADR-0046 - Handle all CRDs with the standard Helm CRD management</li> <li>ADR-0047 - When to upgrade to new Kubernetes versions</li> <li>ADR-0048 - Access Management for Additional Managed Services (AMS-es)</li> <li>ADR-0049 - Running NGINX with Chroot Option</li> <li>ADR-0050 - Use Cluster Isolation to separate the application and its traces from its logs and metrics</li> <li>ADR-0051 - Open cert-manager Network Policies</li> <li>ADR-0052 - Azure Encryption-at-Rest for Object Storage and Block Storage</li> <li>ADR-0053 - Do not expose platform observability services to end-users</li> <li>ADR-0054 - Allow Application Developer write access to Endpoints and EndpointSlices after Proper Risk Acceptance</li> <li>ADR-0055 - [Superseded by ADR-0059]Welkin to consist of both public and private open source</li> <li>ADR-0056 - Only allow Ingress Snippet Annotations after Proper Risk Acceptance</li> <li>ADR-0057 - Do Not Use Managed Kubernetes Services</li> <li>ADR-0058 - Boot disk size on Nodes</li> <li>ADR-0059 - Welkin to Consist of Public Open Source Code and Proprietary Documentation</li> </ul> <p>For new ADRs, please use template.md as basis. More information on MADR is available at https://adr.github.io/madr/. General information about architectural decision records is available at https://adr.github.io/.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/#index-regeneration","title":"Index Regeneration","text":"<p>Pre-requisites:</p> <ul> <li>Install <code>npm</code></li> <li>Install <code>adr-log</code></li> <li>Install <code>make</code></li> </ul> <p>Run <code>make -C docs/adr</code>, then run <code>pre-commit run --all-files</code>.</p>","tags":["NIST SP 800-171 3.13.2","NIST SP 800-171 3.13.3","ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles"]},{"location":"adr/0000-use-markdown-architectural-decision-records/","title":"Use Markdown Architectural Decision Records","text":""},{"location":"adr/0000-use-markdown-architectural-decision-records/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to record architectural decisions made in this project. Which format and structure should these records follow?</p>"},{"location":"adr/0000-use-markdown-architectural-decision-records/#considered-options","title":"Considered Options","text":"<ul> <li>MADR 2.1.2 \u2013 The Markdown Architectural Decision Records</li> <li>Formless \u2013 No conventions for file format and structure</li> </ul>"},{"location":"adr/0000-use-markdown-architectural-decision-records/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"MADR 2.1.2\", because</p> <ul> <li>We need to start somewhere, and it's better to have some format than no format.</li> <li>MADR seems to be good enough for our current needs.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/","title":"Use Rook for Storage Orchestrator","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian Klein, Lars Larsson, Pradyumna Kashyap, Daniel Harr, Viktor Forsberg, Fredrik Liv</li> <li>Date: 2020-11-16</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Welkin has the vision to reduce the compliance burden on multiple clouds (\"Multi-cloud. Open source. Compliant.\"). Many of the Infrastructure Providers we target do not have a storage provider or do not have a storage provider that integrates with Kubernetes. How should we support PersistentVolumeClaims in such cases?</p>"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Storage Orchestrator needs to be popular and well maintained, so that developer can focus on adding value on top of Kubernetes Clusters.</li> <li>Storage Orchestrator needs to be easy to set up, easy to operate and battle-tested, so on-call administrators are not constantly woken up.</li> <li>Storage Orchestrator needs to have reasonable performance. (A local storage provider can deal with high-performance use-cases.)</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#considered-options","title":"Considered Options","text":"<ul> <li>Rook</li> <li>GlusterFS</li> <li>Longhorn</li> <li>NFS Storage Provider</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Rook\", because it is CNCF graduated, hence it is most likely to drive development and adoption long-term. Prady tested it and showed it was easy to use. It supports Ceph as a backend, making it battle-tested. It has reasonable performance.</p>"},{"location":"adr/0001-use-rook-storage-orchestrator/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We no longer need to worry about Infrastructure Provider without native storage.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to deprecate our NFS storage provider.</li> <li>Some manual steps are required to set up partitions for Rook. These will be automated when the burden justifies it.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0001-use-rook-storage-orchestrator/#longhorn","title":"Longhorn","text":"<ul> <li>Good, because it is a CNCF project.</li> <li>Good, because it is well integrated with Kubernetes.</li> <li>Bad, because it is not the most mature CNCF project in the storage class.</li> <li>Bad, because it was not easy to set up.</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#glusterfs","title":"GlusterFS","text":"<ul> <li>Good, because it is battle-tested.</li> <li>Bad, because it is not as well integrated with Kubernetes as other projects.</li> <li>Bad, because it is not a CNCF project (driven by Red Hat).</li> </ul>"},{"location":"adr/0001-use-rook-storage-orchestrator/#nfs-storage-provider","title":"NFS Storage Provider","text":"<ul> <li>Good, because we used it before and we have experience.</li> <li>Bad, because it is a non-redundant, snowflake, brittle solution.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/","title":"Use Kubespray for Cluster Life-cycle","text":"<ul> <li>Status: accepted</li> <li>Deciders: Lars, Johan, Cristian, Emil, Viktor, Geoff, Ewnetu, Fredrik (potentially others who attended the architecture meeting, but I can't remember)</li> <li>Date: 2020-11-17</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Welkin promises: \"Multi-cloud. Open source. Compliant\". So far, we delivered on our multi-cloud promise by using our in-house <code>ck8s-cluster</code> implementation. This strategy feels unsustainable for two reasons: First, we don't have the resources to catch up and keep up with open source projects in the Cluster life-cycle space. Second, we don't want to differentiate on how to set up vanilla Kubernetes Cluster, i.e., lower in the Kubernetes stack. Rather we want to differentiate on services on top of vanilla Kubernetes Clusters.</p>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to differentiate on top of vanilla Kubernetes Cluster.</li> <li>We want to be able to run Welkin on top of as many Infrastructure Providers as possible.</li> <li>We promise building on top of best-of-breeds open source projects.</li> <li>We want to reduce burden with developing and maintaining our in-house tooling for Cluster life-cycle management.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#considered-options","title":"Considered Options","text":"<ul> <li>Rancher</li> <li>kubeadm via in-house tools (ck8s-Cluster)</li> <li>Kubespray</li> <li>kops</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#decision-outcome","title":"Decision Outcome","text":"<p>We chose Kubespray, because it is best aligned with our interests, both feature- and roadmap-wise. It has a large community and is expected to be well maintained in the future. It uses kubeadm for domain knowledge on how to set up Kubernetes Clusters.</p>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We learn how to use a widely-used tool for Cluster lifecycle management.</li> <li>We support many Infrastructure Providers.</li> <li>We can differentiate on top of vanilla Kubernetes.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need training on Kubespray.</li> <li>We need to port our tooling and practices to Kubespray.</li> <li>We need to port <code>compliantkubernetes-apps</code> to work on Kubespray.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#rancher","title":"Rancher","text":"<ul> <li>Good, because it provides Cluster life-cycle management at scale.</li> <li>Bad, because it creates Clusters in an opinionated way, which is insufficiently flexible for our needs.</li> <li>Bad, because it is not a community project, hence entails long-term licensing uncertainty.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kubeadm-via-in-house-tool-ck8s-cluster","title":"kubeadm via in-house tool (ck8s-Cluster)","text":"<ul> <li>Good, because we know it and we built it.</li> <li>Good, because it works well for current use-cases.</li> <li>Bad, because it entails a lot of effort to develop and maintain.</li> <li>Bad, because it is lagging behind feature-wise with other Cluster life-cycle solutions.</li> </ul>"},{"location":"adr/0002-use-kubespray-for-cluster-lifecycle/#kops","title":"kops","text":"<ul> <li>Good, because it integrates well with the underlying Infrastructure Provider (e.g., AWS).</li> <li>Bad, because it supports fewer Infrastructure Providers than Kubespray.</li> </ul> <p>NOTE: In the future, we might want to support <code>compliantkubernetes-apps</code> on top of both kops and Kubespray, but this does not seem to bring value just now.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/","title":"[Superseded by ADR-0019] Push Metrics via InfluxDB","text":"<ul> <li>Status: superseded by ADR-0019</li> <li>Deciders: Johan, Cristian, Viktor, Emil, Olle, Fredrik</li> <li>Date: 2020-11-19</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We want to support workload multi-tenancy, i.e., one Management Cluster -- hosting the tamper-proof logging environment -- and multiple Workload Clusters. Currently, the Management Cluster exposes two end-points for Workload Clusters:</p> <ul> <li>Dex, for authentication;</li> <li>Elasticsearch, for pushing logs (append-only).</li> </ul> <p>Currently, the Management Cluster pulls metrics from the Workload Cluster. This makes it difficult to have multiple Workload Clusters connected to the same Management Cluster.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to support workload multi-tenancy.</li> <li>We want to untangle the life-cycle of the Management Cluster and Workload Cluster.</li> <li>The Management Cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the Workload Cluster.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#considered-options","title":"Considered Options","text":"<ol> <li>Management Cluster exposes InfluxDB; Workload Cluster pushes metrics into InfluxDB.</li> <li>Migrate from InfluxDB to Thanos</li> <li>Migrate from InfluxDB to Cortex</li> </ol>"},{"location":"adr/0003-push-metrics-via-influxdb/#decision-outcome","title":"Decision Outcome","text":"<p>We chose to push metrics from the Workload Cluster to the Management Cluster via InfluxDB, because it involves the least amount of effort and is sufficient for the current use-cases that we want to support. InfluxDB supports a writer role, which makes overwriting metrics difficult -- unfortunately, not impossible.</p>"},{"location":"adr/0003-push-metrics-via-influxdb/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>All of <code>*.$opsDomain</code> can point to the Management Cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup.</li> <li>Multiple Workload Clusters can push metrics to the Management Cluster, which paves the path to workload multi-tenancy.</li> <li>The Management Cluster can be set up first, followed by one-or-more Workload Clusters.</li> <li>Workload Clusters become more \"cattle\"-ish.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Existing Welkin Clusters will need some manual migration steps, in particular changing the <code>prometheus.$opsDomain</code> DNS entry.</li> <li>The Management Cluster exposes yet another endpoint, which should only be available to Workload Clusters and not the Internet. HTTP authentication (over HTTPS) feels sufficient for now, but we need a follow-up decision on how to add another layer of protection to these endpoints.</li> <li>The Workload Clusters will have to properly label their metrics.</li> <li>Although not easy, metrics can be overwritten from the Workload Cluster. We will improve on this when (a) demand for closing this risk increases, (b) we re-evaluate long-term metrics storage.</li> </ul>"},{"location":"adr/0003-push-metrics-via-influxdb/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":"<p>Both Thanos and Cortex seems worthy projects to replace InfluxDB. At the time of this writing, they were both having CNCF Incubating status. The two projects feature a healthy collaboration and are likely to merge in the future.</p> <p>However, right now, migrating away from InfluxDB feels like it adds more cost than benefits. We will reevaluate this decision when InfluxDB is no longer sufficient for our needs.</p>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/","title":"Plan for Usage without Wrapper Scripts","text":"<ul> <li>Status: accepted</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2020-11-24</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We frequently write wrapper scripts. They bring the following value:</p> <ol> <li>They bind together several tools and make them work together as a whole, e.g., <code>sops</code> and <code>kubectl</code>.</li> <li>They encode domain knowledge and standard operating procedures, e.g., how to add a Node, how a Cluster should look like, where to find configuration files.</li> <li>They enforce best practices, e.g., encrypt secrets consumed or produced by tools.</li> </ol> <p>Unfortunately, wrapper scripts can also bring disadvantages:</p> <ol> <li>They make usages that are deviating from the \"good way\" difficult.</li> <li>They risk adding opacity and raise the adoption barrier. People used to the underlying tools may find it difficult to follow how those tools are invoked.</li> <li>They add overhead when adding new features or supporting new use-cases.</li> <li>They raise the learning curve, i.e., newcomers need to learn the wrapper scripts in addition to the underlying tools. Completely abstracting away the underlying tools is unlikely, due to the Law of Leaky Abstractions.</li> </ol>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to make operations simple, predictable, resilient to human error and scalable.</li> <li>We want to have some predictability in how an environment is set up.</li> <li>We want to make Welkin flexible and agile.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#considered-options","title":"Considered Options","text":"<ul> <li>On one extreme, we can enforce wrapper scripts as the only way forward. This would require significant investment, as these scripts would need to be very powerful and well documented.</li> <li>On the other extreme, we completely \"ban\" wrapper scripts.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#decision-outcome","title":"Decision Outcome","text":"<p>We have chosen to keep wrapper scripts in general. However, they need to be written in a way that ensures that our artefacts (e.g., Terraform scripts, Ansible roles, Helmfiles and Helm Charts) are usable without wrapper scripts. Wrapper scripts should also be simple enough so they can be inspected and useful commands can be copy-pasted out. This ensures that said scripts do not need to be \"too\" powerful and \"too\" well documented, but at the same time they do bring the sought after value.</p> <p>This decision applies for new wrapper scripts. We will not rework old wrapper scripts.</p>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Platform Administrators can encode standard operating procedures and scale ways of working.</li> <li>Our professional services team can easily reuse artefacts for new use-cases, without significant development effort.</li> <li>Newcomers will (hopefully) find the right trade-off of barriers, depending on whether they are looking for flexibility or predictability.</li> </ul>"},{"location":"adr/0004-plan-for-usage-without-wrapper-scripts/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>There will be a constant temptation to do things outside wrapper scripts, which will complicated knowledge sharing, operations and support. When this becomes a significant issue, we will need to draft clear guidelines on what should belong in the wrapper scripts and what shouldn't.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/","title":"Use Individual SSH Keys","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Fredrik, Olle, Johan</li> <li>Date: 2021-01-28</li> </ul> <p>Technical Story:</p> <ul> <li>Do not fiddle with the SSH key</li> <li>Create a process of how we should move to use personal SSH keys</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently, we create per-Cluster SSH key pairs, which are shared among administrators. This is problematic from an information security perspective for a few reasons:</p> <ol> <li>It reduces the auditability of various actions, e.g., who SSH-ed into the Kubernetes control plane Nodes.</li> <li>It makes credential management challenging, e.g., when onboarding/offboarding administrators.</li> <li>It makes credential rotation challenging, e.g., the new SSH key pair needs to be transmitted to all administrators.</li> <li>It encourages storing the SSH key pair without password protection.</li> <li>It makes it difficult to store SSH key pairs on an exfiltration-proof medium, such as a YubiKey.</li> <li>It violates the Principle of Least Astonishment.</li> </ol>"},{"location":"adr/0005-use-individual-ssh-keys/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We need to stick to information security best-practices.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#considered-options","title":"Considered Options","text":"<ul> <li>Inject SSH keys via cloud-init.</li> <li>Manage SSH keys via an Ansible role.</li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#decision-outcome","title":"Decision Outcome","text":"<p>We will manage SSH keys via an Ansible role, since it allows rotating/adding/deleting keys without rebooting Nodes. Also, it caters to more environments, e.g., BYO-VMs and BYO-metal. The compliantkubernetes-kubespray project will make it easy to configure SSH keys.</p>"},{"location":"adr/0005-use-individual-ssh-keys/#bootstrapping","title":"Bootstrapping","text":"<p>The above decision raises a chicken-and-egg problem: Ansible needs SSH access to the Nodes, but the SSH access is managed via Ansible. This issue is solved as follows.</p> <p>For cloud deployments, all Terraform providers support injecting at least one public SSH key via cloud-init:</p> <ul> <li>AWS</li> <li>Exoscale</li> <li>GCP</li> <li>OpenStack</li> </ul> <p>The administrator who creates the Cluster bootstraps SSH access by providing their own public SSH key via cloud-init. Then, the Ansible role adds the public SSH keys of the other administrators.</p> <p>BYO-VM and BYO-metal deployments are handled similarly, except that the initial public SSH key is delivered by email/Slack to the VM/metal administrator.</p>"},{"location":"adr/0005-use-individual-ssh-keys/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<ul> <li> <p>Platform Administrators should devise procedures for onboarding and offboarding members of the on-call team, as well as rotating SSH keys.</p> </li> <li> <p>The public SSH keys of all on-call administrators could be stored in a repository in a single file with one key per line.   The comment of the key should clearly identify the owner.</p> </li> <li> <p>Platform Administrator logs (be it stand-alone documents, git or GitOps-like repositories) should clearly list the SSH keys and identities of the administrators configured for each environment.</p> </li> </ul>"},{"location":"adr/0005-use-individual-ssh-keys/#links","title":"Links","text":"<ul> <li><code>ansible.posix.authorized_key</code> Ansible Module</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/","title":"Use Standard Kubeconfig Mechanisms","text":"<ul> <li>Status: accepted</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2021-02-02</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>To increase adoption of Welkin, we were asked to observe the Principle of Least Astonishment. Currently, Welkin's handling of kubeconfig is astonishing. Most tools in the ecosystem use the standard <code>KUBECONFIG</code> environment variable and kubeconfig context implemented in the client-go library. These tools leave it up to the user to set <code>KUBECONFIG</code> or use the default <code>~/.kube/config</code>. Similarly, there is a default kubeconfig context which can be overwritten via command-line. Tools that get Cluster credentials generate a context related to the name of the Cluster.</p> <p>Tools that behave as such include:</p> <ul> <li><code>gcloud container clusters get-credentials</code></li> <li><code>az aks get-credentials</code></li> <li><code>kops</code></li> <li><code>helmfile</code></li> <li><code>helm</code></li> <li><code>kubectl</code></li> <li><code>fluxctl</code></li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Welkin needs to observe the Principle of Least Astonishment.</li> <li>Welkin needs to be compatible with various \"underlying\" vanilla Kubernetes tools.</li> <li>Welkin needs to be usable with various tools \"on top\".</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#considered-options","title":"Considered Options","text":"<ul> <li>Current solution, i.e., scripts wrapping kubeconfigs in sops which then execute \"fixed\" commands, like <code>helmfile</code>, <code>helm</code> and <code>kubectl</code>.</li> <li>\"Lighter\" scripts wrapping and unwrapping kubeconfig, allowing administrators to run <code>helmfile</code>, <code>helm</code> and <code>kubectl</code> as the administrator sees fit.</li> <li>Use standard kubeconfig mechanism.</li> </ul>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#decision-outcome","title":"Decision Outcome","text":"<p>We chose using standard kubeconfig mechanism, because it improves integration both with tools \"below\" Welkin and \"on top\" of Welkin.</p> <p>Tools that produce Kubernetes contexts are expected to use an approach similar to <code>kubectl config set-cluster</code>, <code>set-credentials</code> and <code>set-context</code>. The name of the Cluster, user and context should be derived from the name of the Cluster.</p> <p>Tools that consume Kubernetes contexts are expected to use an approach similar to <code>kubectl</code>, <code>helm</code> or <code>helmfile</code> (see links below).</p>"},{"location":"adr/0006-use-standard-kubeconfig-mechanisms/#links","title":"Links","text":"<ul> <li>Organizing Cluster Access Using kubeconfig Files</li> <li><code>kubectx</code> / <code>kubens</code></li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/","title":"Make Monitoring Forwarders Storage Independent","text":"<ul> <li>Status: accepted</li> <li>Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor</li> <li>Date: 2021-02-09</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>In the context of this ADR, forwarders refers to any components that are necessary to forward monitoring information -- specifically traces, metrics and logs -- to some monitoring database. As of February 2021, Welkin employs two projects as forwarders:</p> <ul> <li>Prometheus for metrics forwarding;</li> <li>Fluentd for log forwarding.</li> </ul> <p>Similarly, two projects are employed as monitoring databases:</p> <ul> <li>InfluxDB for metrics;</li> <li>Elasticsearch for logs.</li> </ul> <p>Overall, the monitoring system needs to be one order of magnitude more resilient than the monitored system. Forwarders improve the resilience of the monitoring system by providing buffering: In case the database is under maintenance or down, the buffer of the forwarders will ensure that no monitoring information is lost. Hence, forwarders are subject to the following tensions:</p> <ul> <li>More buffering implies storage, which make the forwarders vulnerable to storage outages (e.g., disk full, CSI hiccups);</li> <li>Less buffering implies higher risk of losing monitoring information when the database is under maintenance or down.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want a robust monitoring system.</li> <li>We want to monitor the storage system.</li> <li>We want VM-template-based rendering of the Workload Cluster, which implies no cloud native storage integration.</li> <li>We want to make it easier to \"cleanup and start from a known good state\".</li> <li>We want to have self-healing and avoid manual actions after failure.</li> <li>We want to be able to find the root cause of an incident quickly.</li> <li>We want to run as many components non-root as possible and tightly integrate with securityContext.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#considered-options","title":"Considered Options","text":"<ul> <li>Use underlying storage provider for increased buffering resilience (current approach).</li> <li>Use Local Persistent Volumes.</li> <li>Use <code>emptyDir</code> volumes.</li> <li>Use <code>hostPath</code> volumes.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: <code>emptyDir</code> for Prometheus as forwarder, because it allows monitoring of the storage system in some cases (e.g. Rook) and can redeploy automatically after Node failure. It also keeps the complexity down without much risk of data loss.</p> <p>Fluentd as forwarder is deployed via DaemonSet. Both, <code>emptyDir</code> and <code>hostPath</code> can be used.</p>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We can monitor the storage system.</li> <li>Failure of the storage system does not affect monitoring forwarder.</li> <li>Forwarder can be easier deployed \"fresh\".</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Buffered monitoring information is lost if Node is lost.</li> <li><code>emptyDir</code> can cause disk pressure. This can be handled by alerting on low disk space.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-underlying-storage-provider","title":"Use underlying storage provider","text":"<ul> <li>Good, because the forwarder can be restarted on any Node.</li> <li>Good, because the buffer can be large.</li> <li>Good, because no buffered monitoring information is lost if a Node goes down.</li> <li>Good, because buffered monitoring information is preserved if the forwarder is redeployed.</li> <li>Bad, because non-Node-local storage is generally slower. Note, however, that at least Safespring and CityCloud use a central Ceph storage Cluster for the VM's boot disk, which wipes out Node-local's storage advantage.</li> <li>Bad, because the forwarder will fail if storage provider goes down. This is especially problematic for Exoscale, bare-metal and BYO-VMs.</li> <li>Bad, because the forwarder cannot monitor the storage provider (circular dependency).</li> <li>Bad, because setting right ownership requires init containers or alpha features.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-local-persistent-volumes","title":"Use Local Persistent Volumes","text":"<ul> <li>Bad, because the forwarder cannot be restarted on any Node without manual action: \"if a Node becomes unhealthy, then the local volume becomes inaccessible by the Pod. The Pod using this volume is unable to run.\".</li> <li>Bad, because the amount of forwarding depends on the Node's local disk size.</li> <li>Bad, because buffered monitoring information is lost if the forwarder's Node goes down.</li> <li>Good, because buffered monitoring information is preserved if the forwarder is redeployed.</li> <li>Good, because Node-local storage is generally faster.</li> <li>Good, because the forwarder will survive failure of storage provider.</li> <li>Good, because the forwarder can monitor the storage provider (no circular dependency).</li> <li>Bad, because local persistent storage requires an additional configuration step.</li> <li>Bad, because setting right ownership requires init containers or alpha features.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-emptydir","title":"Use <code>emptyDir</code>","text":"<ul> <li>Good, because the forwarder can be restarted on any Node without manual action.</li> <li>Bad, because the amount of forwarding depends on the Node's local disk size.</li> <li>Bad, because buffered monitoring information is lost if the forwarder's Node goes down.</li> <li>Bad, because buffered monitoring information is lost if the forwarder is (not carefully enough) redeployed.</li> <li>Good, because Node-local storage is generally faster.</li> <li>Good, because the forwarder will survive failure of storage provider.</li> <li>Good, because the forwarder can monitor the storage provider (no circular dependency).</li> <li>Good, because works out of the box.</li> <li>Good, because it integrates nicely with <code>securityContext</code>.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#use-hostpath","title":"Use <code>hostPath</code>","text":"<p>Similar to Local Persistent Volumes, but</p> <ul> <li>Worse, because if the forwarder is redeployed on a new Node, buffering information may appear/disappear.</li> <li>Better, because it requires no extra storage provider configuration.</li> </ul>"},{"location":"adr/0007-make-monitoring-forwarders-storage-independent/#links","title":"Links","text":"<ul> <li>Prometheus Operator Storage</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/","title":"Use HostNetwork or LoadBalancer for Ingress","text":"<ul> <li>Status: accepted</li> <li>Deciders: Axel, Cristian, Fredrik, Johan, Olle, Viktor</li> <li>Date: 2021-02-09</li> </ul> <p>Technical Story: Ingress configuration</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Many regulations require traffic to be encrypted over public Internet. Welkin solves this problem via an Ingress Controller and cert-manager. As of February 2021, Welkin comes by default with Ingress-NGINX, but Ambassador is planned as an alternative. The question is, how does traffic arrive at the Ingress Controller?</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to obey the Principle of Least Astonishment.</li> <li>We want to cater to hybrid cloud deployments, including bare-metal ones, which might lack support for Kubernetes-controlled load balancer.</li> <li>Some deployments, e.g., Bring-Your-Own VMs, might not allow integration with the underlying load balancer.</li> <li>We want to keep things simple.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#considered-options","title":"Considered Options","text":"<ul> <li>Via the host network, i.e., some workers expose the Ingress Controller on their port 80 and 443.</li> <li>Over a NodePort service, i.e., <code>kube-proxy</code> exposes the Ingress Controller on a port between 30000-32767 on each worker.</li> <li>As a Service type LoadBalancer, i.e., above plus Kubernetes provisions a load balancer via Service controller.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options:</p> <ol> <li> <p>Use host network if Kubernetes-controlled load balancer is unavailable or undesired. If necessary, front the worker Nodes with a manual or Terraform-controlled load-balancer. This includes:</p> <ul> <li>Where load-balancing does not add value, e.g., if a Deployment is planned to have only a single-Node or single-worker for the foreseeable future: Point the DNS entry to the worker IP instead.</li> <li>Exoscale currently falls in this category, due to its Kubernetes integration being rather recent.</li> <li>Safespring falls in this category, since it is missing load balancers.</li> <li>If the Infrastructure Provider is missing a storage controller, it might be undesirable to perform integration \"just\" for load-balancing.</li> </ul> </li> <li> <p>Use Service type LoadBalancer when available. This includes: AWS, Azure, GCP and CityCloud.</p> </li> </ol> <p>Additional considerations: This means that, generally, it will not be possible to set up the correct DNS entries until after we apply Welkin Apps. There is a risk for \"the Internet\" -- Let's Encrypt specifically -- to perform DNS lookups too soon and cause negative DNS caches with a long lifetime. Therefore, placeholder IP addresses must be used, e.g.:</p> <pre><code>*.$BASE_DOMAIN     60s A 203.0.113.123\n*.ops.$BASE_DOMAIN 60s A 203.0.113.123\n</code></pre> <p>203.0.113.123 is in TEST-NET-3 and okay to use as placeholder. This approach is inspired by kops and should not feel astonishing.</p>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the best of each Infrastructure Provider.</li> <li>Obeys principle of least astonishment.</li> <li>We do not add a load balancer \"just because\".</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Complexity is a bit increased, however, this feels like essential complexity.</li> </ul>"},{"location":"adr/0008-use-hostnetwork-or-loadbalancer-for-ingress/#links","title":"Links","text":"<ul> <li>Cloud Controller Manager</li> <li>Ingress NGINX: Bare Metal Considerations</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/","title":"Use ClusterIssuers for Let's Encrypt","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Lennart</li> <li>Date: 2021-02-26</li> </ul> <p>Technical Story: Make apps less fragile</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Data protection regulations require encrypting network traffic over public networks, e.g., via HTTPS. This requires provisioning and rotating TLS certificates. To automate this task, we use the cert-manager, which automates provisioning and rotation of TLS certificates from Let's Encrypt.</p> <p>There are two ways to configure Let's Encrypt as an issuer for cert-manager: Issuer and ClusterIssuer. The former is namespaced, whereas the latter is Cluster-wide. Should we use Issuer or ClusterIssuer?</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to make compliantkubernetes-apps less fragile, and Let's Encrypt rate limiting is a cause of fragility.</li> <li>We want to make it easy for users to get started with Welkin in a \"secure by default\" manner.</li> <li>We want to have a clear separation between user and administrator resources, responsibilities and privileges.</li> <li>We want to keep the option open for \"light\" renderings, i.e., a single Kubernetes Clusters that hosts both Management Cluster and Workload Cluster components.</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#considered-options","title":"Considered Options","text":"<ul> <li>Use one Issuer per namespace; users need to install their own Issuers in the Workload Clusters.</li> <li>Use ClusterIssuer in Management Cluster; let users install Issuers in the Workload Clusters as required.</li> <li>Use ClusterIssuer in both Management Cluster and Workload Cluster(s).</li> </ul>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Use ClusterIssuers in the Management Cluster; optionally enable ClusterIssuers in the Workload Cluster(s)\", because it reduces fragility, clarifies responsibilities, makes it easy to get started securely.</p> <p>Each Cluster is configured with an optional ClusterIssuer called <code>letsencrypt-prod</code> for Let's Encrypt production and <code>letsencrypt-staging</code> for Let's Encrypt staging. The email address for the ClusterIssuers is configured by the administrator.</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":""},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#direct-lets-encrypt-emails-to-a-logging-mailbox","title":"Direct Let's Encrypt emails to a \"logging\" mailbox","text":"<p>Although Let's Encrypt does not require an email address, cert-managers seems to require all ClusterIssuers/Issuers to be configured with a syntactically valid email address. Said email address will receive notifications when certificates are close to expiry. Given that Welkin comes with Cryptography dashboards, these emails do not seem useful. Hence, ClusterIssuer emails should be directed to an address that has \"logging\" but not \"alerting\" status.</p>"},{"location":"adr/0009-use-cluster-issuers-for-letsencrypt/#separate-registered-domains","title":"Separate registered domains","text":"<p>Let's Encrypt production has a rate limit of 50 certificates per week per registered domain. For example, if <code>awesome-website.workload-cluster.environment.elastisys.se</code> points to the Workload Cluster's Ingress Controller, then an excessive creation and destruction of Ingress resources may trigger rate limiting for all of <code>elastisys.se</code>.</p> <p>It is therefore advisable to:</p> <ul> <li>Use separate registered domains for development and production environments.</li> <li>Use separate registered domains for Workload Cluster(s) and the Management Cluster, or restrict which Ingress resources can be created by the user.</li> </ul> <p>Note that, the rate limiting risk exists with both Issuers and ClusterIssuers and was not introduced by this ADR.</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/","title":"Run managed services in Workload Cluster","text":"<ul> <li>Status: proposed. This ADR did not reach consensus, with strong arguments on both sides. However, due to needing a decision in a timely manner, this ADR is actually followed. Therefore, this ADR serves both for visibility and to document a fait accompli.</li> </ul> <ul> <li>Deciders: Cristian</li> <li>Date: 2021-04-29</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>To truly offer our users an option to run containerized workloads in EU jurisdiction, they also need additional managed services, like databases, message queues, caches, etc.</p> <p>Where should these run?</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Some of these services are chatty and need low latency.</li> <li>Some of these services might assume trusted clients over a trusted network.</li> <li>We want to make it easy to run these services with regulatory compliance in mind, e.g., we should be able to reuse Welkin features around monitoring, logging, access control and network segregation.</li> <li>We want to make it difficult for Welkin users to negatively affect managed services.</li> <li>We want to keep support for multiple Workload Cluster, i.e., application multi-tenancy.</li> <li>Many Infrastructure Providers do not support Service type LoadBalancer, which complicates exposing non-HTTP services outside a Kubernetes Cluster.</li> <li>Management Cluster might not exist in a future packaging of Welkin.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Run managed services in Workload Cluster</li> <li>Run managed services in Management Cluster</li> <li>Run managed services in yet another Cluster</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"run managed services in Workload Cluster\".</p>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Latency is minimized: The application consuming the managed service is close to the managed service, without needing to go through intermediate software components, such as a TCP Ingress Controller.</li> <li>NetworkPolicies can be reused for communication segregation.</li> <li>OpenID and RBAC in the Workload Cluster can be reused for user access control.</li> <li>Kubernetes audit log can be re-used for auditing user access managed services. Such access is required, e.g., for manual database migrations and \"rare\" operations like GDPR data correction requests.</li> <li>Ease of exposition: No need for Service type LoadBalancer, which is not supported on all Infrastructure Providers.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Blurs separation of responsibilities between user and administrator.</li> <li>The managed service is easier impacted by user misusage, e.g., bringing a Node into OOM.</li> <li>Workload Cluster can no longer be deployed with \u201cfree for all\u201d security. Platform Administrators need to push and fight back against loose access control.</li> </ul>"},{"location":"adr/0010-run-managed-services-in-workload-cluster/#links","title":"Links","text":"<ul> <li>Service type LoadBalancer</li> <li>Exposing TCP and UDP services with Ingress-NGINX</li> <li>Redis Security</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/","title":"[Superseded by ADR-0046] Let upstream projects handle CRDs","text":"<ul> <li>Status: superseded by ADR-0046</li> <li>Deciders: Arch Meeting</li> <li>Date: 2021-04-29</li> </ul> <p>Technical Story: #446 #369 #391 #402 #436.</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>CustomResourceDefinitions (CRDs) are tricky. They are essentially a mechanism to change the API of Kubernetes. Helm 2 had zero support for CRDs. Helm 3 has support for installing CRDs, but not upgrading them.</p> <p>How should we handle CRDs?</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity and need to be treated specially.</li> <li>Generally need to \u201ctrim fat\u201d and rely on upstream.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#considered-options","title":"Considered Options","text":"<ul> <li>Install and upgrade CRDs as part of the bootstrap step, which is a Helm 2 legacy.</li> <li>Rely on whatever mechanism is proposed by upstream Helm Charts.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Rely on upstream\", because it trims fat and reduces astonishment.</p> <p>At installation, rely on upstream approach to install CRDs (see below). At upgrade, propagate upstream migration steps in Welkin migration steps in each release notes. An issue template was created to ensure we won't forget.</p> <p>Since we \"vendor in\" all Charts, CRDs can be discovered using:</p> <pre><code>grep -R 'kind: CustomResourceDefinition'\n</code></pre>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Less astonishing, compared to installing Chart \"by hand\".</li> <li>Less maintenance, i.e., there is only one source of truth for CRDs.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>None really.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#detailed-audit","title":"Detailed Audit","text":"<p>A detailed audit was performed of all CRDs in Welkin on 2021-04-27.</p> <p>As a summary, all projects encourage installing CRDs as part of standard <code>helm install</code>. Most projects encourage following manual migration steps to handle CRDs. Some projects handle CRD upgrades.</p> <p>A detailed analysis is listed below:</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#cert-manager","title":"cert-manager","text":"<ul> <li>Installation: The cert-manager Helm Chart includes the <code>installCRDs</code> value -- by default it is set to <code>false</code>. If set to <code>true</code>, then CRDs are automatically installed when installing cert-manager, albeit not using the CRDs mechanism provided by Helm.</li> <li>Upgrade: CRDs are supposed to be upgraded manually.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#dex","title":"Dex","text":"<p>Dex can be configured without CRDs. ADR-0012 argues for that approach.</p>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#gatekeeper","title":"gatekeeper","text":"<ul> <li>Installation: Gatekeeper installs CRDs using the mechanism provided by Helm.</li> <li>Upgrade: Gatekeeper wants you to either uninstall-install or run a helm_migrate.sh.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#prometheus-kube-prometheus-stack","title":"Prometheus (kube-prometheus-stack)","text":"<ul> <li>Installation: kube-prometheus-stack installs CRDs using standard Helm mechanism.</li> <li>Upgrade: kube-prometheus-stack expects you to run manual upgrade steps.</li> </ul>"},{"location":"adr/0011-let-upstream-projects-handle-crds/#velero","title":"Velero","text":"<ul> <li>Installation: Velero install CRDs using standard Helm mechanism.</li> <li>Upgrade: Velero includes magic to upgrade CRDs.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/","title":"[Superseded by ADR-0017] Do not persist Dex","text":"<ul> <li>Status: superseded by ADR-0017</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2021-04-29</li> </ul> <p>Technical Story: Reduce Helmfile concurrency for improved predictability</p>"},{"location":"adr/0012-do-not-persist-dex/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Dex requires persisting state to perform various tasks such as tracking refresh tokens, preventing replays, and rotating keys.</p> <p>What persistence option should we use?</p>"},{"location":"adr/0012-do-not-persist-dex/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity</li> <li>Storage adds complexity</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#considered-options","title":"Considered Options","text":"<ul> <li>Use \"memory\" storage</li> <li>Use CRD-based storage</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use memory\", because it simplified operations with little negative impact.</p>"},{"location":"adr/0012-do-not-persist-dex/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Dex brings no additional CRDs, which simplified upgrades.</li> <li>Dex brings no state, which simplified upgrades.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>The authentication flow is disrupted, if Dex is rebooted exactly during an authentication flow. There is no user impact if Dex is restarted after the JWT was issued. Cristian tested this with <code>kubectl</code> and Grafana. Since we will only reboot Dex during maintenance windows, this is unlikely to be an issue in the foreseeable future.</li> </ul>"},{"location":"adr/0012-do-not-persist-dex/#other-considerations","title":"Other Considerations","text":"<p>If Dex becomes a bottleneck and needs replication, or if we want to avoid disrupting authentication flows during operations on Dex, we will have to revisit this ADR.</p>"},{"location":"adr/0013-configure-alerts-in-omt/","title":"[Superseded by ADR-0060] Configure Alerts in On-call Management Tool (e.g., Opsgenie)","text":"<ul> <li>Status: superseded</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2021-06-03</li> </ul> <p>Technical Story: See \"Investigate how to systematically work with alerts\"</p>"},{"location":"adr/0013-configure-alerts-in-omt/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Alerts are some noteworthy IT event, like a Node becoming un-ready, login failure or a disk getting full. Terminology differs across tooling and organizations, but one generally cares about:</p> <ul> <li>P1 (critical) alerts, which require immediate human attention -- the person on-call needs to be notified immediately -- and;</li> <li>P2 (high) alerts which require human attention with 24 hours -- the person on-call needs to be notified next morning;</li> <li>P3 (moderate) alerts which do not require immediate human attention, but should be regularly reviewed.</li> </ul> <p>Other priorities (e.g., P4 and below) are generally used for informational purposes.</p> <p>Dealing with alerts correctly entails prioritizing them (e.g., P1, P2, P3), deciding if someone should be notified, who should be notified, how they should be notified (e.g., SMS or email) and when. \"Who\", \"how\" and \"when\" should include escalation, if the previous notification was not acknowledged within a pre-configured time interval, then the same person if notified via a different channel or a new person is notified.</p> <p>Under-alerting -- e.g., notifying an on-call person too late -- may lead to Service Level Agreement (SLA) violations and a general feeling of administrator anxiety: \"Is everything okay, or is alerting not working?\". Over-alerting -- e.g., notifying a person too often about low-priority alerts -- leads to alert fatigue and \"crying wolf\" where even important alerts are eventually ignored. Hence, configuring the right level of alerting -- in particular notifications -- is extremely important both for SLA fulfillment and a happy on-call team.</p> <p>Where should alerting be configured, so as to quickly converge to the optimal alerting level?</p>"},{"location":"adr/0013-configure-alerts-in-omt/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Allow to quickly silence, un-silence and re-prioritize alerts.</li> <li>Allow arbitrary flexibility, e.g., who should be notified, when should notification happen, when should escalation happen, for what Cluster and namespaces should notification happen, etc.</li> <li>Leverage existing tools and processes.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#considered-options","title":"Considered Options","text":"<ul> <li>Configure alerting in Welkin, specifically Alertmanager.</li> <li>Configure alerting in an On-call Management Tool (OMT), e.g., Opsgenie, PagerDuty.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Welkin \u201cover-alerts\u201d, i.e., forwards all alerts and all relevant information to an On-Call Management Tool (OMT, e.g., Opsgenie). Configuration of alerts happens in the OMT.</p>"},{"location":"adr/0013-configure-alerts-in-omt/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Clear separation of concerns.</li> <li>Alerting does not require per-customer configuration of Welkin.</li> <li>Leverages existing tools and processes.</li> <li>We do not need to implement complex alert filtering in Welkin, e.g., silence alerts during maintenance windows, silence alerts during Swedish holidays, etc.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Does not capture alerting know-how in Welkin.</li> <li>Migration to a new OMT means all alerting configuration needs to be migrated to the new tool. Fortunately, this can be done incrementally.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<ul> <li>Platform Administrators should familiarize themselves with the capabilities of OMT, e.g., OpsGenie. This should be first done using a web UI, since that improves discoverability of such capabilities.</li> <li>When alerting configuration becomes too complex and/or repetitive, Platform Administrators should employ a configuration management tools, such as Terraform, to configure the OMT.</li> </ul>"},{"location":"adr/0013-configure-alerts-in-omt/#links","title":"Links","text":"<ul> <li>Opsgenie documentation</li> <li>Alertmanager documentation</li> <li>Terraform Opsgenie provider</li> <li>Pulumni Opsgenie module</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/","title":"Use bats for testing bash wrappers","text":"<ul> <li>Status: accepted</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2021-06-03</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We write wrapper scripts for simpler and consistent operations. How should we test these scripts?</p>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to use the best tools out there.</li> <li>We want to reduce tools sprawl, i.e., the collective cost (e.g., training) of adding a new tool should outweigh the collective benefit of the new tool.</li> <li>We want to make contributions inviting.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#considered-options","title":"Considered Options","text":"<ul> <li>Do not test bash scripts. (We write perfect scripts 100% of the time, right? )</li> <li>Use <code>alias</code> for mocking, <code>diff</code> and <code>test</code> for assertions.</li> <li>Use bats</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"bats\", because the benefit of using a standard and rather light tool outweighs the cost of collective training on the new tool.</p>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We use a pretty standard tool for testing in the bash universe.</li> <li>We do not risk re-inventing the while by writing our own wrappers around <code>alias</code>, <code>diff</code> and <code>test</code>.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to learn another tool, fortunately, it seems pretty light.</li> </ul>"},{"location":"adr/0014-use-bats-for-testing-bash-wrappers/#other-considerations","title":"Other Considerations","text":"<p>Be very mindful about not overusing bash. Generally bash should only be used for things that you would do in the terminal, but got tired of copy-pasting, like:</p> <ul> <li>Running commands</li> <li>Copying files</li> <li>Setting environment variables</li> <li>Minor path translations</li> </ul> <p>For more advanced functionality prefer upstreaming into Ansible roles/libraries, Helm Charts, upstream source code, etc.</p>"},{"location":"adr/0015-we-believe-in-community-driven-open-source/","title":"We believe in community-driven open source","text":"<ul> <li>Status: accepted</li> <li>Deciders: Rob, Johan, Cristian (a.k.a., Product Management working group)</li> <li>Date: 2021-08-17</li> </ul>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We often get bombarded with questions like \"Why don't you use X?\" or \"Why don't you build on top of Y?\", sometimes preceded by \"product/project X already has feature Y\". Needless to say, this can cause a \"Simpsons Already Did It\" feeling.</p> <p>This ADR clarifies one of the core values of the Welkin project, namely our belief in community-driven open source. The ADR is useful to clarify both to internal and external stakeholders the choices we make.</p>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We do not want to depend on the interests of any single company, be it small or large.</li> <li>The Application Developers need to have a business continuity plan, see ISO 27001, Annex A.17. Therefore, we want to make it easy to \"exit\" Welkin and take over platform management.</li> <li>We want to use the best tools out there.</li> </ul>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#considered-options","title":"Considered Options","text":"<ul> <li>Prefer closed source solutions.</li> <li>Prefer single-company open source solutions.</li> <li>Prefer community-driven open source solutions.</li> </ul>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"prefer community-driven open source solutions\".</p>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We do not depend on the interests of any single company.</li> <li>The Application Developers do not depend on the interests of any single company.</li> <li>Business continuity is significantly simplified for the Application Developers.</li> <li>We have better chances at influencing projects in a direction that is useful to us and the Application Developers. The smaller the project, the easier to influence.</li> </ul>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0015-we-believe-in-community-driven-open-source/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Sometimes we might need to give up \"that cool new feature\" until the community-driven open source solution catches up with their closed source or single-company open source alternative. Alternatively, we might need to put extra time and effort to develop \"that cool new feature\" ourselves.</li> <li>As they are not bound by vendor liability -- e.g., end-of-life promises -- community-driven projects present a greater risk of being abandoned. The smaller the project, the higher the risk.</li> </ul>","tags":["ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"]},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/","title":"[Superseded by ADR-0040] gid=0 is okay, but not by default","text":"<ul> <li>Status: superseded by ADR-0040</li> <li>Deciders: Cristian, Lars, Olle</li> <li>Date: 2021-08-23</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>OpenShift likes to shift (pun intended) the UID -- i.e., assign arbitrary UIDs -- to containers. They do this as an additional security feature, given that OpenShift is a multi-tenant Kubernetes solution. Each OpenShift project receives a non-overlapping UID range. Hence, in case an attacker escapes a container, it will be more difficult to interfere with other processes.</p> <p>However, this shifting of UIDs introduces an additional complexity: What if a process wants to write to the filesystem? What uid, gid and permissions should the files and folders have? To solve this problem, the OpenShift documentation (see \"Support arbitrary user ids\") recommends setting gid=0 on those files and folders. Specifically, the Dockerfiles of the container images should contain:</p> <pre><code>RUN chgrp -R 0 /some/directory &amp;&amp; chmod -R g=u /some/directory\n</code></pre> <p>During execution, OpenShift assigns <code>gid=0</code> as a supplementary group to containers, so as to give them access to the required files.</p> <p>In contrast to OpenShift, Welkin is not a multi-tenant solution. Given previous vulnerabilities in Kubernetes that affected tenant isolation (e.g., CVE-2020-8554 ), we believe that non-trusting users should not share a Workload Cluster. Hence, we do not assign arbitrary UIDs to containers and do not need to assign <code>gid=0</code> as a supplementary group.</p> <p>The <code>gid=0</code> practice above seems to have made its way in quite a few Dockerfiles, however, it is far from being the default outside OpenShift.</p> <p>What should Welkin do with the <code>gid=0</code> practice?</p>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>For user expectations, we want to make it easy to start with Welkin.</li> <li>For better security and easier audits, we do not want to add unnecessary permissions.</li> <li>ID mapping in mounts has landed in Linux 5.12. Once this feature is used in container runtimes and Kubernetes, the <code>gid=0</code> problem will go away.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#considered-options","title":"Considered Options","text":"<ul> <li>Allow <code>gid=0</code> by default.</li> <li>Disallow <code>gid=0</code> by default -- this is what Kubespray does.</li> <li>Never allow <code>gid=0</code>.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"disallow <code>gid=0</code> by default\". Enabling it on a case-by-case basis is okay.</p>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We do not unnecessarily add a permission to containers.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some users will complain about their container images not starting, and we will need to add a less restricted PodSecurityPolicy in their Cluster.</li> </ul>"},{"location":"adr/0016-gid-0-is-okey-but-not-by-default/#other-considerations","title":"Other Considerations","text":"<p>PodSecurityPolicies are deprecated in favor of PodSecurity Admission. This decision will have to be revisited once PodSecurity Admission is stable.</p> <p>In case we notice that the <code>gid=0</code> practice is gaining significant uptake, we will have to revisit this decision to allow <code>gid=0</code> by default.</p> <p>In case ID mapping is implemented in container runtimes and Kubernetes, this problem will likely go away. In that case, this decision might be revisited to never allow <code>gid=0</code>.</p>"},{"location":"adr/0017-persist-dex/","title":"Persist Dex","text":"<ul> <li>Status: accepted</li> <li>Deciders: Architecture Meeting</li> <li>Date: 2021-11-16</li> </ul> <p>Technical Story: Enable Dex persistence</p>"},{"location":"adr/0017-persist-dex/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Dex requires persisting state to perform various tasks such as track refresh tokens, preventing replays, and rotating keys.</p> <p>What persistence option should we use?</p>"},{"location":"adr/0017-persist-dex/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>CRDs add complexity</li> <li>Storage adds complexity</li> <li>We want to frequently reboot Nodes for security patching</li> <li>We want to deliver excellent user experience</li> </ul>"},{"location":"adr/0017-persist-dex/#considered-options","title":"Considered Options","text":"<ul> <li>Use \"memory\" storage</li> <li>Use CRD-based storage</li> </ul>"},{"location":"adr/0017-persist-dex/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use CRD-based storage\", because it improves user experience when Nodes are rebooted.</p> <p>With \"memory\" storage, Dex loses the OpenID keys when restarted, which leads to the user being forced to eventually re-login. Worst off, this forced re-login happens unexpectedly from the user's perspective, when the Kubernetes apiserver chooses to refresh the OpenID keys.</p> <p>Here is the experiment to illustrate the issue:</p> <pre><code>$ curl https://dex.$DOMAIN/.well-known/openid-configuration &gt; before-openid-configuration.json\n$ curl https://dex.$DOMAIN/keys &gt; before-keys.json\n\n$ kubectl delete pods -n dex -l app.kubernetes.io/instance=dex\n\n$ curl https://dex.$DOMAIN/.well-known/openid-configuration &gt; after-openid-configuration.json\n$ curl https://dex.$DOMAIN/keys &gt; after-keys.json\n\n$ diff -y before-openid-configuration.json after-openid-configuration.json\n[empty output, no differences]\n\n$ diff -y before-keys.json after-keys.json\n[all keys are replaced]\n</code></pre>"},{"location":"adr/0017-persist-dex/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Nodes which host Dex can be rebooted for security patching</li> <li>User experience is optimized</li> </ul>"},{"location":"adr/0017-persist-dex/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Dex will have additional permissions in the Management Cluster (see <code>rbac.yaml</code>)</li> <li>We will need to closely monitor migration steps for Dex</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/","title":"Use Probe to Measure Uptime of Internal Welkin Services","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cristian, Lucian, Ravi</li> <li>Date: 2021-11-25</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We need to measure uptime for at least two reasons:</p> <ol> <li>To serve as feedback on what needs to be improved next.</li> <li>To demonstrate compliance with our SLAs.</li> </ol> <p>How exactly should we measure uptime?</p>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to reduce tools sprawl.</li> <li>We want to be mindful about capacity and infrastructure costs.</li> <li>We want to measure uptime as observed by a consumer -- i.e., application or user -- taking into account business continuity measures, such as redundancy, fail-over time, etc.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#considered-options","title":"Considered Options","text":"<ul> <li>Blackbox exporter</li> <li>kubelet prober metrics</li> <li>Prometheus Operator Probe, which essentially wraps the Blackbox exporter in a <code>Probe</code> CustomResource.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"use Probe for measuring uptime of internal Welkin services\", because it measures uptime as observed by a consumer. Although this requires a bit of extra capacity for running Blackbox, the costs are worth the benefits.</p> <p>Instead of configuring Blackbox directly, <code>Probe</code> is a cleaner abstraction provided by the Prometheus Operator.</p> <p>The following is an example for a Probe:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: Probe\nmetadata:\n  name: google-is-up\n  labels:\n    probe: google\n    release: kube-prometheus-stack\nspec:\n  interval: 60s\n  module: http_2xx\n  prober:\n    url: blackbox-prometheus-blackbox-exporter.monitoring.svc.cluster.local:9115\n  targets:\n    staticConfig:\n      static:\n        - https://www.google.com\n</code></pre> <p>This will generate a metric as follows: <code>probe_success{cluster=\"ckdemo-wc\", instance=\"https://www.google.com\", job=\"probe/demo1/google-is-up\", namespace=\"demo1\"}</code>.</p>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We measure uptime as observed by a consumer.</li> <li>Increasing redundancy, reducing failure time, etc. will contribute positively to our uptime, as desired.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We don't currently run Blackbox in the Workload Cluster, so we'll need a bit of extra capacity.</li> </ul>"},{"location":"adr/0018-use-probe-to-measure-internal-uptime/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<p>Blackbox should only be used for measuring uptime of internal services, i.e., those that are only exposed within the Kubernetes Cluster. Examples include additional services, such as PostgreSQL, Redis and RabbitMQ.</p> <p>For external endpoints -- specifically, Dex, Grafana, Kibana, Harbor and Ingress Controllers -- prefer using an external uptime service which integrates with an On-Call Management Tool, e.g., Uptime Cloud Monitor Integration for Opsgenie.</p> <p>External uptime measurement should achieve the similar effect as the commands below:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --include https://harbor.$DOMAIN/api/v2.0/health\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://kibana.$DOMAIN/api/status\n\ncurl --head some-domain.$DOMAIN/healthz  # Pokes the WC Ingress Controller\n</code></pre>"},{"location":"adr/0019-push-metrics-via-thanos/","title":"Push Metrics via Thanos","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-01-20</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently, the Management Cluster exposes several end-points for Workload Clusters:</p> <ul> <li>Dex, for authentication;</li> <li>OpenSearch, for pushing logs (append-only);</li> <li>InfluxDB, for pushing metrics;</li> <li>Harbor, for pulling container images.</li> </ul> <p>InfluxDB has served us really well over the years. However, as we enter a new era of growth, it no longer satisfies our needs. In particular:</p> <ul> <li>It is not community-driven (see ADR-0015 We believe in community-driven open source).</li> <li>The open-source version cannot be run replicated, hence it is a single point of failure.</li> <li>It is rather capacity hungry, eating as much as 2 CPUs and 15 Gi in a standard package environment.</li> <li>It is unsuitable for long-term metrics storage, which we need -- among others -- for proper capacity management.</li> </ul> <p>We decided to migrate from InfluxDB to Thanos, which can both push and pull metrics.</p> <p>Shall we push or pull metrics using Thanos?</p>"},{"location":"adr/0019-push-metrics-via-thanos/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to support multiple Workload Clusters.</li> <li>We want to untangle the life-cycle of the Management Cluster and Workload Cluster.</li> <li>The Management Cluster acts as a tamper-proof logging environment, hence it should be difficult to tamper with metrics from the Workload Cluster.</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#considered-options","title":"Considered Options","text":"<ol> <li>Push metrics from Workload Cluster to Management Cluster.</li> <li>Pull metrics from Workload Cluster to Management Cluster.</li> </ol>"},{"location":"adr/0019-push-metrics-via-thanos/#decision-outcome","title":"Decision Outcome","text":"<p>We chose to push metrics from the Workload Cluster to the Management Cluster via Thanos Receive, because it keeps the \"direction\" of metrics flow. Hence, we keep support for multiple Workload Clusters without any changes.</p> <p>At the time of this writing, pulling metrics via Thanos sidecar seems to be the preferred way to deploy Thanos. We will monitor the ecosystem and our needs, and -- if needed -- move to pulling metrics.</p> <p>At any rate, even if we end up with Thanos Sidecar, migrating in two steps -- first from InfluxDB to Thanos Receive, then to Thanos Sidecar -- feels less risky.</p>"},{"location":"adr/0019-push-metrics-via-thanos/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>All of <code>*.$opsDomain</code> can point to the Management Cluster workers -- optionally fronted by a load-balancer -- which considerably simplifies setup.</li> <li>Multiple Workload Clusters can push metrics to the Management Cluster, which paves the path to workload multi-tenancy.</li> <li>The Management Cluster can be set up first, followed by one-or-more Workload Clusters.</li> <li>Workload Clusters become more \"cattle\"-ish.</li> </ul>"},{"location":"adr/0019-push-metrics-via-thanos/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Metrics are less protected than in a pull architecture. E.g., compromising the Workload Cluster can easier be used to mount an attack against long-term metrics storage.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/","title":"Filter by Cluster label then data source","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2021-01-27</li> </ul> <p>Technical Story: https://github.com/elastisys/compliantkubernetes-apps/issues/742</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Welkin allows multiple Workload Clusters to be connected to a single Management Cluster. This allows the metrics of multiple Workload Clusters to be inspected via the same dashboards.</p> <p>How should we organise metrics to allow users and admins to select for which Clusters they want to see metrics?</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to be able to see metrics for a single Cluster, for multiple Cluster, and even for all Clusters.</li> <li>We want to be able to reuse upstream dashboards, and some are missing filters for the <code>cluster</code> variable.</li> <li>We want to stay flexible.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#considered-options","title":"Considered Options","text":"<ul> <li>Use only the <code>cluster</code> label and expose a single data source.</li> <li>Expose multiple data sources and ignore the <code>cluster</code> label.</li> <li>Filter primarily by <code>cluster</code> label, but allow filtering by data source.</li> <li>Filter primarily by data source, but allow filtering by <code>cluster</code> label.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Filter primarily by <code>cluster</code> label, but allow filtering by data source\", because it fulfills the all decision drivers with little complexity.</p> <p>Prom-label-enforcer can be used to create multiple data sources from a single data store, discriminating by <code>cluster</code> label. To simplify Thanos configuration, we can also discriminate based on <code>tenant_id</code>, which will always contain the same value as <code>cluster</code>.</p> <p>In general, we will aim to fix dashboards missing the <code>cluster</code> variable upstream. However, by also providing filtering based on data source, we facilitate our users to reuse their dashboards, which might not be Cluster-aware.</p>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We support both dashboards with <code>cluster</code> filter and without</li> <li>We can enforce metrics multi-tenancy, i.e., map Grafana users/orgs to data sources, to filter some metrics out.</li> </ul>"},{"location":"adr/0020-filter-by-cluster-label-then-data-source/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[Minor] We need to configure data sources in <code>sc-config.yaml</code><ul> <li>For example, if we forget to add the name of a Workload Cluster, the data source will be missing, but filtering based on <code>cluster</code> label is still possible.</li> </ul> </li> <li>[Minor] Label enforcer uses a bit of resources.<ul> <li>However, we already saved a lot by migrating from InfluxDB to Thanos, so we can afford go back a bit.</li> </ul> </li> </ul>"},{"location":"adr/0021-tls-for-additional-services/","title":"Default to TLS for performance-insensitive additional services","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-02-16</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the Workload Cluster, currently databases (PostgreSQL), in-memory caches (Redis) and message queues (RabbitMQ). Traditionally, when these services are provided as managed services, they are exposed via a TLS-encrypted endpoint. See examples for:</p> <ul> <li>Redis -- notice <code>rediss://</code>;</li> <li>RabbitMQ;</li> <li>PostgreSQL.</li> </ul> <p>In Welkin, the network is assumed trusted, either because we performed a provider audit or because we enabled Pod-to-Pod encryption via WireGuard. Hence, TLS does not improve data security.</p> <p>How should we expose additional services in Welkin? With or without TLS?</p>"},{"location":"adr/0021-tls-for-additional-services/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to stick to best practices and sane defaults.</li> <li>We want to make it easy to port applications to Welkin and its additional services.</li> <li>Some services are performance-sensitive: Redis suffers a significant performance drop with TLS</li> <li>The Spotahome Redis Operator does not support TLS.</li> <li>Some services are performance-insensitive: PostgreSQL and RabbitMQ feature negligible performance impact with TLS.</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#considered-options","title":"Considered Options","text":"<ul> <li>Always disable TLS, since the network in Welkin is trusted.</li> <li>Always enable TLS.</li> <li>By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it.</li> </ul>"},{"location":"adr/0021-tls-for-additional-services/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"By default, enable TLS for performance-insensitive services and disable TLS for performance-sensitive services. Allow TLS to be disabled if the user requests it.\"</p> <p>Specifically:</p> <ul> <li>Never enable TLS for Redis: Performance impact is huge and the network is already trusted. Furthermore, the Spotahome Redis Operator does not support TLS.</li> <li>Enable TLS by default for PostgreSQL and RabbitMQ: Performance impact is negligible and most application are already configured for it.</li> <li>Allow TLS to be disabled if requested for PostgreSQL and RabbitMQ.</li> </ul>"},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/","title":"Use Dedicated Nodes for Additional Services","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-03-03</li> <li>Updated: 2023-01-12</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the Workload Cluster, currently databases (PostgreSQL), in-memory caches (Redis), message queues (RabbitMQ) and distributed tracing (Jaeger).</p> <p>On what Nodes should they run?</p>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#considered-options","title":"Considered Options","text":"<ul> <li>Spread additional services on application Nodes.</li> <li>Run additional services on dedicated Nodes.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"run additional service on dedicated Nodes\", because it improves the stability and security of the platform.</p> <p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=postgresql\nelastisys.io/node-type=redis\nelastisys.io/node-type=rabbitmq\nelastisys.io/node-type=jaegertracing\n</code></pre> <p>and taints:</p> <pre><code>elastisys.io/node-type=postgresql:NoSchedule\nelastisys.io/node-type=redis:NoSchedule\nelastisys.io/node-type=rabbitmq:NoSchedule\nelastisys.io/node-type=jaegertracing:NoSchedule\n</code></pre> <p>Important</p> <p>Dedicated Nodes still contain some Workload Cluster components for logging, monitoring, intrusion detection, etc., so not all their capacity is available to the service.</p>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Performance is more predictable.</li> <li>Responsibility is more clearly separated, i.e., application Nodes vs. additional services Nodes.</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact PostgreSQL.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Forces additional services to be sized based on available Node sizes. While some commonality exists, Node sizes are specific to each infrastructure provider.</li> <li>Latency is somewhat increased. This is an issue mostly for Redis, as other services are a bit more latency tolerant.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<p>For better application performance and security, run system Deployments and StatefulSets -- such as Ingress Controllers, Prometheus, Velero, Gatekeeper and Starboard -- onto dedicated Nodes.</p> <p>Specifically, use the following Node label:</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0022-use-dedicated-nodes-for-additional-services/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A14","NIST SP 800-171 3.13.3"]},{"location":"adr/0023-allow-snippets-annotations/","title":"[Superseded by ADR-0056] Only allow Ingress Configuration Snippet Annotations after Proper Risk Acceptance","text":"<ul> <li>Status: Superseded by ADR-0056</li> <li>Deciders: architecture meeting</li> <li>Date: 2022-08-11</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Configuration snippet annotations are a powerful tool to allow injecting any kind of NGINX configuration into the Ingress NGINX Controller. For example, it allows things such as header renaming, custom authentication, etc.</p> <p>However, with great power comes great responsibility. Configuration snippet may break the Ingress Controller and cause downtime for all applications hosted in the Workload Cluster. Also, it opens up CVE-2021-25742, which means that Application Developers can exfiltrate all Secrets in the Workload Cluster.</p> <p>How shall we best serve Application Developers without compromising platform stability and security?</p>"},{"location":"adr/0023-allow-snippets-annotations/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve Application Developers.</li> <li>We want to ensure platform stability and security.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#considered-options","title":"Considered Options","text":"<ul> <li>Allow <code>the use of \"config-snippets annotations\" with Ingress</code> by default.</li> <li>Disallow <code>the use of \"config-snippets annotations\" with Ingress</code> by default.</li> <li>Never allow <code>the use of \"config-snippets annotations\" with Ingress</code>.</li> <li>Allow <code>the use of \"config-snippets annotations\" with Ingress</code>, but only after Application Developer accepted the downtime and security risks.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Allow <code>the use of \"config-snippets annotations\" with Ingress</code>, but only after Application Developer accepted the downtime and security risks.</p>"},{"location":"adr/0023-allow-snippets-annotations/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Several use-cases commonly requested by Application Developers can be satisfied.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Platform security is at a small risk if this feature is misused by Application Developers.</li> <li>Platform stability is at a small risk if this feature is misused by Application Developers.</li> </ul>"},{"location":"adr/0023-allow-snippets-annotations/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>If you enable this feature, then make sure Application Developers understand and accept the added stability and security risks. A message as follows could be used:</p> <pre><code>Hello!\n\nAfter talking with the team, we have decided that it is okay to enable the `nginx.ingress.kubernetes.io/configuration-snippet` annotation provided that:\n\n(a) there is no other way for you to move forward;\n(b) you understand and are willing to accept the security risks;\n(c) you are okay to take responsibility for downtime caused by misconfiguration; and\n(d) you are okay to take responsibility for updating the annotation as required.\n\nFor (a), please confirm that you checked that you cannot use other annotations instead for your use cases. https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/\n\nFor (b), please confirm you are aware and understand the consequences of this CVE. https://github.com/kubernetes/ingress-nginx/issues/7837.\n\nFor (c), please confirm that you understand that this annotation is quite powerful, meaning that misconfiguration can lead to downtime for all your Ingress resources. Obviously, if Nginx goes down due to any custom configuration, then we cannot take responsibility for that.\n\nFor (d), please confirm that you are okay to take responsibility for making sure that the custom configuration is supported in newer versions of Nginx, as we sometimes upgrade Nginx. Both our release notes and calendar invites for the maintenance windows mention if we are upgrading Nginx.\n\nTo sum up, if you can confirm (a)-(d) then I can enable the `nginx.ingress.kubernetes.io/configuration-snippet` annotation.\n\nRegards,\n</code></pre>"},{"location":"adr/0024-allow-Harbor-robot-account/","title":"Allow a Harbor robot account that can create other robot accounts with full privileges","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-11-17</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We offer UI access to Harbor with admin privileges. Application Developer uses a Harbor operator that needs an admin robot account with privileges to create other robot accounts with full privileges. Should we allow creation of a Harbor robot account which can create other Harbor robot accounts?</p>"},{"location":"adr/0024-allow-Harbor-robot-account/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a platform that is easy to use and easy to automate.</li> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for Application Developers to break the platform via trivial mistakes.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#considered-options","title":"Considered Options","text":"<ul> <li>Do not allow Harbor robot account which can create Harbor robot accounts.</li> <li>Allow Harbor robot accounts which can create Harbor robot accounts.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Allow Harbor robot accounts which can create Harbor robot accounts\", because it does not provide additional privileges, but instead offers a more self-service platform.</p>"},{"location":"adr/0024-allow-Harbor-robot-account/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Harbor is more flexible and easy to automate.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Increases the chances that the Application Developer can cripple the Harbor service.</li> </ul>"},{"location":"adr/0024-allow-Harbor-robot-account/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<p>Make it clear in the ticket requesting this that the Application Developer accepts the risk of \"shooting themselves in the foot\".</p>"},{"location":"adr/0025-local-storage/","title":"Use local-volume-provisioner for Managed Services that requires high-speed disks","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-11-10</li> </ul>"},{"location":"adr/0025-local-storage/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>After performing several storage load testing and PostgreSQL load testing and benchmarking we have discovered that the local storage is significantly faster than network storage. We have one use case where the disk has proven to be too slow for a PostgreSQL database and the performance was not as expected. How should we expose local storage to Managed Services?</p>"},{"location":"adr/0025-local-storage/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve the Application Developer needs.</li> <li>We want to offer fast performance for Managed Services.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We want to find a future-proof solution, which exposes local disks to any application.</li> </ul>"},{"location":"adr/0025-local-storage/#considered-options","title":"Considered Options","text":"<ul> <li>Use the fastest network storage with dedicated IOPS.</li> <li>Use local storage with local-volume-provisioner.</li> <li>Use local storage with local-path-provisioner</li> <li>Use <code>hostPath</code></li> <li>Use <code>local</code></li> </ul>"},{"location":"adr/0025-local-storage/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Use local storage with local-volume-provisioner and move the code within the Kubespray repository.</p>"},{"location":"adr/0025-local-storage/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Services using the local storage are performing better.</li> <li>We are able to provide a PostgreSQL service that meets the high performance requirements.</li> </ul>"},{"location":"adr/0025-local-storage/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Scaling the storage becomes harder as it will involve replacing the Nodes.</li> <li>We are limited by the size of the volumes that are available within the Infrastructure Provider offering.</li> </ul>"},{"location":"adr/0025-local-storage/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>When using the local-volume-provisioner please create dedicated partitions and make sure to reserve enough space for the boot partition. Failing to do so can lead to the entire disk becoming full and the Node becoming unresponsive and crashing.</p>"},{"location":"adr/0025-local-storage/#links","title":"Links","text":"<ul> <li>local-volume-provisioner</li> <li>local-path-provisioner</li> <li><code>hostPath</code></li> <li><code>local</code></li> </ul>"},{"location":"adr/0026-hnc/","title":"Use <code>environment-name</code> as the default root of Hierarchical Namespace Controller (HNC)","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-09-29</li> </ul>"},{"location":"adr/0026-hnc/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>As of apps v0.25 the HNC namespaces are available. Should we allow the root of the HNC to be configurable? What default value should it have?</p>"},{"location":"adr/0026-hnc/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to reduce the number of tickets related to namespace creation.</li> <li>We want to make the Platform Administrators life easier.</li> <li>HNC recommends against using default, as the user could edit the Kubernetes Service in the default namespace.</li> </ul>"},{"location":"adr/0026-hnc/#considered-options","title":"Considered Options","text":"<ul> <li>Hard-code the root of HNC to <code>default</code></li> <li>Hard-code the root of HNC to a value other than <code>default</code></li> <li>Allow the root of HNC to be configured, but provide a default value</li> </ul>"},{"location":"adr/0026-hnc/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: Allow the root of Hierarchical Namespace to be configured, but provide a default value. The default value is the <code>environment-name</code>. If a different value was previously used, then no migration is needed.</p>"},{"location":"adr/0026-hnc/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the Platform Administrators life easier by not having to handle namespace requests tickets</li> <li>Platform Administrator does not have to do the migration</li> <li>We offer flexibility to our Application Developers in choosing if they want to keep existing non-HNC namespaces and from further on use the new HNC namespace to create new namespaces on their own.</li> <li>Increase Application Developer autonomy</li> </ul>"},{"location":"adr/0026-hnc/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some snowflakiness will exist by keeping both the non-HNC and HNC namespaces</li> <li>Some Application Developers might get confused at the beginning by not knowing which namespaces are HNC and which are non-HNC.</li> </ul>"},{"location":"adr/0026-hnc/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>Do not use the \"default\" namespace as the name of the default root HNC namespace because the user could edit the Kubernetes Service in the default namespace. Exclude the core and the AMS namespaces from HNC as we do not want those to be managed by HNC. Do not migrate Application Developer to the new default root Hierarchical Namespace, but do inform the Application Developers that they have the choice to migrate to it, if desired.</p>"},{"location":"adr/0026-hnc/#links","title":"Links","text":"<ul> <li>HNC repository on GitHub</li> <li>HNC</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/","title":"PostgreSQL - Enable external replication","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-10-27</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We have received a few requests from Application Developers to enable external replication on PostgreSQL so that they are able to create copies of the PostgreSQL Cluster themselves to use for testing, development, tuning and disaster recovery purposes. Should we allow and enable external replication for PostgreSQL, or should we offer an alternative that can mimic that?</p>"},{"location":"adr/0027-postgresql-external-replication/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve the Application Developer needs.</li> <li>We want to make the Platform Administrators life easier.</li> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for Application Developers to break the platform via trivial mistakes.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#considered-options","title":"Considered Options","text":"<ul> <li>Allow external replication on PostgreSQL.</li> <li>Do not allow external replication on PostgreSQL.</li> <li>Allow read access to the S3 bucket containing the files that can mimic the replication.</li> <li>Clone the S3 bucket containing the files using rclone to another bucket in a new project preferably owned by the Application Developer and from there the Application Developer is free to use it.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: Clone the S3 bucket containing the files using rclone to another bucket in a new project preferably owned by the Application Developer and from there the Application Developer is free to use it. We rclone the backups (once per night) in this new S3 bucket. We provide no SLA on the rclone job.</p> <p>The option <code>Allow external replication on PostgreSQL</code> is putting the platform stability and integrity at risk, because external replication is done via the replication slots and if the destination of the replication is unreachable or stopped, then the WAL files are kept on disc until the files are sent to destination and confirmed to be received. This means that the WAL files will pile up until the Cluster runs out of space and crashes. This also leads to data loss and data corruption.</p> <p>On Infrastructure Providers that do not have native S3 support with ACL capabilities we need endpoint/credentials from the user to a S3 bucket of their choosing that we can rclone to. We rclone the backups (once per night) in this new S3 bucket and provide credentials to the Application Developer. We provide no SLA on the rclone job. The diagram of the solution looks like this: </p>"},{"location":"adr/0027-postgresql-external-replication/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We make the Platform Administrators life easier by offering them a possibility to clone/replicate their PostgreSQL Cluster from the S3 bucket containing the base backup and WAL files.</li> <li>Platform Administrators can now use 3rd party tools that can pull the base backup and WAL files and clone the PostgreSQL Cluster in another location.</li> <li>Increase Application Developer autonomy</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>For providers that do not offer S3 with ACL, we need to create and maintain an rclone job that copies the files from the initial S3 backup bucket to a new S3 bucket (preferably owned by the Application Developer)</li> <li>When rclone is involved, the time to recover will be up to 24 hours old as we will rclone once per day.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>On providers that offer S3 with ACL, this can be done with small effort. On providers that do not offer S3 with ACL we need to clone the bucket containing the file to another S3 bucket that is preferably owned by the Application Developer. This is done with no SLA on it. Offer this only on request.</p>"},{"location":"adr/0027-postgresql-external-replication/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0027-postgresql-external-replication/#option-1-allow-external-replication-on-postgresql","title":"[option 1] - Allow external replication on PostgreSQL","text":"<ul> <li>Good, because we satisfy the Application Developer need</li> <li>Bad, because it comes with a big risk of breaking the PostgreSQL Cluster and cause downtime.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#option-2-do-not-allow-external-replication-on-postgresql","title":"[option 2] - Do not allow external replication on PostgreSQL","text":"<ul> <li>Good, because we do not have to do any changes/work.</li> <li>Good, because the integrity and stability of the platform is kept intact.</li> <li>Bad, because we are not flexible and do not try to satisfy the Application Developer need.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#option-3-allow-read-access-to-the-s3-bucket-containing-the-files-that-can-mimic-the-replication","title":"[option 3] - Allow read access to the S3 bucket containing the files that can mimic the replication","text":"<ul> <li>Good, we can satisfy the customer need and demonstrate we are flexible and Application Developer oriented.</li> <li>Good, because we keep the platform stability and integrity intact.</li> <li>Bad, because it involves extra work and some snowflakiness.</li> </ul>"},{"location":"adr/0027-postgresql-external-replication/#links","title":"Links","text":"<ul> <li>PostgreSQL replication</li> <li>Streaming ReplicationHNC</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/","title":"Harder Pod eviction when Nodes are going OOM","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2022-12-08</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We had several incidents where Nodes are going OOM and become unresponsive when too many Pods are being scheduled on them. This also leads to Pods getting evicted, and this applies to our Pods that are responsible for the platform security and compliance. Should we enable kubelet hard eviction so as to behold the vital components?</p>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for Application Developers to break the platform via trivial mistakes.</li> <li>We want to make sure that the critical components are not evicted when Nodes are overcommitted.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#considered-options","title":"Considered Options","text":"<ul> <li>Enable kubelet hard eviction.</li> <li>Adjust OOM score so that kernel does not OOM critical Pods</li> <li>Setup priority class for all our apps</li> <li>Do not make any changes and reinforce the responsibility to the customer for not overloading the Nodes.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: <code>Enable kubelet hard eviction</code> &amp; <code>Setup priority class for all our apps</code> .</p> <p>The option <code>Adjust OOM score so that kernel does not OOM critical Pods</code> is not viable as it can be used only if we set requests=limits on all our Pods (see here and here), and this will not allow us to benefit from the burstable capabilities of Kubernetes and have a static allocation of resources which locks the resources on the Nodes even if they are not used most of the time. This reduces the available resources for the Application Developer on the Nodes.</p>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Security and stability of the platform is somewhat improved.</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Application Developer Pods will be stuck in pending until resources are available and this might make the Application Developer feel that their Pods are less important.</li> <li>kubectl may not observe pressure right away</li> </ul>"},{"location":"adr/0028-harder-pod-eviction-when-node-goes-OOM/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>Test multiple configurations using kubelet hard eviction, priority classes and other option to obtain the desired behaviour where the Nodes do not become unresponsive and our components are not getting evicted when the Node is overcommitted by the Application Developer.</p>"},{"location":"adr/0029-expose-jaeger-ui/","title":"Expose Jaeger UI in WC","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-01-12</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#context-and-problem-statement","title":"Context and Problem Statement","text":""},{"location":"adr/0029-expose-jaeger-ui/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a platform that is easy to use and easy to access.</li> <li>We want to ensure platform security and stability.</li> <li>We want to make it hard for customers to break the platform via trivial mistakes.</li> <li>We want to best serve our customer needs.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#considered-options","title":"Considered Options","text":"<ul> <li>Expose Jaeger UI via Ingress in Workload Cluster and use Oauth2-proxy for request authentication.</li> <li>Do not expose Jaeger UI in Workload Cluster.</li> <li>Expose Jaeger UI, but completely behind oauth2-proxy. Configure domain, groups, IP allowlisting and request logging for protecting it.</li> <li>Expose Jaeger UI and auditing access via request logging in Oath2Proxy.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: <code>Expose Jaeger UI, but completely behind oauth2-proxy. Use config domain, groups, IP allowlisting and request logging for protecting it.</code> . If this solution turns out to serve our customer needs and our needs, we will consider adopting it for exposing other services UIs like Alertmanager and Prometheus.</p>"},{"location":"adr/0029-expose-jaeger-ui/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Security and stability of the platform is maintained.</li> <li>We serve our customer needs.</li> <li>We deliver a platform that is easy to use and easy to access.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need an additional component that needs to be configured, installed and maintained.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#recommendation-to-operators","title":"Recommendation to Operators","text":"<p>Make sure that you use domain listing, groups and IP allowlisting.</p>"},{"location":"adr/0029-expose-jaeger-ui/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0029-expose-jaeger-ui/#option-1-expose-jaeger-ui-via-ingress-in-workload-cluster-and-use-oauth2-proxy-for-request-authentication","title":"[option 1] - Expose Jaeger UI via Ingress in Workload Cluster and use Oauth2-proxy for request authentication","text":"<ul> <li>Good, because we deliver a platform that is easy to use and easy to access.</li> <li>Good, because we serve our customer need</li> <li>Bad, because we lose the ability to audit the access to it as they do not have the concept of users and does not have RBAC.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#option-2-do-not-expose-jaeger-ui","title":"[option 2] - Do not expose Jaeger UI","text":"<ul> <li>Good, because the integrity and stability of the platform is kept intact.</li> <li>Bad, because we do not satisfy the customer need to easily access the UI</li> <li>Bad, because we do not serve our customer needs.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#option-3-expose-jaeger-ui-but-completely-behind-oauth2-proxy-configure-domain-groups-ip-allowlisting-and-request-logging-for-protecting-it","title":"[option 3] - Expose Jaeger UI, but completely behind oauth2-proxy. Configure domain, groups, IP allowlisting and request logging for protecting it","text":"<ul> <li>Good, because we deliver a platform that is easy to use and easy to access.</li> <li>Good, because we serve our customer need</li> <li>Good, because we do not lose the ability to audit the access to it as using this option we can audit the access.</li> <li>Good, because we the security and stability of the platform is maintained by using the domain listing, groups and IP allowlisting.</li> <li>Bad, because we need an additional component that needs to be configured, installed and maintained.</li> </ul>"},{"location":"adr/0029-expose-jaeger-ui/#option-4-expose-jaeger-ui-and-audit-access-via-request-logging-in-oath2proxy","title":"[option 4] - Expose Jaeger UI and audit access via request logging in Oath2Proxy","text":"<ul> <li>Good, because we deliver a platform that is easy to use and easy to access.</li> <li>Good, because we serve our customer need</li> <li>Good, because we do not lose the ability to audit the access to it as using this option we can audit the access.</li> <li>Bad, because it is less secure than the <code>option 3</code></li> <li>Bad, because we need an additional component that needs to be configured, installed and maintained.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/","title":"Run ArgoCD on the Elastisys Nodes","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-01-26</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We run additional services in the Workload Cluster on dedicated Nodes, currently databases (PostgreSQL), in-memory caches (Redis), message queues (RabbitMQ) and distributed tracing (Jaeger). Where should we run ArgoCD?</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to deliver an affordable additional managed service and avoid resource waste.</li> <li>ArgoCD has a small footprint, e.g., 200m CPU and 0.5GB RAM for one of our largest deployments.</li> <li>We want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#considered-options","title":"Considered Options","text":"<ul> <li>Spread ArgoCD services on application Nodes.</li> <li>Run ArgoCD services on dedicated Nodes.</li> <li>Run ArgoCD services on Elastisys Nodes, and scale up the Nodes.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Run ArgoCD services on Elastisys Nodes, and scale up the Nodes.\", because it improves the stability and security of the platform, avoids resource waste and makes the ArgoCD service more affordable to the Application Developers.</p> <p>Scale up the Elastisys Nodes to 4C8GB before installing managed ArgoCD.</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The ArgoCD service infrastructure footprint is lower than when using dedicated Nodes, due to less per-Node overhead (Fluentd, Falco)</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact ArgoCD</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to scale up the Elastisys Nodes to 4C8GB</li> <li>We are sharing the Elastisys Nodes resources with the other Elastisys platform components, e.g., Ingress Controller.</li> </ul>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>"},{"location":"adr/0030-run-argocd-on-elastisys-nodes/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/","title":"Run csi-cinder-controllerplugin on the Elastisys Nodes","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-01-26</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We use the Cinder CSI Driver to manage the lifecycle of OpenStack Cinder Volumes. Currently, the csi-cinder-controllerplugin is running on arbitrary Nodes. Where should we run the csi-cinder-controllerplugin?</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#considered-options","title":"Considered Options","text":"<ul> <li>Run the csi-cinder-controllerplugin on arbitrary Nodes.</li> <li>Run the csi-cinder-controllerplugin on Elastisys Nodes.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Run the csi-cinder-controllerplugin on Elastisys Nodes\", because it improves the stability and security of the platform and makes the application Nodes \"light\".</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The application Nodes infrastructure footprint is lower.</li> <li>Security and stability of additional services is somewhat improved, e.g., SystemOOM due to an application won't impact the csi-cinder-controllerplugin.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to change the code to be able to make csi-cinder-controllerplugin run on the Elastisys Nodes.</li> </ul>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<p>Specifically, use the following Node labels</p> <pre><code>elastisys.io/node-type=elastisys\n</code></pre> <p>and taint:</p> <pre><code>elastisys.io/node-type=elastisys:NoSchedule\n</code></pre> <p>Remember to also add tolerations and Node affinity to all affected Pods.</p>"},{"location":"adr/0031-run-csi-cinder-controllerplugin-on-elastisys-nodes/#links","title":"Links","text":"<ul> <li>Taints and Tolerations</li> <li>Well-Known Labels, Annotations and Taints</li> <li>Kubespray <code>node_labels</code> and <code>node_taints</code></li> </ul>"},{"location":"adr/0032-boot-disk-size/","title":"[Superseded by ADR-0058]Boot disk size on Nodes","text":"<ul> <li>Status: superseded by ADR-0058</li> <li>Deciders: Cristian Klein, Olle Larsson, Lucian Vlad, Fredrik Liv, Robin Wallace, Pavan Gunda</li> <li>Date: 2023-02-14</li> </ul>"},{"location":"adr/0032-boot-disk-size/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We have often defaulted to using boot disks of 50GB where possible but as of late we have noticed that for some environments this is not sufficient. We also noticed that the available space is commonly filled up by Application Developer container images. We would like to have the same boot disk sizes for all our Nodes on all the Infrastructure Providers if possible. Should we increase the boot disk size to a bigger size?</p>"},{"location":"adr/0032-boot-disk-size/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve the Application Developer needs.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> </ul>"},{"location":"adr/0032-boot-disk-size/#considered-options","title":"Considered Options","text":"<ul> <li>Keep boot disk size to 50Gb but increase the alert threshold and do regular cleanup</li> <li>Keep boot disk size to 50Gb and increase boot disk size only where is needed</li> <li>Increase the boot disk size to 100GB for all Nodes irrespective of Node size</li> <li>Increase the boot disk size to 100GB only for Nodes that are bigger than 4C8GB</li> <li>Use local disk of 100GB size for control plane Nodes</li> </ul>"},{"location":"adr/0032-boot-disk-size/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Increase the boot disk size to 100GB for all Nodes irrespective of Node size &amp; Use local disk of 100GB size for control plane Nodes</p>"},{"location":"adr/0032-boot-disk-size/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>There will be more available space for pulling images</li> <li>Reduce the number of alerts and ops time</li> <li>Improve platform stability and scalability</li> </ul>"},{"location":"adr/0032-boot-disk-size/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We need to replace all existing Nodes for our existing environments</li> <li>Cost will increase depending on number of Nodes and price per GB of storage</li> </ul>"},{"location":"adr/0032-boot-disk-size/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<ul> <li>Try to use same VM flavors on all environments</li> <li>Use VM flavor with local disk of 100GB or whichever is closest to this size depending on Infrastructure Provider for control plane Nodes.</li> <li>For worker Nodes use 100GB boot disk size on Infrastructure Providers that allow it and on the ones that we can't use the VM flavor with the closest disk size.</li> </ul>"},{"location":"adr/0032-boot-disk-size/#links","title":"Links","text":"<ul> <li>Some recommendations</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/","title":"Run Cluster API controllers on Management Cluster","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cluster API management and Tekton meeting</li> <li>Date: 2023-03-15</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>With Cluster API there are multiple different ways to organise the management hierarchy that have different impacts on the environment in regard to cost, availability, security and ease of Deployment and maintainability.</p> <p>Where should we run the Cluster API controller?</p>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to minimise the impact it has on resources</li> <li>We want to ease the Deployment and maintenance process</li> <li>We want to be able to tolerate faults in management and Workload Clusters</li> <li>We want to maintain good security posture</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Independent Clusters<ul> <li>All Clusters run the Cluster API controllers and all Clusters manage themselves independently.</li> </ul> </li> <li>Controller Cluster<ul> <li>A new separate Controller Cluster runs the Cluster API controllers and manages all Clusters in the environment.</li> </ul> </li> <li>Management Cluster<ul> <li>The Management Cluster runs the Cluster API controllers and manages all Clusters in the environment.</li> </ul> </li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Management Cluster\", because it strikes the balance between security, as it has relatively good availability properties and no Kubernetes admin credentials have to be stored in the Workload Cluster, and maintainability, as Clusters can be managed centralised.</p>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Requires no additional resources</li> <li>Requires single instance of Cluster API controllers per environment</li> <li>Requires less Deployment and maintenance efforts to manage</li> <li>Management Cluster already provides services to allow backups, monitoring, and logging</li> <li>Workload Cluster will not have Kubernetes admin credentials</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Cluster API controllers availability relies on Management Cluster<ul> <li>Main consideration is control plane, since Management Cluster has sufficient Nodes for the controller to reschedule</li> <li>Workload Cluster will still function but it will lose auto heal and auto scaling functions</li> </ul> </li> <li>Management Cluster will have Kubernetes admin credentials for the Workload Cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#independent-cluster","title":"Independent Cluster","text":"<ul> <li>Good, no additional resource requirements</li> <li>Good, all Clusters have required supporting services</li> <li>Good, all Clusters are independently impacted by failures</li> <li>Good, all Clusters are independently impacted by configuration mistakes, although...</li> <li>Bad, it is easier to do configuration mistakes</li> <li>Bad, it requires more effort to manage for each Cluster</li> <li>Bad, all Clusters have to be bootstrapped</li> <li>Bad, will contain Kubernetes admin credentials in Workload Cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#controller-cluster","title":"Controller Cluster","text":"<ul> <li>Bad, requires additional resources</li> <li>Bad, requires additional supporting services</li> <li>Bad, Management and Workload Cluster lose management (auto healing and auto scaling) on Controller Cluster failure, although...</li> <li>Good, Management and Workload Cluster state can be backed up and restored</li> <li>Good, environment can be managed as a group or individually if needed, although...</li> <li>Bad, all Cluster can be impacted by configuration mistakes, although...</li> <li>Good, it is harder for configuration mistakes</li> <li>Good, it requires less effort to manage each Cluster</li> <li>Good, single Cluster has to be bootstrapped</li> <li>Good, will not contain Kubernetes admin credentials in Workload Cluster</li> </ul>"},{"location":"adr/0033-run-cluster-api-controllers-on-service-cluster/#management-cluster","title":"Management Cluster","text":"<ul> <li>Good, no additional resource requirements</li> <li>Good, Management Cluster has required supporting services</li> <li>Bad, Workload Cluster loses management (auto healing and auto scaling) on Management Cluster failure, although...</li> <li>Good, Workload Cluster state can be backed up and restored</li> <li>Good, environment can be managed as a group or individually if needed, although...</li> <li>Bad, all Cluster can be impacted by configuration mistakes, although...</li> <li>Good, it is harder for configuration mistakes</li> <li>Good, it requires less effort to manage each Cluster</li> <li>Good, single Cluster has to be bootstrapped</li> <li>Good, will not contain Kubernetes admin credentials in Workload Cluster</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/","title":"How to run multiple AMS packages of the same type in the same environment","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-23</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Some of the Application Developers request multiple AMS packages of the same size or different sizes, using 1 package per request or multiple packages in the same request (batch-order). How should we add the second or n-th package? Should we scale the Nodes vertically or horizontally? If we scale horizontally, then in some cases we need to add AMS packages of different sizes and this brings up the problem where we need to stick the package to a specific set of Nodes otherwise for example, after a maintenance that reboots the Nodes, a postgres-4 package might be scheduled on a Node that was intended for a postgres-8 package and the postgres-8 Pod will not be able to be scheduled anymore because it does not fit on the postgres-4 Node. How can we overcome this issue? Should we add a <code>elastisys.io/ams-cluster-name</code> label/taint for the AMS with dedicated Nodes?</p>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to best serve the Application Developer needs.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Always scale the Nodes vertically and fit multiple PostgreSQL packages on the same pair of Nodes.</p> <ul> <li>but how many packages should we place on the same pair of Nodes? max 3 packages? max 5 packages?</li> <li>up to what Node sizes?</li> <li>what about resource waste? we will not find VM flavors that match exactly our sum of packages.</li> </ul> <p>On the other hand we reduce the resources allocated to our services(each Node is eating ~ 1 CPU and 2GB ram, by placing 3 packages on the same Node we reduce by 2CPU and 4GB RAM the resource waste)</p> </li> <li> <p>Always scale horizontally and place each package on its own set of dedicated Nodes.</p> </li> <li> <p>Scale the AMS dedicated Nodes vertically so that it fits all packages on the same set of Nodes.</p> <ul> <li>but how many packages should we place on the same pair of Nodes? max 3 packages? max 5 packages?</li> <li>up to what Node sizes?</li> <li>what about resource waste? we will not find VM flavors that match exactly our sum of packages.</li> </ul> </li> </ol>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: 2</p> <ol> <li> <p>Always scale horizontally and place each package on its own set of dedicated Nodes.</p> <ul> <li>\"Add additional label <code>elastisys.io/ams-cluster-name</code> to a set of Nodes dedicated to a specific package\"</li> <li>\"Do not taint the Nodes.\"</li> </ul> <p>Respect ADR-0022 and add the <code>elastisys.io/node-type</code> taint and label.</p> </li> </ol>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>The platform stability and scalability is improved.</li> <li>We provide extra isolation of the AMS.</li> <li>Being able to choose from options of scaling the Nodes both vertically and horizontally shows that we are flexible, and we can satisfy more of the Application Developer needs.</li> <li>We can add more labels that will better describe and schedule our AMS services, like <code>local-disk</code> and others.</li> <li>With options 1 and 3 more resources are available to the Application Developer.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>For option 2 we need to add the new label to the AMS repository and update documentation.</li> <li>The infrastructure footprint is increased for option 2.</li> <li>With options 1 and 3 the stability of the AMS package is reduced, because if 1 Node is unresponsive then it will affect not 1 AMS package, but multiple AMS packages.</li> </ul>"},{"location":"adr/0034-how-to-run-multiple-ams-packages-of-the-same-type/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<ul> <li>Use label like: <code>elastisys.io/ams-cluster-name</code></li> <li>Update the AMS repository and documentation with this label, and set it by default to automatically get picked up by kubectl</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/","title":"Run Tekton on Management Cluster","text":"<ul> <li>Status: accepted</li> <li>Deciders: Cluster API management and Tekton meeting</li> <li>Date: 2023-03-15</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>With Tekton we need to decide where to run it, as it will need considerable permissions in the target Cluster to be able to manage it.</p> <p>So, where should we run Tekton?</p>"},{"location":"adr/0035-run-tekton-on-service-cluster/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to ease the maintenance and management process</li> <li>We want to maintain good security posture</li> <li>We want to be able to tolerate faults</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#considered-options","title":"Considered Options","text":"<ul> <li>Tekton on Management Cluster</li> <li>Tekton on each Cluster</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option \"Tekton on Management Cluster\", because it will follow nicely together with the decision for Cluster API controller and management hierarchy. No credentials have to be stored in the Workload Cluster, and management can be done centralised.</p>"},{"location":"adr/0035-run-tekton-on-service-cluster/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Requires single instance of Tekton per environment</li> <li>Requires less maintenance and management efforts</li> <li>Tekton should be unaffected if the Workload Cluster is subjected to faults or a bad change and should be able to keep managing the Workload Cluster and perform rollback as needed.</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Possibility that Tekton itself goes into a bad state by applying a bad change<ul> <li>This should however only impact the Management Cluster</li> </ul> </li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0035-run-tekton-on-service-cluster/#tekton-on-management-cluster","title":"Tekton on Management Cluster","text":"<ul> <li>Good, single instance to setup and manage</li> <li>Good, centralised management of all Clusters</li> <li>Good, environment can be managed as a group or individually if needed</li> <li>Good, Tekton itself should be unaffected if it applies a bad change to the Workload Cluster and will be available to perform rollback</li> <li>Bad, Tekton itself can potentially go into a bad state if it applies a bad change to the Management Cluster</li> <li>Good, no need for high privilege credentials in Workload Cluster</li> <li>Bad, aggregating high privilege credentials in Management Cluster</li> <li>Good, limited access to Management Cluster</li> </ul>"},{"location":"adr/0035-run-tekton-on-service-cluster/#tekton-on-each-cluster","title":"Tekton on each Cluster","text":"<ul> <li>Bad, multiple instances to setup and manage</li> <li>Bad, individual management of each Cluster</li> <li>Good, each Cluster individually impacted by failures</li> <li>Bad, Tekton itself can potentially go into a bad state if it applies a bad change to the Cluster</li> <li>Bad, need for high privilege credentials in Workload Cluster</li> <li>Good, no aggregation of high privilege credentials in Management Cluster</li> <li>Bad, wider access to Workload Cluster</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/","title":"Run Ingress-NGINX as a DaemonSet","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-16</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently we run Ingress-NGINX as a DaemonSet. This can potentially feel like a waste of resources in large environments. Running the Ingress Controller as a Deployment with at least two replicas is a possibility.</p> <p>Should we run Ingress-NGINX as a Deployment or as a DaemonSet? What do we do for Infra Providers that do not have Service type LoadBalancer?</p>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to deliver a stable and secure platform.</li> <li>We want to keep application Nodes \"light\", so the application team knows what capacity is available for their application.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> <li>We want to keep things simple.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#considered-options","title":"Considered Options","text":"<ol> <li>Keep running the Ingress-NGINX as a DaemonSet.</li> <li>Run Ingress-NGINX as a Deployment with 2 or more replicas depending on the environment size and requirements.</li> <li>Do not run Ingress-NGINX on the AMS Nodes.</li> <li>For Infra Providers without Service type LoadBalancer continue using host network as decided in adr0008</li> <li>For Infra Providers without Service type LoadBalancer start using Service type NodePort for NGINX and also use the external load balancer to route traffic from ports 80/443 to Node ports 30080/30443.</li> </ol>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options: 1 &amp; 3 &amp; 5</p> <ul> <li>\"Keep running Ingress-NGINX as a DaemonSet.\"</li> <li>\"Do not run Ingress-NGINX on the AMS Nodes.\"</li> <li>\"For Infra Providers without Service type LoadBalancer start using Service type NodePort for NGINX and also use the external load balancer to route traffic from ports 80/443 to Node ports 30080/30443\" -&gt; This supersedes adr0008.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We keep things simple and have the same solution on all Infrastructure Providers.</li> <li>We keep the platform stable and secure, and don't risk dropping traffic when we replace Nodes or Nodes become unavailable.</li> <li>No changes are needed.</li> <li>More resources are available on the AMS Nodes.</li> <li>Reduce complexity.</li> <li>We will now use <code>externalTrafficPolicy: local</code> and with this we will reduce latency and preserve the client\u2019s source IP address, which is essential for some applications that rely on knowing the client's IP, such as for logging, security, or geolocation services.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Feels like some resources are wasted on very large environments with many Nodes.</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<ul> <li>Do not run the Ingress-NGINX on the AMS Nodes.</li> <li>For Infra Providers without Service type LoadBalancer start using Service type NodePort for NGINX and also use the external load balancer to route traffic from ports 80/443 to Node ports 30080/30443</li> </ul>"},{"location":"adr/0036-run-ingress-nginx-as-daemonset/#links","title":"Links","text":"<ul> <li>Issue using host network</li> </ul>"},{"location":"adr/0037-enforce-ttl-on-jobs/","title":"Enforce TTL on Jobs","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-30</li> </ul>"},{"location":"adr/0037-enforce-ttl-on-jobs/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Lingering finished Jobs are annoying, put unnecessary pressure on the API server, and causes bloat metrics. I fail to see any point of having the possibility of keeping Jobs in Kubernetes around forever. Others agree with me and since k8s 1.23 TTL on Jobs entered a stable API state.</p> <p>So, should we enforce the usage of TTL on Jobs?</p>"},{"location":"adr/0037-enforce-ttl-on-jobs/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes MSE burden.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0037-enforce-ttl-on-jobs/#considered-options","title":"Considered Options","text":"<ol> <li>Do nothing</li> <li>Enforce TTL via Gatekeeper, aka reject if not set in Job spec.</li> <li>Enforce TTL via Gatekeeper mutation, aka update Job spec if not set.</li> <li>Enforce TTL via Gatekeeper mutation, aka update Job spec if not set. and lower the value if set too high.</li> </ol>"},{"location":"adr/0037-enforce-ttl-on-jobs/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: 3 - \"Enforce TTL via Gatekeeper mutation, aka update Job spec if not set.\" We decided for a default Job TTL of 7 days, as this is a good compromise between being able to inspect the Pod -- e.g., check exit status and logs -- and not keeping Pods for too long.</p>"},{"location":"adr/0037-enforce-ttl-on-jobs/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Application Developers don't need to worry about setting any value</li> <li>We don't have to worry about setting any value for our jobs (assuming we will run Gatekeeper in SC, in the meantime we can just update our Job specs in Apps)</li> <li>There won't be any lingering Jobs in Kubernetes (assuming that no bad finalizers are set)</li> <li>TTL of Jobs will be visible in our IO site and possibly stated in our ToS</li> </ul>"},{"location":"adr/0037-enforce-ttl-on-jobs/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some Application Developers might not agree with our set TTL (not likely though IMO)</li> </ul>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/","title":"Replace the starboard-operator with the trivy-operator","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-03-30</li> </ul>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>The Maintainers of Starboard deprecated it in favor of Trivy Kubernetes with Trivy operator. They will no longer make any bigger changes to Starboard operator. They announced the change in March 2023.</p> <p>We currently use Starboard operator for scanning images with Trivy and for running the CIS Kubernetes benchmark with kube-bench. Trivy operator has support for scanning images and running a version of the CIS Kubernetes benchmark.</p> <p>Can or should we follow the evolution and replace starboard-operator with trivy-operator?</p>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security and stability.</li> <li>We want to use the best tools out there.</li> </ul>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#considered-options","title":"Considered Options","text":"<ol> <li>Do nothing</li> <li>Move ahead and replace starboard-operator with trivy-operator</li> </ol>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: 2 - Move ahead and replace starboard-operator with trivy-operator and include the CIS Kubernetes benchmark</p>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#positive-consequences","title":"Positive Consequences","text":"<p>The main and the good reason for replacing starboard-operator with trivy-operator is that starboard-operator is getting replaced as stated above.</p>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#negative-consequences","title":"Negative Consequences","text":"<p>I have no obvious reason not to do it other than that we may want to wait for a second before we do it as the current state of the chart is slightly unstable. For example:</p> <p>https://github.com/aquasecurity/trivy-operator/discussions/1071</p>"},{"location":"adr/0038-replace-starboard-operator-with-trivy-operator/#links","title":"Links","text":"<ul> <li>Trivy-operator dashboard!</li> <li>Hot to request Pass and Fail</li> <li>CIS Kubernetes benchmark goes under ClusterComplianceReports.</li> <li>NSA, CISA Kubernetes Hardening Guidance v1.2</li> </ul>"},{"location":"adr/0039-application-dev-permissions/","title":"Application developer privilege elevation","text":"<ul> <li>Status: accepted</li> <li>Deciders: Olle, Christian</li> <li>Date: 2023-04-11</li> </ul>"},{"location":"adr/0039-application-dev-permissions/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Kubernetes comes with a set of default ClusterRoles that are meant to be user-facing. One such ClusterRole is <code>admin</code>. It is meant to be granted on a per-namespace basis, and it grants read and write permissions on most of the default namespaced resources. Welkin grants the <code>admin</code> ClusterRole in the appropriate namespaces to application developers. The <code>admin</code> ClusterRole is elevated with additional privileges through the use of the ClusterRole aggregation feature. Application developers are granted additional permissions for some Cluster-wide resources through the use of extra ClusterRoles and clusterRoleBindings.</p> <p>At times, application developers request access to additional resources not yet granted by Welkin, both concerning namespaced and Cluster-scoped resources. How should we manage such requests and when should the core permissions granted by Welkin be elevated?</p>"},{"location":"adr/0039-application-dev-permissions/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain a stable and secure platform.</li> <li>We want to adhere to our managed service customer needs.</li> </ul>"},{"location":"adr/0039-application-dev-permissions/#considered-options","title":"Considered Options","text":"<ol> <li>Never elevate privileges for application developers.<ul> <li>Application developer privileges are strictly tied to the running version of Welkin.</li> </ul> </li> <li>Elevate privileges on a case-by-case basis<ul> <li>Application developers can request additional privileges.   The request can be accepted or rejected by the Platform administrators.   If accepted, the elevated privileges should, if possible, be implemented into the core platform.</li> </ul> </li> </ol>"},{"location":"adr/0039-application-dev-permissions/#decision-outcome","title":"Decision Outcome","text":"<ul> <li>(2) Elevate privileges on a case-by-case basis</li> </ul>"},{"location":"adr/0039-application-dev-permissions/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Application developer is happy if the Platform administrators accept the request.</li> </ul>"},{"location":"adr/0039-application-dev-permissions/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Could potentially lead to feature sprawl unless the additional privileges are implemented into to the core platform.</li> </ul>"},{"location":"adr/0040-allow-group-id-0/","title":"Allow running containers with primary and supplementary group id 0","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-04-19</li> </ul>"},{"location":"adr/0040-allow-group-id-0/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Kubernetes has removed PodSecurityPolicy admission in version v1.25. The replacement, Pod Security admission, does not even in its most restricted profile limit what primary group id (Unix <code>gid</code>) or supplementary group id's (Unix <code>groups</code>) containers are allowed to run with. In terms of <code>securityContext</code> the following fields are not restricted:</p> <ul> <li><code>fsGroup</code></li> <li><code>runAsGroup</code></li> <li><code>supplementaryGroups</code></li> </ul> <p>With our current Kubernetes installer, Kubespray, the default <code>restricted</code> PodSecurityPolicy does not allow using id 0 for any of the <code>securityContext</code> fields listed above.</p> <p>Should we with the introduction of Pod Security admission follow the new standard of allowing group id 0, or should we continue to restrict it through some 3rd party admission webhook?</p>"},{"location":"adr/0040-allow-group-id-0/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security.</li> <li>For user expectations, we want to make it easy to start with Welkin.</li> </ul>"},{"location":"adr/0040-allow-group-id-0/#considered-options","title":"Considered Options","text":"<ol> <li>Allow group id 0 by default - default behavior of PSA.</li> <li>Keep current behavior and only allow group id 0 upon request.</li> </ol>"},{"location":"adr/0040-allow-group-id-0/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: 1 - \"Allow group id 0 by default - default behavior of PSA\".</p>"},{"location":"adr/0040-allow-group-id-0/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We follow upstream Kubernetes restricted Pod Security Standards.   With PodSecurityPolicy we use the one provided by Kubespray which has changed over time from first allowing group id 0 to now in its latest iteration not allowing it.</li> <li>Application Developers can run their containers inspired by the OpenShift pattern for supporting arbitrary user IDs etc.</li> </ul>"},{"location":"adr/0040-allow-group-id-0/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>It can be argued that in the event of an container escape vulnerability, our security is slightly weakened. However, a non-root user with group 0 is still not a privileged user.</li> </ul>"},{"location":"adr/0041-encryption-at-rest/","title":"Rely on Infrastructure Provider for encryption-at-rest","text":"<ul> <li>Status: accepted</li> <li>Deciders: arch meeting</li> <li>Date: 2023-05-11</li> </ul>"},{"location":"adr/0041-encryption-at-rest/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>A reason why people want encryption-at-rest is to add another safeguard to data confidentiality. Encryption-at-rest is a must have for phones and laptops as they can easily be stolen or get lost. However, in our opinion, it is a nice-to-have addition for servers, which are supposed to be in a physically protected and secure data-center, with the disks being safely disposed. The same should be true if the Infrastructure Provider offers object storage - the disks comprising the storage layer should be nicely tucked away deep inside a secure data-center.</p> <p>Anyhow, we are sometimes asked by Application Developers on why we don't simply do full-disk encryption at the VM level, using something like <code>cryptsetup</code>, or why we don't always encrypt data before it is shipped to object storage. Furthermore, some Application Developers require that all data is encrypted-at-rest. How should we in Welkin handle encryption-at-rest both for VMs and object storage?</p>"},{"location":"adr/0041-encryption-at-rest/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to avoid security theatre.</li> <li>We want to avoid operational complexity.</li> <li>We want to avoid cloud-provider dependent implementation sprawl.</li> <li>We want to make Application Developers that require encryption-at-rest happy.</li> </ul>"},{"location":"adr/0041-encryption-at-rest/#considered-options","title":"Considered Options","text":"<p>VM disk encryption:</p> <ol> <li>Store the encryption key on the VM's /boot disk.     This is obvious security theatre. For example, if server disks are stolen, the VM's data is in the hands of the thief.</li> <li>Let admins type the encryption key on the VM's console.     Asking admins to do this is time-consuming, error-prone, effectively jeopardizing uptime. Instead, Welkin recommends automatic VM reboots during application \"quiet times\", such as at night, to ensure the OS is patched without sacrificing uptime.</li> <li>Let the VM pull the encryption key via instance metadata or instance configuration.     This would imply storing the encryption key on the Infrastructure Provider. If the Infrastructure Provider doesn't have encryption-at-rest, then the encryption key is also stored unencrypted, likely on the same server as the VM is running. Hence, this quickly ends up being security theatre.</li> <li>Let the VM pull the encryption key from an external location which features encryption-at-rest.     This would imply that the VM needs some kind of credentials to authenticate to the external location. Again these credentials are stored unencrypted on the Infrastructure Provider, so we are back to option 3.</li> <li>Rely on Infrastructure Provider to provide encryption-at-rest.</li> </ol> <p>Object storage encryption:</p> <ol> <li>Encrypt data before shipping to object storage.     This is not doable since not all applications that interacts with object storage in Welkin supports this.     Using a proxy gateway that can handle the encryption could have been a solution, however the only tool that we've found, MinIO Gateway, has been deprecated.</li> <li>Encrypt data using server-side-encryption.     In Welkin, we use Openstack Swift and the S3 API when interacting with object storage.     Openstack swift has no notion of server-side encryption where the data is encrypted at rest with a user provided key.     The S3 API has server-side encryption but again some applications in Welkin lacks the features necessary to leverage it.</li> <li>Rely on Infrastructure Provider to provide encryption-at-rest.</li> </ol>"},{"location":"adr/0041-encryption-at-rest/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen options:</p> <ul> <li>VM disk encryption: \"Rely on Infrastructure Provider to provide encryption-at-rest\".</li> <li>Object storage encryption: \"Rely on Infrastructure Provider to provide encryption-at-rest\".</li> </ul>"},{"location":"adr/0041-encryption-at-rest/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We can offer encryption-at-rest for Application Developers that require it.</li> <li>We don't increase the operational complexity.</li> <li>We avoid security theatre.</li> </ul>"},{"location":"adr/0041-encryption-at-rest/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>We limit what Infrastructure Providers Application Developers can choose if they require encryption-at-rest.</li> </ul>"},{"location":"adr/0042-argocd-dynamic-hnc-namespaces/","title":"ArgoCD with dynamic HNC namespaces","text":"<ul> <li>Status: accepted</li> <li>Deciders: product owner, DX, GoTo</li> <li>Date: 2023-05-23</li> </ul>"},{"location":"adr/0042-argocd-dynamic-hnc-namespaces/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Argo CD cannot create HNC namespaces and deploy services into them. Our current Argo offering is a namespaced installation where a CronJob patches the Cluster secret with the list of managed Namespaces every minute. An issue occurs when an Application Developer tries to create an Application which also includes a Namespace. As the new not yet created/patched to the Cluster secret namespace is not managed, Argo CD fails at the comparison stage.</p> <p>Currently, upstream ArgoCD is unaware of HNC resources, and does not give subnamespaces the needed precedence, read more here. The Cluster secret that takes a list of managed namespaces, does not take a regular expression, so it is hard for Application Developers to create namespaces as part of an Application.</p> <p>So, do we want to let Application Developer create and delete subnamespaces dynamically via Argo CD? If yes, how?</p>"},{"location":"adr/0042-argocd-dynamic-hnc-namespaces/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes Platform Administrator burden.</li> <li>We want to best serve our Application Developers.</li> <li>We want to make the operator life easier.</li> </ul>"},{"location":"adr/0042-argocd-dynamic-hnc-namespaces/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Wait for the Argo Project to integrate better with HNC and ask customers to only create subnamespaces manually until this is fixed upstream.</p> <ul> <li>Good, because we are inline with how upstream Argo is built.</li> <li>Bad, because this will not give customers a smooth GitOps experience.</li> <li>Bad, because it makes it hard to use ApplicationSets to dynamically create Applications on new namespaces.</li> </ul> </li> <li> <p>Use OPA to restrict operations on our namespaces and give customers the ability to create/delete namespaces?</p> <ul> <li>Bad, because giving Application developers access to create/delete namespaces has been a no-go since the beginning of Welkin for various security reasons.</li> <li>Bad, because building OPA policies to restrict operations on namespaces, would take away from how community does things and makes it hard for us to pivot to new big changes in the future.</li> </ul> </li> <li> <p>To deploy all the Applications into one big namespace.</p> <ul> <li>Bad, because proposing developers to deploy different versions of their components within the same namespace goes against the goal of achieving separation and isolation through namespaces.</li> </ul> </li> <li> <p>Sync Waves and Phases.</p> <ul> <li>We investigated this, but found that sync waves are not a solution in this case, as the reconciliation is failing at the comparison stage which happens before sync waves are executed.</li> </ul> </li> <li> <p>Test to see if ArgoCD accepts regex in Cluster secret.</p> <ul> <li>We investigated this, but found that the secret does not accept wildcards or regex. See the open issue here.</li> </ul> </li> <li> <p>Setup Argo CD Cluster-wide installation.</p> <ul> <li>Bad, because Cluster-wide installation would give Argo a lot of permissions and right now, there is no good solution to stop Argo from deploying applications into our system namespaces such as Falco, gatekeeper-system, etc., assuming that one needs to use a wildcard as destinations in ArgoCD projects.</li> <li>Bad, because even if Argo CD is installed Cluster-wide, When ArgoCD syncs by kind, it does not prioritize subnamespaces first. See here.</li> <li>Bad, because this would also require us to build a lot of OPA policies and later makes it hard to pivot to new ways.</li> </ul> </li> </ol>"},{"location":"adr/0042-argocd-dynamic-hnc-namespaces/#decision-outcome","title":"Decision Outcome","text":"<p>The decision is to let the Argo CD project adopt practices around HNC. Until then:</p> <ul> <li>Application Developers need to create subnamespaces manually and deploy applications into it.</li> <li>Application Developers cannot template the namespace as a value in their manifests.</li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/","title":"Rclone and Encryption adheres Cryptography Policy","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2023-05-04</li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Welkin can be configured to replicate the primary backup to a secondary backup on a second object storage. Under the hood, this is performed using <code>rclone</code>. Such replication improves disaster recovery and mitigates ransomware attacks.</p> <p>However, this second object storage sometimes needs to be on a different Infrastructure Provider and/or in a different country. In such cases, it is desirable to make the secondary backup completely opaque to the Infrastructure Provider, so that they do not count as a new data sub-processor.</p> <p>Is <code>rclone</code>'s encryption sufficient for our purposes? Does it comply with the recommended cryptography policy?</p>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes Platform Administrator burden.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> <li>We want to avoid Infrastructure Provider dependent implementation sprawl.</li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Use <code>rclone</code> to replicate the data from our <code>primary</code> Infrastructure Provider to <code>secondary</code> public Infrastructure Provider on a second object storage.</p> <ul> <li><code>Good</code>, because it supports our goals of data redundancy, privacy, compliance, and cross-infrastructure-provider flexibility.</li> <li><code>Bad</code>, because in some jurisdictions and regulatory frameworks, Infrastructure Providers might be considered data sub-processors, which could impose additional requirements and compliance obligations.</li> </ul> </li> <li> <p>Use <code>rclone</code> to replicate the data from our primary region to secondary region (outside Sweden) within the same public Infrastructure Provider on a second object storage.</p> <ul> <li><code>Good</code>, because it supports our goals of data redundancy, resilience, disaster recovery preparedness, privacy, compliance, encrypted data transfer, and cross-region accessibility.</li> </ul> </li> <li> <p>Use <code>rclone</code> to replicate the data from our <code>primary</code> Infrastructure Provider to <code>secondary</code> Compliant Infrastructure Provider on a second object storage.</p> <ul> <li>We investigated this with Infrastructure Providers, but found that there is no good way to enable communication between public Infrastructure Provider object storage and compliant Infrastructure Provider object storage.</li> </ul> </li> <li> <p>Do nothing and accept the risk of data loss.</p> <ul> <li><code>Bad</code>, because not implementing data replication leaves our services vulnerable to data loss in case of hardware failures, system errors, or accidental deletions, and recovery options become limited.</li> </ul> </li> </ol>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option:</p> <ul> <li> <p>Use <code>rclone</code> to replicate the data from our primary region to secondary region (outside Sweden) within the same public Infrastructure Provider on a second object storage.</p> </li> <li> <p>As a result, and as per the ECRYPT-CSA report, We concluded that the <code>Salsa20</code> and <code>Poly1305</code> ciphers comply with our use of the Cryptography Policy and we can use the encryption feature in <code>rclone</code> for our replication strategy.</p> </li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We have <code>Data Redundancy</code> and <code>Resilience</code> that provides a safeguard against data loss due to hardware failures, natural disasters, or other unforeseen events.</li> <li>We don't increase the operational complexity.</li> <li>We avoid security theatre.</li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#recommendations-to-platform-administrators","title":"Recommendations to Platform Administrators","text":"<ul> <li>Platform Administrator should encrypt the backups before sending to an off-site location outside of Sweden and use the encryption feature in <code>rclone</code> which adheres to our cryptography policy.</li> </ul>"},{"location":"adr/0043-rclone-and-encryption-adhere-cryptography-policy/#links","title":"Links","text":"<ul> <li>XSalsa20 with 192-bit nonce</li> <li>ECRYPT-CSA report</li> <li>Configuration Encryption</li> </ul> <ul> <li>secretbox</li> </ul>"},{"location":"adr/0044-argocd-managing-its-own-namespace/","title":"ArgoCD is not allowed to manage its own namespace","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2023-10-12</li> </ul>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Currently, ArgoCD is setup in namespaced mode. We give it a list of Namespaces that ArgoCD should manage and a list of permissions that it has on those namespaces through ArgoCD's inclusions and Kubernetes RBAC to the service account to back this. This service account has a larger set of permissions than a developer's most privileged accounts do.</p> <p>If an Application Developer wants to deploy ArgoCD resources such as <code>apps</code>, <code>applicationSets</code>, and <code>appProjects</code> into the <code>argocd-system</code> namespace via ArgoCD itself, it will fail since <code>argocd-system</code> is not a managed namespace in ArgoCD. This restriction prevents users from using features like Apps of Apps.</p> <p>Do we want to make <code>argocd-system</code> a managed namespace in our ArgoCD offering?</p>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes Platform Administrator burden.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> <li>We want to avoid Infrastructure Provider dependent implementation sprawl.</li> </ul>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Yes, we make ArgoCD manage its own namespace.</p> </li> <li> <p>No, we do not make ArgoCD manage its own namespace.</p> </li> </ol>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option:</p> <ul> <li>No, we do not allow ArgoCD to manage its own namespace.<ul> <li>ArgoCD, through an Application Developer's configuration, should not deploy standard Kubernetes resources (such as Pods or secrets) directly into its own namespace. If there is a requirement for such deployments, it should be initiated through a service ticket, ensuring that it undergoes thorough security and stability assessments to prevent any compromises to the platform.</li> </ul> </li> </ul>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>We get to keep ArgoCD secure.</li> <li>A malicious user cannot exploit a potential bug in ArgoCD to deploy resources into <code>argocd-system</code> namespace.</li> <li>An Application Developer will not be able to deploy a malicious Pod and read secrets in <code>argocd-system</code> namespace.</li> </ul>"},{"location":"adr/0044-argocd-managing-its-own-namespace/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>An Application Developer may also not be able to deploy standard Kubernetes resources via ArgoCD into <code>argocd-system</code> namespaces.<ul> <li>This should not count as a negative consequence, because our current security stance is that Application Developers should not be supposed to deploy into the <code>argocd-system</code> Namespace at all.</li> </ul> </li> <li>Features such as Apps of Apps will not be available in our offering.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/","title":"Use specialised prebuilt images","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Architecture meeting</li> <li>Date: 2024-01-18</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Some of the upstream images we include rely on package or plugins managers to enable support during runtime for different infrastructure providers.</p> <p>This is a security concern as a malicious actor could modify the packages or plugins to compromise the workload, and a reliability concern as even a good actor could take down packages which would render the workload unavailable.</p> <p>To improve both the security and reliability of the platform we should therefore take better control over the workloads it includes to provide the images it needs, without external reliance on packages or plugins.</p>"},{"location":"adr/0045-use-specialised-prebuilt-images/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Improve the security of the platform by preventing malicious packages and plugins to be installed during runtime.</li> <li>Improve the reliability of the platform by ensuring required packages and plugins are provided in the images.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#considered-options","title":"Considered Options","text":"<ol> <li>Build specialised images one for all required configurations</li> <li>Build specialised images one for each required configuration</li> <li>Build no specialised images</li> </ol>"},{"location":"adr/0045-use-specialised-prebuilt-images/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option 2, since it will improve upon the two issues above and provide each configuration with images using minimal software.</p> <p>The tagging should follow the following scheme:</p> <pre><code>&lt;registry&gt;/&lt;repository&gt;/&lt;application&gt;:&lt;application-version&gt;-&lt;variant-identifier&gt;&lt;variant-version&gt;\n</code></pre> <p>With variant matching the configuration variant to support, example different object storage services AWS/S3, OpenStack/Swift, etc., and the version of the plugin supporting it.</p> <p>Example for Velero with AWS/S3 support:</p> <pre><code>ghcr.io/elastisys/compliantkubernetes-apps/velero:1.12.3-aws1.8.2\n</code></pre> <p>The application version should not have any version prefix and the variant identifier and variant version should not have any separator. This is similar to the version scheme used by the Docker Community and how they identify different base image variants.</p>"},{"location":"adr/0045-use-specialised-prebuilt-images/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Increased control over the software supply chain.</li> <li>Increased platform security and reliability.</li> <li>Starts the process to better manage, update, and verify images used as part of the platform.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Additional maintenance burden to manage images in addition to charts.</li> <li>Departs from the upstream charts and images.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0045-use-specialised-prebuilt-images/#option-1-build-specialised-images-one-for-all-required-configurations","title":"Option 1: Build specialised images one for all required configurations","text":"<ul> <li>Good, removes reliance on external packages and plugins during runtime.</li> <li>Bad, requires larger images that might contain vulnerabilities that affect all configurations.</li> <li>Bad, requires modifications to upstream images and potentially charts to support it.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#option-2-build-specialised-images-one-for-each-required-configuration","title":"Option 2: Build specialised images one for each required configuration","text":"<ul> <li>Good, removes reliance on external packages and plugins during runtime.</li> <li>Good, allows minimal images that might isolate vulnerabilities to only affect certain configurations.</li> <li>Bad, requires modifications to upstream images and potentially charts to support it.</li> </ul>"},{"location":"adr/0045-use-specialised-prebuilt-images/#option-3-build-no-specialised-images","title":"Option 3: Build no specialised images","text":"<ul> <li>Bad, requires external packages and plugins during runtime.</li> <li>Good, allows use of upstream images and charts as is.</li> </ul>"},{"location":"adr/0046-handle-crds/","title":"Handle all CRDs with the standard Helm CRD management","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2024-02-22</li> </ul>"},{"location":"adr/0046-handle-crds/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>How should we handle CRDs?</p> <p>We previously decided on letting upstream projects handle CRDs in any way they please as to decrease our own maintenance efforts. This decision has since proved to be flawed, due to the way some upstream project has decided to handle CRDs.</p> <p>Helm 3 has support for installing CRDs, but not upgrading them, but not all upstream projects use this way of installing CRDs. Instead, they include the CRDs as regular resources in the chart, meaning that there is no way of deleting the installed chart release without also removing CRDs and any associated Custom Resource objects.</p> <p>One example of an issue that this can cause is that both our Apps and Cluster API uses cert-manager to provision certificates. If one were to uninstall cert-manager using our Apps, they would lose required certificate resources that Cluster API needs for its Cluster management. A reinstall of cert-manager with Apps would not reinstall these specific Cluster API certificates. This is an issue since we do not want Apps to mess with the underlying infrastructure layer, be it Kubespray or Cluster API.</p>"},{"location":"adr/0046-handle-crds/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We need to manage CRDs in a way that is consistent and that allows us to perform necessary actions in Apps without unintended effects on the environment.</li> <li>We want to maintain separation of concerns between the Kubespray / Cluster API layer and our Apps layer.</li> </ul>"},{"location":"adr/0046-handle-crds/#considered-options","title":"Considered Options","text":"<ul> <li>Option 1: Make cert-manager opt-in in Apps so that we can disable it for CAPI Clusters, and let CAPI handle it.</li> <li>Option 2: Improve Apps handling of CRDs, avoiding removing CRDs when uninstalling / cleaning apps, using the way Helm 3 recommends.</li> <li>Option 3: Make use of CAPI Operator to handle missing certs.</li> </ul>"},{"location":"adr/0046-handle-crds/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Option 2\". We decided to supersede ADR-0011 and instead ensure that all charts handle CRDs in the way that Helm 3 recommends. This allows us to remove any app without risking it also deleting CRDs so that there is not risk of us accidentally deleting resources and losing data.</p>"},{"location":"adr/0046-handle-crds/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>CRDs will be handled in the same way independent of which project / app it is created by.</li> <li>Decreases the risk of accidentally deleting Custom Resources and leaving environments in undefined states.</li> </ul>"},{"location":"adr/0046-handle-crds/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Will require increased maintenance and development efforts from us to properly handle CRDs for all apps.</li> </ul>"},{"location":"adr/0047-kubernetes-versions/","title":"When to upgrade to new Kubernetes versions","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Architecture meeting</li> <li>Date: 2024-03-07</li> </ul>"},{"location":"adr/0047-kubernetes-versions/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>With the addition of Cluster API we need to take decisions on when we want to release a new minor version of Kubernetes. Cluster API is a fast moving project, and it would allow us to support new minor versions of Kubernetes almost directly after they've been released. However immediately upgrading to a new minor version could potentially introduce bugs or regressions not yet found and patched upstream.</p> <p>For Kubespray we have not needed to take the same decision, as we follow when Kubespray releases a new minor version, which then includes a new minor version of Kubernetes. Kubespray is not as fast moving, and when it releases a new minor version it includes a new minor version of Kubernetes that is already a few patches in. This makes the potential of it introducing bugs and regressions much smaller as they should have been found and patched upstream at that point.</p> <p>But what should be our strategy be to upgrade Kubernetes with Cluster API?</p>"},{"location":"adr/0047-kubernetes-versions/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to ensure the Kubernetes version is up to date</li> <li>We want to ensure the Kubernetes version is stable</li> <li>We want to reduce maintainer effort on keeping Cluster API and Kubespray equivalent.</li> </ul>"},{"location":"adr/0047-kubernetes-versions/#considered-options","title":"Considered Options","text":"<ol> <li>Upgrade as soon as there is a new minor version</li> <li>Upgrade when Kubespray upgrades</li> </ol>"},{"location":"adr/0047-kubernetes-versions/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: option 2, because we want to keep our Kubernetes installers in the same pace when it comes to new Kubernetes minor versions, and then we know that this version will be more thoroughly tested and patched upstream.</p>"},{"location":"adr/0047-kubernetes-versions/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Ensures our Kubernetes version is more stable</li> <li>Reduces our maintainer effort as we keep Cluster API and Kubespray equivalent.</li> </ul>"},{"location":"adr/0047-kubernetes-versions/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Slower adoption of new versions in Cluster API<ul> <li>Mitigator 1: rework patch process to accelerate the release of new patch versions</li> <li>Mitigator 2: keep main branch of Cluster API aligned with latest minor version</li> </ul> </li> </ul>"},{"location":"adr/0047-kubernetes-versions/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0047-kubernetes-versions/#option-1-upgrade-as-soon-as-there-is-a-new-minor-version","title":"Option 1: Upgrade as soon as there is a new minor version","text":"<ul> <li>Good, because Cluster API will be up to date</li> <li>Bad, because new minor Kubernetes versions may introduce bugs and regressions as it has not had the time to be tested by the community at large</li> <li>Bad, because we get a version skew between Cluster API and Kubespray</li> </ul>"},{"location":"adr/0047-kubernetes-versions/#option-2-upgrade-when-kubespray-upgrades","title":"Option 2 - Upgrade when Kubespray upgrades","text":"<ul> <li>Bad, because Cluster API will not be fully up to date<ul> <li>Mitigator 1: rework patch process to accelerate the release of new patch versions</li> <li>Mitigator 2: keep main branch of Cluster API aligned with latest minor version</li> </ul> </li> <li>Good, because new minor Kubernetes versions will be more tested by the community at large</li> <li>Good, because we do not get a version skew between Cluster API and Kubespray</li> </ul>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/","title":"Access Management for Additional Managed Services (AMS-es)","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2024-01-19</li> </ul>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Elastisys is working around the clock to improve the security of our platform, so as to allow Application Developers to better protect Personal Data. Currently, we don't have the access management for the AMS-es i.e. how these are supposed to be accessed by Pods that are deployed by Application Developers.</p> <p>We are considering Network Policies for AMS-es to enhance security and control. A key aspect of this change concerns how these services, such as our additional managed Redis or PostgreSQL, etc. instances, interact with applications deployed by Application Developers.</p> <p>How should the access be managed and how should it be communicated to the Application Developers so that we have a smooth transition?</p>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes Platform Administrator burden.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Allow Specific AMS-es Namespace Labels</p> <ul> <li><code>Good</code>, because we have precise control over which namespaces can access the AMS-es, enhancing security.</li> <li><code>Good</code> because labels clearly indicate intent for access, making policy enforcement and audits simpler.</li> <li><code>Bad</code>, because it requires manual labelling of namespaces, which can be cumbersome in large environments.</li> </ul> </li> <li> <p>Give Application Developers a heads-up before deploying the new AMS-es Release with Network Policies enabled and let Application Developers label the namespaces they Want to communicate with AMS-es.</p> <ul> <li><code>Good</code>, because Application Developers will have the freedom to choose which namespaces need access, promoting responsibility and autonomy.</li> <li><code>Good</code>, because notification from us will allow developers to prepare and avoid sudden access issues upon AMS-es Deployment.</li> <li><code>Bad</code>, because some developers might overlook the communication, leading to inconsistencies in access management.</li> </ul> </li> <li> <p>Still Give Application Developers a heads up but we label all the Application Developer namespaces that already exist and let them add and remove labels after the Deployment.</p> <ul> <li><code>Good</code>, because it ensures all current applications maintain access post-Deployment of AMS-es new release, preventing service disruptions.</li> <li><code>Bad</code>, because initially grants broad access, potentially exposing AMS-es to namespaces that no longer require it.</li> </ul> </li> <li> <p>Allow Application Developer Namespace Labels</p> <ul> <li><code>Good</code>, because it simplifies the process for developers, as they can use existing namespace labels without needing specific AMS-es labels.</li> <li><code>Bad</code>, because it does not offer fine-grained access control specifically for AMS-es, potentially leading to broader than necessary access.</li> </ul> </li> <li> <p>Allow All Namespaces in Cluster</p> <ul> <li><code>Good</code>, because it ensures all namespaces have immediate access to AMS-es, simplifying connectivity and integration.</li> <li><code>Bad</code>, because it significantly increases the risk of unauthorized access, making AMS-es vulnerable to potential breaches.</li> </ul> </li> </ol>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#decision-outcome","title":"Decision Outcome","text":"<p>Access Management Strategy:</p> <p>Chosen option 1 &amp; 2 i.e. We will allow specific AMS-es Pod labels, specifically <code>elastisys.io/&lt;ams-name&gt;-&lt;ams-name-cluster-name&gt;-access: allow</code> and Application developers need to add the labels to their existing application Pods with <code>elastisys.io/&lt;ams-name&gt;-&lt;ams-name-cluster-name&gt;-access: allow</code> to ensure continuity.</p> <p>Example- <code>elastisys.io/redis-&lt;redis-cluster-name&gt;-access: allow</code>.</p> <p>This label will enable communication between Application Developers application Pods and the designated AMS-es.</p> <p>For New Additional Managed Services (AMS-es) Clusters Ordered:</p> <p>Network Policy will be enabled by default. Application Developers will need to actively label the application Pods with <code>elastisys.io/&lt;ams-name&gt;-&lt;ams-name-cluster-name&gt;-access: allow</code> to gain access to our AMS-es.</p>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>There will be no sudden disruption in communication between Application Developers applications and our AMS-es.</li> <li>We don't increase the operational complexity.</li> <li>We avoid the security theatre.</li> <li>This approach not only enhances security but also gives Application Developers greater control over which applications can communicate with our managed services. Among others, this facilitates compliance with:<ul> <li>GDPR Art. 32 \u201cSecurity of Processing\u201d;</li> <li>ISO 27001:2022 Annex A Control 8.22 \u201cSegregation of Networks\u201d.</li> </ul> </li> </ul>"},{"location":"adr/0048-access-management-for-AMS-with-network-policies/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Initial overhead for Application Developers.</li> <li>With some responsibility shifted to Application Developers, there's a risk of misconfiguration like incorrectly labelled namespaces could deny access to AMS-es.</li> </ul>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/","title":"Running NGINX with Chroot Option","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2023-12-23</li> </ul>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Elastisys is continuously working to improve the security of our platform to better protect Personal Data for Application Developers. Currently, the <code>ingress-nginx</code> controller has the ability to list all secrets in the environment, which poses a security risk.</p> <p>We needed a way to restrict NGINX's access to only necessary TLS secrets. The initial approach to use Gatekeeper constraints was not feasible due to the Kubernetes admission API's limitations as it doesn't check for <code>READ (get/list/watch)</code> operations.</p> <p>Should we consider using <code>Role-Based Access Control (RBAC)</code> and potentially running NGINX in a <code>chroot</code> mode for enhanced security?</p>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to find a solution which is scalable and minimizes the Platform Administrator burden.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Write an Operator for RBAC Management</p> <ul> <li><code>Good</code>, because it provides precise control over which secrets NGINX can access, enhancing security.</li> <li><code>Good</code> because it automatically applies RBAC policies, reducing manual configuration and potential errors.</li> <li><code>Bad</code>, because it requires development and maintenance of a custom operator, which is complex and time-consuming.</li> </ul> </li> <li> <p>Use Namespaced RoleBindings</p> <ul> <li><code>Good</code>, because it is easier and faster to implement compared to developing an operator.</li> <li><code>Bad</code>, because it requires manual configuration of RoleBindings, which can be cumbersome in large environments and prone to human error.</li> <li><code>Bad</code>, because it increases overhead as namespaces need to be manually managed and updated.</li> </ul> </li> <li> <p>Run Ingress-NGINX in chroot with custom seccomp profile</p> <ul> <li><code>Good</code>, because attackers can not exploit recent CVEs to access secrets Cluster-wide.</li> <li><code>Bad</code>, because if someone were to break out of the chroot and attack the controller itself, they would have access to the <code>clone</code> and <code>unshare</code> syscalls on the host. This allows the attacker to create threads on the host as well as any type of namespace. It is not entirely clear what kind of harm this could cause.</li> </ul> </li> </ol>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option <code>3</code> i.e We will run <code>ingress-nginx</code> in a <code>chroot</code> environment to limit its access to the host system, improving security and also, maintaining the custom Ingress-NGINX chroot <code>seccomp profile</code>.</p> <p>The solution proposed has already been implemented and will be available from app version <code>0.35</code> onwards.</p>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Attackers can not exploit recent CVEs to access secrets Cluster-wide.</li> <li>Increased platform security and reliability.</li> <li>We avoid the security theater.</li> </ul>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Initial overhead for Platform Administrators.</li> </ul>"},{"location":"adr/0049-run-ingress-nginx-in-chroot/#links","title":"Links","text":"<ul> <li>Exploiting CVE-2023-5044</li> <li>Kubernetes Blog on Ingress-NGINX 1.2.0</li> <li>Elastisys/compliantkubernetes-apps#1854</li> </ul>"},{"location":"adr/0050-use-cluster-isolation/","title":"Use Cluster Isolation to separate the application and its traces from its logs and metrics","text":"<ul> <li>Status: accepted</li> <li>Deciders: Product Team</li> <li>Date: 2024-08-16</li> </ul>"},{"location":"adr/0050-use-cluster-isolation/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Let us introduce the context of this ADR. First, we look at the regulatory and information security landscape. Second, we look at the technological landscape. Finally, we merge these two discussions to devise the problem statement.</p>"},{"location":"adr/0050-use-cluster-isolation/#regulatory-and-information-security-landscape","title":"Regulatory and Information Security Landscape","text":"<p>The NIS2 Directive came into force 2023 and will affect how platforms hosting software critical to society are secured. Article 21 lists 10 cybersecurity risk-management measures which all essential and important entities will need to implement.</p> <p>This ADR focuses on the following two measures, reproduced below verbatim from the NIS2 Directive:</p> <ul> <li>(e) security in network and information systems acquisition, development and maintenance, including vulnerability handling and disclosure;</li> <li>(i) human resources security, access control policies and asset management.</li> </ul> <p>The most common way to implement these measures is using a concept called Security Zone. A Security Zone is a logical grouping of IT systems with similar needs of protection. For example, one Security Zone hosts mission-critical applications, whereas another Security Zone hosts non-mission-critical applications.</p> <p>Each Security Zone has a separate network, either isolated physically (OSI layer 1) or logically (OSI layer 2, e.g., VLAN). The Internet itself can be conceptualized as a Security Zone with the lowest protection class. Network traffic can only travel between Security Zones through firewalls. Usually, connections are only allowed from Security Zones with higher protection class to those with lower. As an additional measures, applications and protocols are designed to ensure that:</p> <ul> <li>A compromise of information availability and integrity of a Security Zone with lower protection class does not affect a Security Zone with a higher protection class. For example, updates from the Internet need to pass through a manual review and validation process on a non-active production environment before being applied to the active production environment.</li> <li>Confidential information is not leaked from a Security Zone with higher protection class into one with lower protection class. For example, application logs required for diagnostics may traverse from a higher to lower class. This, of course, entails that adequate requirements are set on the application development process, such as, logs should contain not confidential information. However, application traces, given that they may contain function call parameters and hence pose a higher risk of leaking confidential information, should remain in the Security Zone with the higher protection class.</li> </ul> <p>Besides network isolation, Security Zones with higher protection class may be accessed only by staff with a given security clearance. For example, the Security Zone in the highest protection class may be accessed only by staff who cleared the Swedish S\u00c4PO Security Clearance or the German S\u00dcG. In contrast, Security Zones in a lower protection class may be accessed by the staff of suppliers, application developers, platform support staff, etc.</p>"},{"location":"adr/0050-use-cluster-isolation/#technological-landscape","title":"Technological Landscape","text":"<p>Somewhat simplified, a containerized platform, such as Welkin, hosts the application stack and observability stack. The three pillars of observability are metrics, logs and traces. Metrics and logs are rather \"explicit\" in nature, i.e., the application developer usually add code to their application to explicitly decide what metrics and what log lines the application produces. Traces are rather \"implicit\" in nature, i.e., the application developer includes a library in their application and some code snippet, which automatically produces traces of all function calls and returns, including invoked parameters.</p> <p>The application and observability stack may be separated through various levels of isolation: labels, namespace isolation, Node isolation and Cluster isolation. See Levels of Isolation for a description of each isolation level and what kind of isolation they achieve.</p>"},{"location":"adr/0050-use-cluster-isolation/#problem-statement","title":"Problem Statement","text":"<p>Given the regulatory and information security landscape above, what level of isolation should Welkin employ between the application stack and observability stack?</p>"},{"location":"adr/0050-use-cluster-isolation/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to make it easy to protect code which runs in Security Zones with higher protection class, even if that means a reduction in productivity.</li> <li>We want to protect code which runs in Security Zones with lower protection class, but a reduction in productivity should be minimized.</li> <li>We want to avoid \"Cluster sprawl\" which increases maintenance burden.</li> </ul>"},{"location":"adr/0050-use-cluster-isolation/#considered-options","title":"Considered Options","text":"<ol> <li>Use Namespace isolation between application and observability stack.<ul> <li>Good, because it avoids Cluster sprawl.</li> <li>Bad, because Security Zones are only enforced using NetworkPolicies, which is usually insufficient for high-security organizations.</li> </ul> </li> <li>Use Node isolation between application and observability stack.<ul> <li>Good, because it avoids Cluster sprawl.</li> <li>Good, because each Node may run in a different Security Zone.</li> <li>Bad, because Kubernetes and its CNI tends to assume non-restricted communication between Nodes. The resulting firewall rules might look very generous and hard to audit.</li> <li>Bad, because the Kubernetes control plane is shared between Security Zones, which violates the spirit of zoning.</li> </ul> </li> <li>Use Cluster isolation between application and observability stack.<ul> <li>Good, because each Cluster can run in a different Security Zone.</li> <li>Good, because the application and observability stack can be in different Security Zones.</li> <li>Bad, because traces end up in the same Security Zone as logs and metrics.</li> </ul> </li> <li>Use Cluster isolation between application and observability stack. Host the application and its traces in one Cluster, while the other Cluster hosts logs and metrics.<ul> <li>Good, because it avoids Cluster sprawl, i.e., only two Clusters, instead of the minimum of one.</li> <li>Good, because it follows the Security Zone model.</li> </ul> </li> <li>Use Cluster isolation with one Cluster for each of application, traces, logs and metrics.<ul> <li>Neutral, because, compared to the option above, it does allow more Security Zones. However, many Security Zones are likely to be of the same protection class, hence, the benefits in terms of security does not outweigh the cost.</li> <li>Bad, because it leads to Cluster sprawl.</li> </ul> </li> </ol>"},{"location":"adr/0050-use-cluster-isolation/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"Option 4: Host the application and its traces in one Cluster, while the other Cluster hosts logs and metrics\", because it provides the right tradeoff between number of Security Zones and Cluster sprawl.</p> <p></p>"},{"location":"adr/0050-use-cluster-isolation/#other-considerations","title":"Other Considerations","text":""},{"location":"adr/0050-use-cluster-isolation/#glossary-used-in-welkin","title":"Glossary used in Welkin","text":"<p>Welkin uses the following glossary:</p> <ul> <li>The Cluster hosting the application and traces is called the Workload Cluster.</li> <li>The Cluster hosting logs and metrics is called the Management Cluster.</li> </ul> <p>The pair of Cluster form a Welkin Environment.</p>"},{"location":"adr/0050-use-cluster-isolation/#automated-platform-updates-via-tekton","title":"Automated Platform Updates via Tekton","text":"<p>If the Workload Cluster is in a Security Zone with a high protection class, then we recommend against using auto-updates with Tekton. Prefer running the migration scripts manually for added human supervision. See ADR-0035 Run Tekton on Management Cluster.</p>"},{"location":"adr/0050-use-cluster-isolation/#cluster-api","title":"Cluster API","text":"<p>If the Workload Cluster is in a Security Zone with a higher protection class than the Management Cluster, then the Workload Cluster should run its own Cluster API controller. This goes against ADR-0033 Run Cluster API controllers on Management Cluster.</p>"},{"location":"adr/0050-use-cluster-isolation/#tamper-proof-logging","title":"Tamper-Proof Logging","text":"<p>Some regulations and information security standards require tamper-proof logging. In other words, a compromise of the application Deployment should not enable an attacker to remove their trails and hinder forensics. Welkin already observes this principle for its observability stack, e.g., the access that the Workload Cluster has to the Service Cluster cannot be used to remove old log entries. (Of course, garbage new log entries may be created, but that only makes an attack more obvious during log review.</p> <p>Cluster isolation between the application and its logs adds another layer of protection: The attacker would need to:</p> <ul> <li>compromise the application;</li> <li>compromise the underlying Workload Cluster to reach log collection components (Fluentd);</li> <li>compromise the OpenSearch endpoint on the Service Cluster.</li> </ul>"},{"location":"adr/0050-use-cluster-isolation/#links","title":"Links","text":"<ul> <li>NIS2 Directive</li> <li>Proposal for an implementation of the NIS2 Directive in Sweden</li> <li>This is the NIS2 Directive from the Swedish Civil Contingencies Agency</li> <li>NIS2 in Germany</li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/","title":"Open cert-manager Network Policies","text":"<ul> <li>Status: accepted</li> <li>Deciders: Product Team</li> <li>Date: 2024-08-22</li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Certificates is a critical component of any web-based application, as it asserts trust and security for the End User.</p> <p>Setting up certificates require a few fundamental things which is different depending on if the issuer is using DNS-01 challenges or HTTP-01 challenges:</p> <p>For DNS-01 challenges one needs:</p> <ul> <li>The issuer must be configured with correct credentials for the DNS provider.<ul> <li>Application developer responsibility</li> </ul> </li> <li>The Network Policy must be configured to allow cert-manager controller access to the DNS provider, for both API and DNS.<ul> <li>Platform administrator responsibility</li> </ul> </li> </ul> <p>For HTTP-01 challenges one needs:</p> <ul> <li>The certificate domain must point towards the Ingress-controller of the Cluster.<ul> <li>Application developer responsibility.</li> </ul> </li> <li>The Network Policy must be configured to allow cert-manager controller access to the Ingress-controller of the Cluster, and the Ingress-controller access to the cert-manager resolver.<ul> <li>Platform administrator responsibility.</li> </ul> </li> </ul> <p>This means that there is a shared responsibility. For DNS-01 challenges this may cause complete failure of certificate issuing if platform administrators are not updated on the issuer application developers are using. And for HTTP-01 challenges this may cause frequent alerts if certificates are misconfigured regarding packets blocked by Network Policies which may hide potentially critical issues.</p> <p>The certificates setup by application developers should be application developers full responsibility to keep correctly configured and valid. However the platform does not currently provide the best experience for application developers to take that responsibility.</p> <p>Therefore we must consider opening up the Network Policy for cert-manager more to ensure that application developers have the best chance to manage certificates, and minimise the potential of the platform hindering it.</p>"},{"location":"adr/0051-open-cert-manager-netpols/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security and stability</li> <li>We want to find a solution that is scalable and minimises platform administrator burden</li> <li>We want to find a solution that is the least astonishing and that best serves application developers</li> <li>We want to make it easier to understand faults within the platform</li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#considered-options","title":"Considered Options","text":"<ol> <li>Open egress by default from cert-manager to <code>0.0.0.0:53/tcp</code> and <code>0.0.0.0:53/udp</code> to ensure any DNS providers should just work for DNS-01 challenges.</li> <li>Open egress by default from cert-manager to <code>0.0.0.0:80/tcp</code> to ensure cert-manager can perform self-checks to identify invalid HTTP-01 challenges.</li> <li>Do not open additional egress from cert-manager.</li> </ol>"},{"location":"adr/0051-open-cert-manager-netpols/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: option 1 and option 2.</p>"},{"location":"adr/0051-open-cert-manager-netpols/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>It give the application developer better experience when setting up issuers and certificates.<ul> <li>It should just work for application developers to setup DNS-01 issuers.</li> <li>It should giver clearer responses for application developers to why certificates cannot be issued.</li> </ul> </li> <li>It gives the platform administrator better experience when monitoring environments.<ul> <li>It minimises the role of the platform administrator to ensure certificates can be issued.</li> <li>It minimises the risk of critical alerts for cert-manager being shadowed by misconfigured certificates.</li> </ul> </li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>The Network Policy for cert-manager will be more permissive.<ul> <li>This would add unrestricted egress on <code>:80/tcp</code>, <code>:53/tcp</code>, and <code>:53/udp</code>.</li> <li>This is somewhat offset by the fact that cert-manager is normally configured by default to allow egress to <code>0.0.0.0:443/tcp</code> to connect to Let's Encrypt as they by choice do not provide list of IP addresses to allowlist.</li> </ul> </li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0051-open-cert-manager-netpols/#option-1","title":"Option 1","text":"<ul> <li>Good because it ensure that any DNS provider for DNS-01 challenges should just work.</li> <li>Good because it gives application developers more independence.</li> <li>Bad, because it opens up cert-manager to talk to any DNS server.</li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#option-2","title":"Option 2","text":"<ul> <li>Good because it ensure that cert-manager can identify invalid HTTP-01 challenges.</li> <li>Good because it gives application developers more independence.</li> <li>Bad, because it opens up cert-manager to talk to any HTTP server.</li> </ul>"},{"location":"adr/0051-open-cert-manager-netpols/#option-3","title":"Option 3","text":"<ul> <li>Good, because it ensure that cert-manager runs with the most limited Network Policies.</li> <li>Bad, because application developers cannot configure DNS-01 and HTTP-01 challenges as they want.</li> <li>Bad, because it can cause constant Network Policy alerts that shadows larger problems.</li> <li>Bad, because it can lead to certificates in risk of expiry due to missed errors.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/","title":"Azure Encryption-at-Rest for Object Storage and Block Storage","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2024-02-22</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Elastisys has introduced Azure as a new Infrastructure Provider and wants to ensure that all data is protected with encryption-at-rest, following our established security practices.</p> <p>According to the previous ADR-0041-encryption-at-rest, we decided to rely on the infrastructure Provider for encryption-at-rest to reduce complexity and leverage built-in security features.</p> <p>With Azure, we need to determine how to best implement encryption-at-rest for both object storage and VM-level disk storage.</p> <ul> <li>What options does Azure provide to ensure data is encrypted effectively?</li> <li>How do we align these options with our security policies and existing operational practices?</li> <li>How can we ensure compliance with our established standards while minimizing operational overhead?</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want alignment with our existing ADR-0041: Encryption-at-Rest</li> <li>We want to maintain platform security and stability.</li> <li>We want to avoid operational complexity.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#considered-options","title":"Considered Options","text":"<ol> <li> <p>For Object Storage, Use Service-Side Encryption with Microsoft-Managed Key &amp; For VM-Level, Use Azure Disk Storage Server-Side Encryption (Both enabled by default).</p> </li> <li> <p>For Object Storage, Use Service-Side Encryption with Microsoft-Managed Key plus Double Encryption &amp; For VM-Level, Use Azure Disk Storage Server-Side Encryption with Microsoft-Managed Key.</p> </li> <li> <p>For Object Storage, Use Service-Side Encryption with Customer-Managed Key &amp; For VM-Level, Use Azure Disk Storage Server-Side Encryption with Customer-Managed Key (Requires Azure Managed service called Azure Disk Encryption Set).</p> </li> </ol>"},{"location":"adr/0052-azure-encryption-at-rest/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option: Option 1: Use Azure's default service-side encryption with Microsoft-managed keys for object storage and Azure Disk Storage Server-Side Encryption for VM-level disks.</p> <p>When deciding on the best encryption method for VM-level disk storage in Azure, we considered four main options: Azure Disk Storage Server-Side Encryption, Encryption at Host, Azure Disk Encryption (ADE) and Confidential disk encryption.</p> <p>Each option offers different levels of control, security, and operational complexity. For more comparisons, you can refer here.</p> <p>We selected Azure Disk Storage Server-Side Encryption for VM-level disk storage because it provides a balance of strong security, ease of use, compliance, operational efficiency, and cost-effectiveness.</p> <p>This approach aligns with our strategy to leverage infrastructure Provider capabilities for encryption-at-rest, as outlined in ADR 0041: Encryption-at-Rest, while avoiding the complexities and potential performance drawbacks associated with other encryption methods.</p>"},{"location":"adr/0052-azure-encryption-at-rest/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Aligns with our decision ADR 0041: Encryption-at-Rest to rely on the infrastructure Provider for encryption, reducing the need for custom encryption solutions.</li> <li>Maintains simplicity by leveraging Azure's default encryption settings, reducing complexity for Platform Administrators and Application Developers.</li> <li>Provides strong encryption-at-rest using 256-bit AES, meeting the security requirements without additional operational overhead.</li> <li>Avoids additional costs associated with managing customer-managed keys or implementing extra encryption layers.</li> <li>No significant performance impact, as Azure optimizes encryption and decryption processes.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Using Microsoft-managed keys might not meet the requirements for some organizations that need full control over their encryption keys.</li> <li>Some customers might feel less secure using Microsoft-managed keys instead of managing their own keys.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0052-azure-encryption-at-rest/#option-1","title":"Option 1","text":"<ul> <li>Good, because it aligns with our ADR 0041 which follows the established decision to rely on infrastructure Provider encryption, ensuring consistent security practices.</li> <li>Good, because it provides encryption-at-rest using 256-bit AES encryption, aligning with Elastisys' cryptography policy.</li> <li>Good, because encryption is enabled by default, requiring no additional configuration, reducing setup complexity.</li> <li>Bad, because it relies on Microsoft-managed keys, limiting control over key management, which might not be suitable for all organizations.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#option-2","title":"Option 2","text":"<ul> <li>Good, because it protects sensitive data with two layers of encryption with two different key, ensuring data remains secure even if one layer is compromised.</li> <li>Bad, because double encryption requires additional setup and configuration, increasing the complexity and operational burden on Platform Administrators and Application Developers.</li> <li>Bad, because double encryption can introduce performance overhead due to the extra encryption and decryption processes, potentially impacting application performance and resource utilization.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#option-3","title":"Option 3","text":"<ul> <li>Good, because customer have full Control Over Encryption Keys.</li> <li>Bad, because it increases the operational burden on customer as need to manage the entire lifecycle of encryption keys, including rotation, backup, and recovery.</li> <li>Bad, because it doesn't align with our previous ADR 0041: Encryption-at-Rest.</li> </ul>"},{"location":"adr/0052-azure-encryption-at-rest/#links","title":"Links","text":"<ul> <li>ADR-0041</li> <li>Azure-storage-service-side-encryption</li> <li>disk-encryption-overview</li> <li>cryptography</li> <li>Double encryption</li> <li>comparison</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/","title":"Do not expose platform observability services to end-users","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Product team</li> <li>Date: 2024-04-04</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We are being asked whether we can manage observability data (logs, metrics, traces) from external and/or public clients, ensuring secure ingestion of data into our platform observability stack (OpenSearch for logs, Prometheus for metrics, and Jaeger for traces). There are two distinct categories of clients sending observability data:</p> <p>External Clients: Systems or services under the control of Application Developers, such as VMs or servers running in trusted environments like Safespring etc. External clients can authenticate using an API key or equivalent.</p> <p>Public Clients: Applications that run on devices or in browsers that are not under the control of Application Developers, such as mobile apps (Flutter) and Single-Page Applications (Angular), which are exposed to the public internet. Public clients cannot be authenticated using an API key or equivalent, since that API key could be easily extracted from the public client.</p> <p>The core problem is how to securely manage the ingestion of observability data from both external and public clients into the platform observability stack.</p> <ul> <li>Can we allow external clients to push observability data directly to our platform observability services?</li> <li>Can we allow public clients (e.g., mobile apps, SPAs) to push observability data directly to our platform observability services?</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain platform security and stability.</li> <li>We want to avoid operational complexity.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Do not allow public clients to push data directly to our platform observability services but allow external clients with authentication and authorization mechanisms.</p> </li> <li> <p>Allow public and external clients to push data directly to our platform observability services.</p> </li> </ol>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option: Option 1: Do not allow public clients to push data directly to our platform observability services but allow external clients with authentication and authorization mechanisms.</p> <p>External Clients: We will allow external clients to push observability data (logs, metrics, traces) directly into our platform observability services under the following conditions:</p> <ul> <li>Logs: External clients can push logs to our OpenSearch service with authentication and authorization mechanisms. However, as per our ToS-A5.3 As a whole, for the intended use-case, OpenSearch is optimized for platform and containerized application observability which means logs are expected to come from the application running in the workload Cluster, and external logs should align with this use-case, with exceptions allowed for observability of legacy code that brings business value to the workload Cluster.</li> <li>Metrics: External clients can push metrics to a Prometheus Pushgateway, which Prometheus scrapes periodically.</li> <li>Traces: While we do not currently support traces from external clients, we may explore the option of allowing external clients to push traces to an OpenTelemetry collector in our platform observability services.</li> </ul> <p>Additional Considerations Note In accordance with our ToS-A5.3 As a whole, for the intended use-case, please note the following:</p> <ul> <li>It is critical to acknowledge that while these components (OpenSearch, Prometheus) are technically capable of handling external clients, our platform observability services are designed and optimized to function as a secure platform for containerized applications. Using these components in isolation or for other use-cases introduces unknown risks, and such configurations are not covered by the Self-managed Plan. Therefore, support for external clients observability data is limited to Application Developers compliance with the intended use-case of our platform observability services.</li> <li>Authentication and authorization mechanisms will be in place to ensure only trusted external clients can send observability data. Any use of these services outside of the intended platform observability scope may not be supported by the platform.</li> </ul> <p>Public Clients: We will not allow public clients to push observability data directly to platform observability services for the following reasons:</p> <ul> <li>System Design We assessed that the systems we use (Jaeger, OpenSearch, Prometheus) are not designed for public clients.</li> <li>Security Risks: Embedding authentication credentials in client-side code (e.g., in a mobile app or web app) exposes them to the public, making it easy for attackers to intercept and misuse those credentials. This poses a significant risk to the security of our platform.</li> <li>Data Leakage: Publicly exposing observability endpoints (such as OpenSearch, Prometheus, or Jaeger) could lead to sensitive data leaks.</li> </ul> <p>General Recommendation - Secure forwarding approach for public clients: We recommend that Application Developers route observability data through a secure backend API that authenticates logged-in users to significantly reduce the attack surface, forwards the data to our platform observability services for processing, and thus it ensures that our sensitive observability endpoints remain protected from public exposure.</p>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#security-considerations-for-end-user-access","title":"Security considerations for End-User access","text":"<ul> <li> <p>We also concluded that our platform observability stack (OpenSearch, OpenSearch Dashboards, Thanos, and Grafana) is strictly intended for application and platform observability by Application Developers and Platform Administrators. These services are not meant to be accessed by end-users, either directly or indirectly, as this would pose significant security risks, not just performance concerns. This could expose our internal platform data, potentially compromising the security of our entire platform. We do not trust the isolation of these observability components to the extent of granting access to end-users.</p> </li> <li> <p>Allowing end-users access to these observability services violates our secure design principles. Even if the isolation mechanisms of these services were trusted, exposing them to end-users would not align with our secure platform design. The risk is not limited to performance degradation but extends to the core security posture of our platform.</p> </li> <li> <p>Conclusion: So if the end-users require direct or indirect access to our platform observability services (e.g., Grafana), these should be considered application components rather than platform components. In such cases, Application Developers should install and manage these components in a self-service manner within the Workload Cluster, separate from our core platform observability services.</p> </li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>It reduces the risk of unauthorized access to our platform observability services, protecting the Cluster from data leaks, credential abuse, and DoS attacks.</li> <li>By routing observable data from public clients through a backend API, we maintain control over the data flow and can apply additional security measures, such as logging and throttling.</li> <li>We can selectively allow trusted external services to push observability data directly, without exposing our platform observability services to the broader internet.</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Implementing a secure backend API for public clients adds complexity to the system and increases the workload for Application Developers.</li> <li>Routing observability data through a backend API may introduce some delays compared to direct ingestion.</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#option-1","title":"Option 1","text":"<ul> <li>Good, because embedding credentials in public-facing apps exposes our infrastructure to potential attacks, while trusted external clients are allowed to push data directly with proper authentication and authorization mechanisms, ensuring security without sacrificing performance for external clients.</li> <li>Good, because public clients, which are less trusted, must route data through a backend API developed by Application Developers, ensuring secure data handling, whereas external clients can push data directly to the platform observability services without the latency that public clients experience due to the intermediary backend API.</li> <li>Good, because directly exposing observability endpoints to public clients could lead to data leakage, compromising sensitive information.</li> <li>Bad, because it might require the development, maintenance to allow external clients to push data to our platform observability services.</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#option-2","title":"Option 2","text":"<ul> <li>Good, because direct ingestion allows for immediate observability from both public and external clients, which enables faster insights and issue resolution.</li> <li>Bad, because public clients expose sensitive credentials (e.g., API keys or tokens), making them vulnerable to theft and misuse by attackers.</li> <li>Bad, because unauthorized access t our platform observability services could lead to data breaches or system compromise.</li> <li>Bad, because public clients could overwhelm platform observability services with large volumes of requests or malicious data, causing performance degradation and potential service outages.</li> </ul>"},{"location":"adr/0053-do-not-expose-platform-observability-services-to-end-users/#links","title":"Links","text":"<ul> <li>opentelemetry-collector</li> <li>k8s-otel-expose</li> <li>Deployment</li> <li>Data-leakage</li> <li>ToS-A5.3 As a whole, for the intended use-case</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/","title":"Allow Application Developer write access to Endpoints and EndpointSlices after Proper Risk Acceptance","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Arch Meeting</li> <li>Date: 2024-02-08</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Our Platform strives to balance security with flexibility. In doing so, we frequently encounter trade-offs between ensuring robust security and maintaining full Platform functionality. The presence of low-risk vulnerabilities CVE-2021-25740 related to Endpoints and EndpointSlices objects introduces several risks, primarily involving cross-namespace traffic that can bypass intended security controls.</p> <p>This means that applications hosted in the Workload Cluster and running in one namespace could potentially interact with or access services in another namespace, thereby breaching the intended security isolation.</p> <p>Should we allow Application Developers to create Endpoints and EndpointSlices objects on our Platform?</p>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to avoid operational complexity.</li> <li>We want to best serve the Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#considered-options","title":"Considered Options","text":"<ol> <li> <p>Allow creation and editing of Endpoints and EndpointSlices by default.</p> </li> <li> <p>Disallow creation and editing of Endpoints and EndpointSlices by default.</p> </li> <li> <p>Never allow creation and editing of Endpoints and EndpointSlices.</p> </li> <li> <p>Allow creation and editing of Endpoints and EndpointSlices, but only after Application Developer accepted the security risks.</p> </li> </ol>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option: Option 4: Allow creation and editing of Endpoints and EndpointSlices, but only after Application Developer accepted the security risks.</p> <p>Opting for this option provides a balanced approach, allowing Platform flexibility while still emphasizing the importance of security through best practices and proactive risk management.</p>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Allows Application Developers to maintain essential connections to external resources without breaking existing setups.</li> <li>Ensures that Application Developers have the freedom to configure their services according to their needs.</li> <li>Encourages Application Developers education, adherence to best practices, and proactive monitoring to mitigate potential risks.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>The vulnerability remains, potentially exposing Application Developers to risk if misconfigured.</li> <li>Continuous effort is needed to ensure Application Developers are aware of the risks and follow best practices.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#option-1","title":"Option 1","text":"<ul> <li>Good, because it allows Application Developers the flexibility to configure Endpoints and EndpointSlices as needed without requiring intervention from Platform Administrators.</li> <li>Good, because disabling Endpoints and EndpointSlices by default or tightening security to eliminate low-risk vulnerabilities would increase operational complexity for both Application Developers and Platform Administrators.</li> <li>Bad, because in multi-tenant environments, a single tenant's misconfiguration could expose vulnerabilities to others, leading to potential security breaches.</li> <li>Bad, because even low-risk vulnerabilities could become more exploitable in the future, especially if misconfigurations occur.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#option-2","title":"Option 2","text":"<ul> <li>Good, because strict security policies reduce the risk of attacks, particularly in multi-tenant environments where isolation and protection are crucial.</li> <li>Bad, because restricting access to external systems introduces significant operational challenges for Application Developers.</li> <li>Bad, because enforcing strict security policies could break existing Application Developers workflows, requiring significant reconfiguration.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#option-3","title":"Option 3","text":"<ul> <li>Good, because it eliminates the risk associated with Endpoints and EndpointSlices, ensuring that cross-namespace traffic and security breaches are not possible.</li> <li>Bad, because restricting access to external systems introduces significant operational challenges for Application Developers.</li> <li>Bad, because enforcing strict security policies could break existing Application Developers workflows, requiring significant reconfiguration.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#option-4","title":"Option 4","text":"<ul> <li>Good, because it offers a middle ground by allowing flexibility while ensuring Application Developers are aware of and accept the associated risks before creating and editing Endpoints and EndpointSlices.</li> <li>Bad, because it relies on Application Developers fully understanding and accepting the risks, which may not always be the case.</li> </ul>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>If you enable this feature, then make sure Application Developers understand and accept the added stability and security risks. A message as follows could be used:</p> <pre><code>Hello Team,\n\nAfter careful discussion, we have decided that it is permissible to grant Application Developers the ability to create Endpoints and EndpointSlices objects, provided the following conditions are met:\n\n(a) There is no other viable alternative for your use case;\n(b) You understand and accept the security risks involved;\n\nFor (a), please confirm that you have explored all other potential options for your use case and found that using Endpoints or EndpointSlices is the only way forward. Refer to the Kubernetes documentation for possible alternatives: https://kubernetes.io/docs/concepts/services-networking/service/\n\nFor (b), please confirm that you are aware of the security implications and understand the consequences of this CVE: https://github.com/kubernetes/kubernetes/issues/103675.\n\nTo summarize, if you can confirm points (a) through (b), we will proceed with enabling the necessary permissions for creating Endpoints and EndpointSlices.\n\nRegards,\nElastisys Managed Services Team\n</code></pre>"},{"location":"adr/0054-allow-write-access-to-endpoints-and-endpointslices-after-risk-acceptance/#links","title":"Links","text":"<ul> <li>CVE-2021-25740</li> <li>ADR-0039</li> </ul>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/","title":"[Superseded by ADR-0059]Welkin to consist of both public and private open source","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Management Team</li> <li>Date: 2024-11-07</li> </ul>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Elastisys Welkin consists of open source software, and indeed, most of Welkin itself is also public:</p> <ul> <li>the Kubespray-based installer;</li> <li>the documentation (accessible also as a web site); and</li> <li>the core application components themselves.</li> </ul> <p>Some parts, however, have never been public open source, such as:</p> <ul> <li>the so-called Additional Managed Services (e.g. PostgreSQL);</li> <li>the Cluster API-based installer; and</li> <li>the internal tools and documentation Elastisys needs to operate the managed service on public clouds.</li> </ul> <p>This ADR is about those components that are not publicly available. The ones that already are public will stay public.</p> <p>For context, it is important to note the difference between public and private vs. closed or open source:</p> <ul> <li>Public and open source: Code is publicly available for anyone to view, use, modify, and distribute. This is a very common distribution model for open source software, such as Linux or Kubernetes.</li> <li>Public but closed source: Software is accessible to the public but with proprietary code that can't be modified. This is the standard for typical proprietary software.</li> <li>Private and open source: Code is made available under an open source license to a restricted set of authorized users or organizations.</li> <li>Private and closed source: Code and access to the software is restricted to only specific users, with no modification allowed.</li> </ul> <p>Should the additional components of Welkin's source code be open or closed source, and should distribution thereof be public or private?</p>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Elastisys wants to protect Welkin users' open source freedom</li> <li>Elastisys wants to offer product support customers good value for their money, and incentivise these to keep paying Elastisys, not only for support but also for access to future Welkin development, security patches, etc.</li> <li>Product support customers should have ability to adapt Welkin if needed</li> <li>Elastisys was founded on a bedrock on knowledge sharing and openness, and wants that to be reflected in the products and services it develops</li> <li>Elastisys needs to protect its intellectual property</li> </ul>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#considered-options","title":"Considered Options","text":"<ol> <li>Offer everything as public open source.</li> <li>Keep the core open and public, other parts as private and open.</li> <li>Keep the core open and public, other parts as private and proprietary (open core model).</li> </ol>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#decision-outcome","title":"Decision Outcome","text":"<p>Option 2: Keep the core open and public, other parts as private and open.</p> <p>This gives a balanced approach where those that pay for features that are needed in e.g. enterprise settings or for advanced on-premise use can have access to up to date open source code for as long as their engagement with Elastisys is ongoing.</p> <p>Once a company stops paying for up to date open source access and product support, they can still keep the code as it was when they had access to it, but do not get further updates, and Elastisys is also not under obligation to keep QA processes ongoing to ensure that future versions of Welkin are compatible with any special solutions developed as part of a past customer engagement.</p>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Good, because Welkin can stay true to its open source commitment, and thus also preserves user freedoms.</li> <li>Good, because customers of product support get exactly what they pay for: access to the open source powering the best security-focused Kubernetes distribution in the world, possible to modify to their liking, and with upstream fixes by Elastisys for the duration of the contract period.</li> <li>Good, because with a clear distinction between what is public and private open source, we are clear that everything is open source \u2013 what differs is only whether it is distributed in private or public.</li> <li>Good, because Elastisys protects its intellectual property around what is needed to operate a Kubernetes platform service at scale on several underlying cloud infrastructure platforms.</li> </ul>"},{"location":"adr/0055-welkin-to-consist-of-both-public-and-private-open-source/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Bad, because a public open source distribution is easier to show every single detail about, as all details are laid out in public anyway.</li> <li>Bad, because there may be a general perception that \"open source\" loosely means public open source, and that it should all be available on e.g. GitHub.</li> <li>Bad, because there is no way for community contributions.</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/","title":"Only allow Ingress Snippet Annotations after Proper Risk Acceptance","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Product Team</li> <li>Date: 2024-11-07</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>We previously decided to allow configuration snippet annotations for Ingress NGINX, but only after formal risk acceptance from the Application Developer. However, we had not yet decided on handling specific use cases, such as whether server snippets should have stricter controls due to their higher risk profile.</p> <p>The difference is that configuration snippets allows to configure for whole location blocks and are validated by the Ingress Controller, while server snippets allows to configure for whole server block and are not validated by the Ingress Controller, they are passed directly to NGINX, which increases the vulnerability exposure area.</p> <p>Snippet Annotations are a powerful tool to allow injecting custom configurations, including both configuration and server snippets, into the Ingress NGINX Controller. For example, it allows things such as header renaming, custom authentication, or other advanced use cases etc.</p> <p>However, with great power comes great responsibility. Snippet annotations may break the Ingress Controller and cause downtime for all applications hosted in the Workload Cluster. Also, it opens up CVE-2021-25742, and possibly other CVEs, which means that Application Developers can exfiltrate all Secrets in the Workload Cluster.</p> <p>How shall we best serve Application Developers without compromising platform stability and security?</p>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve Application Developers.</li> <li>We want to ensure platform stability and security.</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#considered-options","title":"Considered Options","text":"<ul> <li>Allow <code>the use of \"snippet annotations\" with Ingress</code> by default.</li> <li>Disallow <code>the use of \"snippet annotations\" with Ingress</code> by default.</li> <li>Never allow <code>the use of \"snippet annotations\" with Ingress</code>.</li> <li>Allow <code>the use of \"snippet annotations\" with Ingress</code>, but only after Application Developer accepted the downtime and security risks.</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: Allow <code>the use of \"snippet annotations\" with Ingress</code>, but only after Application Developer accepted the downtime and security risks.</p>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Several use-cases commonly requested by Application Developers can be satisfied.</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Platform security is at a small risk if this feature is misused by Application Developers.</li> <li>Platform stability is at a small risk if this feature is misused by Application Developers.</li> </ul>"},{"location":"adr/0056-allow-snippets-annotations-after-risk-acceptance/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<p>If you enable this feature, then make sure Application Developers understand and accept the added stability and security risks. A message as follows could be used:</p> <pre><code>Hello!\n\nAfter talking with the team, we have decided that it is okay to enable the snippet annotations provided that:\n\n(a) There is no alternative method to achieve the required customization;\n(b) The Application Developers fully understands and accepts the security risks involved;\n(c) The Application Developers takes responsibility for any downtime caused by misconfiguration; and\n(d) The Application Developers takes responsibility for updating the annotation as required.\n\nFor (a), please confirm you have evaluated and ruled out other options.\n\nFor (b), confirm you understand and accept potential vulnerabilities, such as CVEs like https://github.com/kubernetes/ingress-nginx/issues/7837.\n\nFor (c), please confirm that you understand that this snippet annotations is quite powerful, meaning that misconfiguration can lead to downtime for all your Ingress resources. Obviously, if Nginx goes down due to any custom configuration, then we cannot take responsibility for that.\n\nFor (d), please confirm that you are okay to take responsibility for making sure that the custom configuration is supported in newer versions of Nginx, as we sometimes upgrade Nginx. Both our release notes and calendar invites for the maintenance windows mention if we are upgrading Nginx.\n\nTo sum up, if you can confirm (a)-(d) then we can proceed with enabling snippet annotations for your use case.\n\nRegards,\n</code></pre>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/","title":"Do Not Use Managed Kubernetes Services","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Product Team</li> <li>Date: 2024-09-26</li> </ul>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Several of our Infrastructure Providers offer managed Kubernetes services. These services are designed to simplify the operational overhead of managing Kubernetes control planes. Customers of our self-managed service, especially those operating on modern cloud platforms like AWS, have expressed interest in leveraging these managed control planes to streamline their stack management. This raises the question: should we consider integrating cloud-managed Kubernetes services into our Welkin stack?</p>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to maintain Platform security and stability.</li> <li>We want to avoid operational and development complexity.</li> <li>We want to best serve Application Developers.</li> <li>We want to make the Platform Administrator life easier.</li> <li>We want to maintain fine-grained control over the Kubernetes stack.</li> <li>We want to reduce the complexity of the testing matrix.</li> <li>We want to keep platform portability.</li> </ul>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#considered-options","title":"Considered Options","text":"<ol> <li>Yes, always use Managed Kubernetes services.</li> <li>Yes, use Managed Kubernetes services if they provide sufficient functionality.</li> <li>No, we do not use Managed Kubernetes services because we want to reduce QA complexity and keep platform portability.</li> </ol>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: 3 - No, we do not use Managed Kubernetes services because we want to reduce QA complexity and keep platform portability.</p> <p>While managed Kubernetes services may seem appealing, adopting them introduces challenges that conflict with our operational and strategic goals. These include:</p> <ul> <li>losing fine-grained control over the Kubernetes stack, e.g., how we configure the apiserver's audit logs;</li> <li>dealing with non-equivalent service offerings across Infrastructure Providers, resulting in a complex and unmanageable test matrix; and</li> <li>the inability to support bare metal deployments.</li> </ul>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Quality Assurance: When we release an upgraded platform version, you get a well-tested package of up-to-date versions of Kubernetes and other platform level components, which we confirmed that are compatible with each other.</li> <li>Regulatory Compliance: You get the Kubernetes Cluster which is configured according to the relevant EU Regulations and Directives, Best Security Practices, Industry Specific Regulations applicable in Sweden, as documented here, e.g., we use ntp.se for time synchronization.</li> <li>Platform Portability: You can install the very same Kubernetes Platform on any Infrastructure Provider, either via a Cluster API provider or Kubespray. This means that a Welkin environment looks and feels the same, no matter if its hosted on Azure, EU cloud or on-prem. Customer Application can run as required within a specific geographical location or jurisdiction, with differences being hidden away by the platform.</li> <li>Need for Fine-Grained Control: The ability to maintain fine-grained control over the Kubernetes stack is a key requirement, which would be limited by using managed Kubernetes services.</li> </ul>"},{"location":"adr/0057-why-we-do-not-use-cloud-managed-kubernetes-services/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>This means we don't leverage reduced operational burden which comes with mature Kubernetes offerings, such as Azure Kubernetes Service (AKS). However, we see this as a price worth paying for a portable platform.</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/","title":"Boot disk size on Nodes","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Product Team</li> <li>Date: 2024-12-19</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>This ADR supersedes ADR-0032 which previously established that all Nodes should have boot disk of size 100GB irrespective of Node size and that control plane Nodes should have 100GB local disk.</p> <p>After running like this for over 1 year we have discovered that for small and medium environments this is too much and can be lowered for some of the Nodes to reduce the infrastructure footprint.</p> <p>Should we use different boot disk sizes for Nodes than we set before?</p>"},{"location":"adr/0058-boot-disk-sizes/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to best serve the Application Developer needs.</li> <li>We want to find a solution which is scalable and minimizes administrator burden.</li> <li>We don't want to waste infrastructure.</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/#considered-options","title":"Considered Options","text":"<ol> <li>Do nothing and keep decision from ADR-0032</li> <li>Change boot disk size to 50GB for all Nodes irrespective of size and type.</li> <li>Keep boot disk size to minimum 100GB for all WC worker Nodes</li> <li>Keep boot disk size to minimum 100GB for all WC Elastisys Nodes</li> <li>Keep boot disk size to minimum 100GB for all control plane Nodes in both MC and WC</li> <li>Keep boot disk size to minimum 100GB for all control plane Nodes in WC</li> <li>Change boot disk size to minimum 50GB for all Nodes irrespective of size and type in MC Cluster and leave all WC Cluster Nodes with 100GB boot disk size.</li> <li>Keep the recommended boot disk size to minimum 100GB for all Nodes irrespective of size and type, however allow on request to change MC Nodes and some WC Nodes to use 50GB.</li> <li>Do not change the default values</li> </ol>"},{"location":"adr/0058-boot-disk-sizes/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: 8 + 9. This means we will keep the recommended boot disk size to minimum 100GB for all Nodes irrespective of size and type, however allow on request to change MC Nodes and some WC Nodes to use 50GB. The default value stays at 100 GB.</p>"},{"location":"adr/0058-boot-disk-sizes/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Improved Reliability for application Nodes</li> <li>Reduce the number of alerts and ops time</li> <li>Improve platform stability and scalability</li> <li>Reduce the infrastructure cost for small and medium environments.</li> <li>By not changing default configurations, no existing Clusters will be impacted, reducing the risk of unintentional disruption.</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Some alerts might appear for Nodes that will use 50GB volumes</li> <li>More ops time would be needed to scale up the 50GB volumes</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<ul> <li>Try to use same VM flavors on all environments</li> <li>Use VM flavor with local disk of minimum 50GB, recommended 100GB or whichever is closest to this size depending on Infrastructure Provider for control plane Nodes.</li> <li>For worker Node use 100GB boot disk size on Infrastructure Providers that allow it and on the ones that we can't use the VM flavor with the closest disk size.</li> </ul>"},{"location":"adr/0058-boot-disk-sizes/#links","title":"Links","text":"<ul> <li>Discussion on boot disk size</li> </ul>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/","title":"Welkin to Consist of Public Open Source Code and Proprietary Documentation","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Management Team</li> <li>Date: 2025-01-20</li> </ul>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>Welkin\u2019s current policy, as defined in ADR-0055, is to maintain a mix of public and private open-source components for its product. This approach aimed to balance openness with control over sensitive intellectual property.</p> <p>However, new information has emerged. Essential and important entities (as defined in NIS2), who are key stakeholders, require access to the codebase to understand, troubleshoot, and maintain critical systems. Restricting access to portions of the code limits their ability to operate effectively and creates friction in their workflows.</p> <p>Additionally, by maintaining a private portion of the open-source code, we introduce unnecessary complexity into our development and operational processes. Ensuring parity between public and private repositories adds overhead and detracts from our goals of transparency and ease of use.</p> <p>Given these factors, should Welkin move to a model of fully public open-source code while keeping documentation proprietary?</p>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>Elastisys wants to protect Welkin users' open source freedom</li> <li>Elastisys wants to offer product support customers good value for their money, and incentivise these to keep paying Elastisys, not only for support but also for access to future Welkin development, security patches, etc.</li> <li>Product support customers should have ability to adapt Welkin if needed</li> <li>Elastisys was founded on a bedrock on knowledge sharing and openness, and wants that to be reflected in the products and services it develops</li> <li>Elastisys needs to protect its intellectual property</li> </ul>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#considered-options","title":"Considered Options","text":"<ol> <li>Yes, move to a model of fully public open-source code.</li> <li>No, keep decision made in ADR-0055.</li> </ol>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#decision-outcome","title":"Decision Outcome","text":"<p>Option 1: Move to a model of fully public open-source code.</p> <p>This reflects Welkin's commitment to supporting its users, especially critical infrastructure operators, while continuing to provide value through proprietary offerings.</p>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Good, because Welkin can stay true to its open source commitment, and thus also preserves user freedoms.</li> <li>Good, because critical infrastructure operators will have direct access to the codebase, enabling easier troubleshooting, customization, and maintenance.</li> <li>Good, because with a fully open-source codebase, Welkin is better positioned to attract contributors and benefit from external expertise.</li> </ul>"},{"location":"adr/0059-welkin-to-consist-public-open-source-code-and-proprietary-documentation/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Bad, because the move to a fully public codebase reduces the exclusivity of Welkin's software.</li> <li>Bad, because supporting a larger community and addressing external contributions can introduce additional workload for Welkin\u2019s developers.</li> <li>Bad, because public code increases the likelihood of forks, which can fragment the user base and dilute the impact of Welkin\u2019s primary offering.</li> <li>Bad, because with code available for free, some potential customers may choose to self-host rather than purchase Welkin's proprietary documentation or services.</li> </ul>"},{"location":"adr/0060-group-alerts-in-alertmanager/","title":"Group alerts in Alertmanager","text":"<ul> <li>Status: Accepted</li> <li>Deciders: Product Team</li> <li>Supersedes: ADR-0013 Configure Alerts in On-call Management Tool (e.g., Opsgenie)</li> <li>Date: 2025-04-29</li> </ul>"},{"location":"adr/0060-group-alerts-in-alertmanager/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>As a Welkin administrator, I experience frustration when receiving multiple alerts from the same component for the same underlying problem. This is particularly noticeable when identical alerts are fired for each Pod, such as when a Fluentd issue triggers multiple alerts from every Fluentd Pod.</p> <p>ADR-0013 previously decided to configure alerting within the on-call management tool (OMT). However, given that some OMT's -- such as OpsGenie -- lacks robust support for alert grouping, we aim to leverage Alertmanager's native grouping capabilities to mitigate this issue.</p> <p>Should we begin utilizing the grouping feature in Alertmanager?</p>"},{"location":"adr/0060-group-alerts-in-alertmanager/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>We want to find a solution which is scalable and minimizes the platform administrator burden.</li> <li>We want to make the operator life easier.</li> <li>Minimize the volume of multiple identical alerts originating from the same component for the same problem.</li> </ul>"},{"location":"adr/0060-group-alerts-in-alertmanager/#considered-options","title":"Considered Options","text":"<ol> <li>Option 1: Yes, use grouping in Alertmanager. Leverage Alertmanager's built-in functionality to aggregate related alerts before sending them to the OMT.</li> <li>Option 2: No, look at other on-call management tools that provide this feature. Seek out and potentially migrate to a different OMT that offers more sophisticated native alert grouping capabilities.</li> <li>Option 3: No, look at implementing this in OpsGenie. Investigate and potentially develop custom solutions or integrations within OpsGenie to achieve alert grouping.</li> </ol>"},{"location":"adr/0060-group-alerts-in-alertmanager/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen Option 1: Yes, use grouping in Alertmanager.     - Generic prioritization, grouping, inhibition, and silencing of alerts will primarily be handled within Alertmanager.     - Environment-specific silencing (e.g., for infrastructure provider maintenance windows) and differentiation between 24/7 and 6-22 customer support will continue to be managed in the On-Call Management Tool (OMT).     - Alertmanager's default settings for <code>group_wait</code> and <code>group_interval</code> are considered sufficient for the current needs, but are subject to tuning based on operational feedback.</p>"},{"location":"adr/0060-group-alerts-in-alertmanager/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>Reduced Alert Fatigue: It would significantly make the administrator's life easier by reducing the sheer volume of redundant alerts to handle, leading to a more scalable alert management process.</li> <li>Tool Agnostic: It would help Welkin OS users (administrators) since the grouping logic is not dependent on a specific on-call management tool or its intricate configuration, ensuring consistency regardless of the OMT in use.</li> </ul>"},{"location":"adr/0060-group-alerts-in-alertmanager/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>Potential Grouping Limitations: There may be some limitations to configuration with Alertmanager grouping that could be solved by a more advanced on-call management tool. For example, Alertmanager might not inherently be able to group alerts from entirely different components that are firing for the same root cause without complex rule definitions. Further investigation into these specifics is needed</li> </ul>"},{"location":"adr/0060-group-alerts-in-alertmanager/#recommendation-to-platform-administrators","title":"Recommendation to Platform Administrators","text":"<ul> <li>Configuration: Refer to the Alertmanager configuration documentation for detailed options.</li> <li>Implementation Details and Tuning: The initial implementation of grouping will likely result in a mix of \"over-grouping\" (too many disparate alerts grouped together) and \"under-grouping\" (related alerts not being grouped). This will require continuous tuning based on operational experience to achieve the ideal state where alerts having the same underlying cause are grouped effectively.</li> <li>While Alertmanager's inhibition feature is powerful for suppressing notifications, it carries a risk of missing critical alerts if not configured carefully. This feature will be revisited and considered for implementation at a later stage.</li> </ul>"},{"location":"adr/template/","title":"[short title of solved problem and solution]","text":"<ul> <li>Status: [proposed | rejected | accepted | deprecated | \u2026 | superseded by ADR-0005 </li> <li>Deciders: [list everyone involved in the decision] </li> <li>Date: [YYYY-MM-DD when the decision was last updated] </li> </ul> <p>Technical Story: [description | ticket/issue URL] </p>"},{"location":"adr/template/#context-and-problem-statement","title":"Context and Problem Statement","text":"<p>[Describe the context and problem statement, e.g., in free form using two to three sentences. You may want to articulate the problem in form of a question.]</p>"},{"location":"adr/template/#decision-drivers","title":"Decision Drivers","text":"<ul> <li>[driver 1, e.g., a force, facing concern, \u2026]</li> <li>[driver 2, e.g., a force, facing concern, \u2026]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#considered-options","title":"Considered Options","text":"<ul> <li>[option 1]</li> <li>[option 2]</li> <li>[option 3]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#decision-outcome","title":"Decision Outcome","text":"<p>Chosen option: \"[option 1]\", because [justification. e.g., only option, which meets k.o. criterion decision driver | which resolves force | \u2026 | comes out best (see below)].</p>"},{"location":"adr/template/#positive-consequences","title":"Positive Consequences","text":"<ul> <li>[e.g., improvement of quality attribute satisfaction, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"adr/template/#negative-consequences","title":"Negative Consequences","text":"<ul> <li>[e.g., compromising quality attribute, follow-up decisions required, \u2026]</li> <li>\u2026</li> </ul>"},{"location":"adr/template/#pros-and-cons-of-the-options","title":"Pros and Cons of the Options","text":""},{"location":"adr/template/#option-1","title":"[option 1]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#option-2","title":"[option 2]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#option-3","title":"[option 3]","text":"<p>[example | description | pointer to more information | \u2026] </p> <ul> <li>Good, because [argument a]</li> <li>Good, because [argument b]</li> <li>Bad, because [argument c]</li> <li>\u2026 </li> </ul>"},{"location":"adr/template/#links","title":"Links","text":"<ul> <li>[Link type] [Link to ADR] </li> <li>\u2026 </li> </ul>"},{"location":"ciso-guide/","title":"CISO and DPO Guide Overview","text":"<p>This part of our open source documentation is intended for CISOs, DPOs or similar. Our goal is to help users of Welkin prove to both internal or external auditors that the application runs on top of a secure and compliant platform.</p> <p>The vision of the project is to secure Europe\u2019s digital future for services critical to society. One very important aspect in order for us to deliver on that vision is to provide best in class documentation for how we enable users to fulfill their security and compliance requirements, control by control.</p> <p>From the start, Welkin has been built based on relevant EU regulations and industry best practices. We've drawn inspiration from, and based architectural decision on information gathered from GDPR, NIS, NIS2, NIST, ENISA, MSB (Swedish Civil Contingencies Agency), BSI (German Federal Office for Information Security), US Department of Defense, industry specific regulations and much more.</p>"},{"location":"ciso-guide/#platform-architecture-influenced-by-information-security-best-practices","title":"Platform architecture influenced by information security best practices","text":""},{"location":"ciso-guide/#additional-resources","title":"Additional resources","text":"<p>Resources for the Chief Information Security Officer (CISO) or similar:</p> <ul> <li>ISO 27001</li> <li>NIS2</li> <li>Cyber Resilience Act (CRA)</li> </ul> <p>Resources for the Data Protection Officers (DPO) or similar:</p> <ul> <li>GDPR</li> <li>HSLF-FS 2016:40</li> </ul> <p>You might also want to read the Frequently Asked Questions (FAQ).</p>"},{"location":"ciso-guide/audit-logs/","title":"Audit Logs","text":"<p>To help comply with various data protection regulations, Welkin comes built-in with audit logs, which can be accessed via OpenSearch Dashboard.</p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#what-are-audit-logs","title":"What are audit logs?","text":"<p>In brief, audit logs are lines answering \"who did what and when?\".</p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#why-are-audit-logs-important","title":"Why are audit logs important?","text":"<p>Audit logs help both with proactive and reactive security:</p> <ul> <li>Regular audit log reviews give you a chance to catch an attacker before they succeed.</li> <li>After-the-fact, audit logs allow you to gather evidence for forensics and assess the extend of the damage caused by an attacker.</li> </ul>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#what-audit-logs-are-included","title":"What audit logs are included?","text":"<p>Welkin follow a risk-based approach to audit logs. We enable audit logs on all APIs which can be used to compromise data security. Then we filter high-volume low-risk audit logs. Don't forget that, at the end of the day, logs are only as useful as someone looks at them. See log review for details.</p> <p>Specifically, the following audit logs are configured by default:</p> <ul> <li>Kubernetes API audit logs;</li> <li>SSH access logs.</li> </ul> <p>Further audit logs can be configured on a case-by-case basis, as described below.</p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#kubernetes-api-audit-logs","title":"Kubernetes API Audit Logs","text":"<p>The audit logs are stored in the <code>kubeaudit*</code> index pattern. The audit logs cover calls to the Kubernetes API, specifically who did what and when on which Kubernetes Cluster.</p> <p>Thanks to integration with your Identity Provider (IdP), if who is a person, their email address will be shown. If who is a system -- e.g., a CI/CD pipeline -- the name of the ServiceAccount is recorded.</p> <p>Your change management or incident management process should ensure that you also cover why.</p> <p>Both users (Application Developers) and administrators will show in the audit log. The former will change resources related to their application, whereas the latter will change Welkin system components.</p> <p>The exact configuration of the Kubernetes audit logs can be found here.</p> <p>To view the audit logs for a specific user:</p> <ol> <li>Open the <code>Audit user</code> dashboard in OpenSearch;</li> <li>Under <code>User selector</code> add the name of the user you want to audit (e.g admin@example.com);</li> <li>Apply changes;</li> </ol> <p></p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#ssh-access-logs","title":"SSH Access Logs","text":"<p>Welkin also captures highly privileged SSH access to the worker Nodes in the <code>authlog*</code> index pattern. Only administrators should have such access.</p> <p></p> <p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.2.1 User Registration and Deregistration</li> </ul> <p>Many data protection regulation will require you to individually identify administrators, hence individual SSH keys. This allows you to individually identify administrators in the SSH access log.</p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#assessment-on-usage-of-group-accounts","title":"Assessment on Usage of Group Accounts","text":"<p>Usage of group accounts needs to be restricted to exceptional situations and clearly specified, according to:</p> <ul> <li>BDEW \"Anforderungen an sichere Steuerungs- und Telekommunikationssysteme\" v3.0 Requirement 4.5.2;</li> </ul> <ul> <li>ISO/IEC 27002:2022 5.16, 5.17, 5.18, 8.5, 8.15;</li> <li>ISO/IEC 27019:2017 9.2.1, 9.3.1, 9.4.2, 12.4.1</li> </ul> <p>Furthermore, a clear assessment needs to documented to show that such usage does not add an unacceptable risk. This section provides such an assessment.</p> <p>Platform administrators need to perform certain exceptional operations via a very privileged access. These operations are:</p> <ul> <li>Kubespray maintenance;</li> <li>incident resolution at the operating system level;</li> <li>break glass scenarios.</li> </ul> <p>Welkin recommends such access be done by SSH-ing using the group account <code>ubuntu</code> with individual SSH keys, then using <code>sudo</code> to gain access to the <code>root</code> account. This is standard practice with the Ubuntu Cloud Images recommended by Welkin.</p> <p>We assessed such usage not to pose unnecessary risk because:</p> <ul> <li>The individual having logged in can be identified in the SSH Access Logs by looking at the IP address and SSH key. Seeing a log line <code>EVE_IP logged in as EVE_USERNAME using EVE_SSH_KEY</code> does not reduce any risk compared to <code>EVE_IP logged in as ubuntu using EVE_SSH_KEY</code>. Hence, in case of an insider attack or a platform administrator account take-over, one can already narrow down a list of suspected individuals.</li> <li>Even with individual accounts, we are extremely limited in knowing exactly what the platform administrator did after they logged in, due to technical reasons. Most interesting actions are often hidden in temporary scripts, such as those installed in <code>/tmp</code> during normal Ansible usage. If we really wanted to be 100% able to prove that an individual did something bad, then we would need individual accounts and individual root accounts and eBPF intercept all syscalls and log these into a tamper-proof environment. This is, of course, not technically impossible, however very costly in terms of system resources -- not to mention questionable in terms of added security, given the permissions of root accounts.</li> </ul> <p>To mitigate the risk of insider attacks or platform administrator account take-over, Welkin recommends the following security measures:</p> <ul> <li>Severely limit the number of people with platform administrator access.</li> <li>Do background checks on people with platform administrator access.</li> <li>Enforce security hygiene on platform administrator workstations, e.g., no personal errands nor unauthorized applications.</li> <li>Enforce storing SSH keys on a Hardware Security Module (HSM) which requires user interaction before logging in, such as YubiKeys.</li> </ul> <p>Regarding the last point, the BDEW white paper itself recommends:</p> <p>Where technically possible, strong 2-factor authentication shall be used, e.g. through the use of tokens or smart cards.</p> <p>Note that, except these two group accounts on the underlying Linux operating system level on Nodes (<code>ubuntu</code> and <code>root</code>), all Welkin access happens via individual accounts, as illustrated in the Credentials page.</p>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#audit-logs-for-additional-services","title":"Audit Logs for Additional Services","text":"<p>The Kubernetes Audit Logs capture user access to additional services, i.e., <code>kubectl exec</code> or <code>kubectl port-forward</code> commands. Additional services usually do not have audit logging enabled, since that generates a lot of log entries. Too often the extra bandwidth, storage capacity, performance loss comes with little benefit to data security.</p> <p>Prefer audit logs in your application to capture audit-worthy events, such as login, logout, patient record access, patient record change, etc. Resist the temptation to enable audit logging too \"low\" in the stack. Messages like \"Redis client connected\" are plenty and add little value to your data protection posture.</p> <p>Out of all additional services, audit logging for the database makes the most sense. It can be enabled via PGAudit. Make sure you discuss your auditing requirements with the service-specific administrator, to ensure you find the best risk-reduction-to-implementation-cost trade-off. Typically, you want to discuss:</p> <ul> <li>which databases and tables are audited: e.g., audit <code>app.users</code>, but not <code>app.emailsSent</code>;</li> <li>what operations are audited: e.g., audit <code>INSERT/UPDATE/DELETE</code>, but not <code>SELECT</code>;</li> <li>by which users: e.g., audit person access, but not application access.</li> </ul>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/audit-logs/#further-reading","title":"Further Reading","text":"<ul> <li>Kubernetes Auditing</li> <li>PGAudit</li> </ul>","tags":["HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","HIPAA S48 - Audit Controls - \u00a7 164.312(b)","MSBFS 2020:7 4 kap. 16 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.1.7","NIST SP 800-171 3.3.1","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/backup/","title":"Backup Dashboard","text":"","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>S\u00e4kerhetskopiering</p> <p>12 \u00a7 V\u00e5rdgivaren ska s\u00e4kerst\u00e4lla att personuppgifter som behandlas i informationssystem s\u00e4kerhetskopieras med en fastst\u00e4lld periodicitet. S\u00e4kerhetskopiorna ska f\u00f6rvaras p\u00e5 ett s\u00e4kert s\u00e4tt, v\u00e4l \u00e5tskilda fr\u00e5n originaluppgifterna.</p> <p>13 \u00a7 V\u00e5rdgivaren ska besluta om hur l\u00e4nge s\u00e4kerhetskopiorna ska sparas och hur ofta \u00e5terl\u00e4sningstester av kopiorna ska g\u00f6ras.</p> <p>Allm\u00e4nna r\u00e5d: Hur ofta \u00e5terl\u00e4sningstester ska g\u00f6ras b\u00f6r styras av resultaten av \u00e5terkommande riskanalyser.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.3.1 Information Backup</li> <li>A.17.1.1 Planning Information Security Continuity</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#welkin-backup-dashboard","title":"Welkin Backup Dashboard","text":"<p>The Welkin Backup Dashboard allows to quickly audit the status of backups and ensure the Recovery Point Objective are met.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/backup/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>In case there is a violation of backup policies:</p> <ul> <li>Ask the administrator to check the status of the backup jobs.</li> <li>Ask the developers to check if they correctly marked Kubernetes resources with the necessary backup annotations.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering"]},{"location":"ciso-guide/capacity-management/","title":"Capacity Management (Kubernetes Status) Dashboard","text":"","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/capacity-management/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/capacity-management/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>The ability to ensure the ongoing confidentiality, integrity, availability and resilience of processing systems and services; [highlights added]</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/capacity-management/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/capacity-management/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.1.3 Capacity Management</li> </ul> <p>The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/capacity-management/#welkin-status-dashboard","title":"Welkin Status Dashboard","text":"<p>The Welkin Status Dashboard shows a quick overview of the status of your Kubernetes Cluster. This includes:</p> <ul> <li>Unhealthy Pods</li> <li>Unhealthy Nodes</li> <li>Resource requested of the total resources in the Cluster</li> <li>Pods with missing resource requests</li> </ul> <p>This makes it easy to identify when your Cluster is not working correctly and helps you identify configuration that isn't following best practise.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"ciso-guide/cryptography/","title":"Cryptography Dashboard","text":"","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Behandling av personuppgifter i \u00f6ppna n\u00e4t</p> <p>15 \u00a7 Om v\u00e5rdgivaren anv\u00e4nder \u00f6ppna n\u00e4t vid behandling av personuppgifter, ska denne ansvara f\u00f6r att</p> <ol> <li>\u00f6verf\u00f6ring av uppgifterna g\u00f6rs p\u00e5 ett s\u00e5dant s\u00e4tt att inte obeh\u00f6riga kan ta del av dem, och</li> <li>elektronisk \u00e5tkomst eller direkt\u00e5tkomst till uppgifterna f\u00f6reg\u00e5s av stark autentisering.</li> </ol>","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.10.1.2 Key Management</li> </ul>","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#welkin-cryptography-dashboard","title":"Welkin Cryptography Dashboard","text":"<p>The Welkin Cryptography Dashboard allows to quickly audit the status of cryptography. It shows, amongst others, the public Internet endpoints (Ingresses) that are encrypted and the expiry time. Default Welkin configurations automatically renew certificates before expiry.</p>","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/cryptography/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>In case there is a violation of cryptography policies:</p> <ul> <li>If a certificate is expired and was not renewed, ask the administrator to check the status of <code>cert-manager</code> and <code>ingress-controller</code> component.</li> <li>If an endpoint is not encrypted, ask the developers to set the necessary Ingress annotations.</li> </ul>","tags":["HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"ciso-guide/faq/","title":"CISO FAQ","text":"","tags":["ISO 27001 Annex A 5.32 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#do-we-need-to-make-our-application-source-code-public-when-using-welkin","title":"Do we need to make our application source code public when using Welkin?","text":"<p>TL;DR</p> <p>Definitely NOT, you own your application source code and you decide what to do with it.</p> <p>Elastisys hereby confirms that Welkin and its Additional Managed Services (AMS) are NOT putting Application Developers (users) in a situation which obliges them to make their software or source code running on Welkin available to the public.</p> <p>Should we at Elastisys become aware of such an issue existing, we will immediately rectify the situation by replacing problematic components. You can read more about how we take architectural decisions here.</p> <p>As evidence, that the architectural decision process works \u2013 in particular when it comes to licensing issues \u2013 here are some decisions we took:</p> <ul> <li>We decided only to offer TimescaleDB Apache 2 Edition (licensed under Apache 2.0) and NOT TimescaleDB \u201cCommunity Edition\u201d (licensed under the Timescale license). The Timescale license contains some problematic clauses and is, to our knowledge, not tested in court. You can read more about the subtle differences between the TimescaleDB versions by opening this link.</li> <li>We replaced Elasticsearch with OpenSearch (licensed under Apache 2.0), when Elasticsearch changed to the Elastic license. You can read more about the context by opening this link.</li> <li>We replaced InfluxDB with Thanos. This was due to the fact that the open-source version was too limiting. You can read more about this decision by opening this link.</li> <li>We made a risk assessment regarding Grafana, and determined that its AGPL license does not pose a problem. You can read more about our assessment below.</li> </ul> <p>You can read more about our commitment to community-driven open-source by opening this link.</p>","tags":["ISO 27001 Annex A 5.32 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#will-grafanalabs-change-to-agpl-licenses-affect-welkin","title":"Will GrafanaLabs change to AGPL licenses affect Welkin?","text":"<p>TL;DR</p> <p>Users and administrators of Welkin are unaffected.</p> <p>Part of Welkin -- specifically the CISO dashboards -- are built on top of Grafana, which recently changed its license to AGPLv3. In brief, if Grafana is exposed via a network connection -- as is the case with Welkin -- then AGPLv3 requires all source code including modifications to be made available.</p> <p>The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear. Welkin only configures Grafana and does not change its source code. Hence, we determined that Welkin is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3.</p> <p>As a result, Welkin continues to be distributed under Apache 2.0 as before.</p>","tags":["ISO 27001 Annex A 5.32 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#will-minio-change-to-agpl-licenses-affect-welkin","title":"Will Min.io change to AGPL licenses affect Welkin?","text":"<p>TL;DR</p> <p>Users and administrators of Welkin are unaffected.</p> <p>Min.io recently changed its license to AGPLv3.</p> <p>Certain installations of Welkin may use Min.io for accessing object storage on Azure or GCP. However, Welkin does not currently include Min.io. In brief, if Min.io is exposed via a network connection, then AGPLv3 requires all source code including modifications to be made available.</p> <p>The exact difference between \"aggregate\" and \"modified version\" is somewhat unclear. When using Min.io with Welkin, we only use Min.io via its S3-compatible API. Hence, we determined that Welkin is an \"aggregate\" work and is unaffected by the \"viral\" clauses of AGPLv3.</p> <p>As a result, Welkin continues to be distributed under Apache 2.0 as before.</p>","tags":["ISO 27001 Annex A 5.32 Intellectual Property Rights"]},{"location":"ciso-guide/faq/#can-i-use-datadoglogzioelastic-cloud-with-welkin","title":"Can I use Datadog/Logz.io/Elastic Cloud with Welkin?","text":"<p>TL;DR</p> <p>Technically, yes, but legally speaking and from a GDPR perspective, NO. Why is that?</p> <ul> <li>Logs contain personal data.</li> <li>Personal data should NOT be shipped to US cloud providers.</li> </ul> <p>Use Welkin's built-in OpenSearch instead.</p> <p>Application and platform logs are highly likely to contain personal data. Note that, according to GDPR Art. 4 any information that can be directly or indirectly related to an individual is personal data. There are court rulings clarifying that:</p> <ul> <li>email addresses and user IDs are personal data;</li> <li>IP addresses are personal data;</li> <li>browser-generated information (e.g., cookies, URLs, fingerprints, user agents) can be personal data.</li> </ul> <p>According to the so-called \"Schrems II\" ruling, US law -- in particular US CLOUD Act and US FISA are incompatible -- with EU GDPR and personal data processing.</p> <p>Furthermore, according to a French court ruling it doesn't matter if the data-center is located in the EU/EEA. A US company is still under US jurisdiction and considered at risk of US CLOUD Act and US FISA.</p> <p>Most Software-as-a-Service log management platforms -- like Datadog, Logz.io and Elastic Cloud -- are operated by US entities and run on US clouds. Hence, using them to process logs poses a high risk that personal data ends up being processed on a US cloud. Therefore, personal data is at risk of US CLOUD Act and US FISA, which is incompatible with GDPR.</p> <p>Fortunately, Welkin comes with OpenSearch built-in, so you can benefit from full-text search over your application logs while complying with GDPR.</p>","tags":["ISO 27001 Annex A 5.32 Intellectual Property Rights"]},{"location":"ciso-guide/gdpr-art-17/","title":"How do I comply with GDPR Art. 17 Right to erasure (\"right to be forgotten\")?","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p>","tags":["GDPR Art. 17 Right to erasure (\"right to be forgotten\")","ISO 27001 Annex A 8.10 Information Deletion"]},{"location":"ciso-guide/gdpr-art-17/#how-do-i-comply-with-gdpr-art-17-right-to-erasure-right-to-be-forgotten","title":"How do I comply with GDPR Art. 17 Right to erasure (\"right to be forgotten\")?","text":"<p>Here is how we recommend to comply with GDPR Art. 17 if you host your application on Welkin.</p> <p>Note that, this page assumes you have no other sub-processors than your Welkin supplier.</p>","tags":["GDPR Art. 17 Right to erasure (\"right to be forgotten\")","ISO 27001 Annex A 8.10 Information Deletion"]},{"location":"ciso-guide/gdpr-art-17/#preparation","title":"Preparation","text":"<p>Prepare as follows:</p> <ol> <li>Map out where you store personal data. Besides production database and production PersistentVolumes, this may include backups, audit logs and application logs.</li> <li>Check the retention period of backups and logs. By default, there are 30 days in Welkin, but can be changed if needed.</li> <li>Devise a process to remember to re-delete forgotten data after disaster recovery. For example, you might want to review all GDPR Art. 17 requests received within the last 30 days after restoring data from backups.</li> </ol>","tags":["GDPR Art. 17 Right to erasure (\"right to be forgotten\")","ISO 27001 Annex A 8.10 Information Deletion"]},{"location":"ciso-guide/gdpr-art-17/#when-receiving-a-request-from-a-data-subject","title":"When receiving a request from a data subject","text":"<p>Once you have these in place, we recommend you proceed as follows when receiving a request from a data subject.</p> <ol> <li>Track the request of the data subject in some internal system, e.g., email or a service ticket system. Try to keep as little personal data as possible, e.g., only contact email.</li> <li>Delete data from the production database. No need to write code. Just issue a command like <code>DELETE FROM users WHERE userId=?</code>. Welkin will record in its audit logs that an Application Developer connected directly to the database. For extra security via traceability, you can even enable PostgreSQL audit logs.</li> <li> <p>Reply to the data subject with an email like the following. This email assumes the default backup and log retention period of 30 days.</p> <p>Hello data subject,</p> <p>As requested, we removed your personal data from our database.</p> <p>Please note that your data may persist for 30 days in our backups, audit logs and application logs. We have procedures in place to make sure that we re-delete your personal data if we ever need to restore from backups within the next 30 days.</p> <p>After 30 days, your personal data will be removed from backups and I will delete this email, so you will be forever forgotten.</p> </li> <li> <p>Delete the request from your internal system after 30 days.</p> </li> </ol>","tags":["GDPR Art. 17 Right to erasure (\"right to be forgotten\")","ISO 27001 Annex A 8.10 Information Deletion"]},{"location":"ciso-guide/gdpr-art-17/#further-reading","title":"Further Reading","text":"<ul> <li>Audit Logs</li> <li>Application Logs</li> <li>Backups</li> </ul>","tags":["GDPR Art. 17 Right to erasure (\"right to be forgotten\")","ISO 27001 Annex A 8.10 Information Deletion"]},{"location":"ciso-guide/intrusion-detection/","title":"Intrusion Detection Dashboard","text":"","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst</p> <p>18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter.</p>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.2.1 Controls Against Malware</li> <li>A.12.6.1 Management of Technical Vulnerabilities</li> <li>A.16.1.7 Collection of Evidence</li> </ul>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#welkin-intrusion-detection-dashboard","title":"Welkin Intrusion Detection Dashboard","text":"<p>The Welkin Intrusion Detection Dashboard allows to quickly audit any suspicious activity performed by code inside the Cluster, such as writing to suspicious files (e.g., in <code>/etc</code>) or attempting suspicious external network connections (e.g., SSH to a command-and-control server). Such activities may indicate anything from a misconfiguration issue to an ongoing attack. Therefore, this dashboard should be regularly reviewed, perhaps even daily.</p>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#intrusion-detection-rules","title":"Intrusion Detection Rules","text":"<p>Welkin's Mission and Vision requires all alerts to be actionable.</p> <p>Therefore, Welkin enables by default the so-called Falco stable rules. The Falco incubating and Falco sandbox rules are disabled by default and can be enabled by configuring <code>falco.customRules</code> accordingly.</p>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/intrusion-detection/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Platform Administrators need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker.</p> <p>In less severe cases, simply contact the developers to investigate their code and fix any potential misconfiguration.</p>","tags":["HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","MSBFS 2020:7 4 kap. 18 \u00a7","NIST SP 800-171 3.14.6","NIST SP 800-171 3.14.7","ISO 27001 Annex A 8.7 Protection Against Malware","ISO 27001 Annex A 8.16 Monitoring Activities"]},{"location":"ciso-guide/isa-iec-62443/","title":"ISA/IEC 62443","text":"<p>There are several standards and frameworks whose goal is to improve the security of Industrial Control Systems (ICS). One of these is ISA/IEC 62443, whose purpose is to improve the availability, integrity and confidentiality of ICS.</p> <p>This page is aimed at system owners and explains how Welkin fulfills the foundational requirements of ISA/IEC 62443.</p>"},{"location":"ciso-guide/isa-iec-62443/#fr-1-identification-and-authentication-control","title":"FR 1: Identification and Authentication Control","text":"<p>All Welkin Service Endpoints are exposed via HTTPS and require OpenID-based authentication with an Identity Provider (IdP). Provided that your IdP is configured securely, this means that Welkin can only be accessed via individual usernames and passwords.</p> <p>You may further protect Welkin Service Endpoints as follows:</p> <ul> <li>Configure your IdP with Multi-Factor Authentication (MFA): This removes the risk of an attacker gaining access to ICS via a compromised password.</li> <li>Configure IP allowlisting: This adds one more layer of protection in that an attacker must first gain access to an internal network before mounting an attack.</li> <li>Run Welkin in an air-gapped network: This means that platform administrators and application developers need on-site presence to gain access to Service Endpoints.</li> </ul>"},{"location":"ciso-guide/isa-iec-62443/#fr-2-use-control","title":"FR 2: Use Control","text":"<p>All Welkin Service Endpoints provide for access control and leave an audit log.</p> <p>When it comes to access control, Welkin enforces a strong distinguishing between:</p> <ul> <li>Application Developers, who are limited to administering their application; and</li> <li>Platform Administrators, who have more permissions, as required for platform maintenance and troubleshooting.</li> </ul> <p>Furthermore, each Service Endpoint features fine-grained access control. For example the Kubernetes Endpoint implements Kubernetes Role-Based Access Control (RBAC).</p> <p>Audit logs store information about who did what and when. Combined with periodic log reviews they form a powerful tool to deter and detect unauthorized access.</p>"},{"location":"ciso-guide/isa-iec-62443/#fr-3-system-integrity-and-fr-4-data-confidentiality","title":"FR 3: System Integrity and FR 4: Data Confidentiality","text":"<p>The fine-grained access control described above is carefully configure to ensure system integrity and data confidentiality. For example, Welkin technically prevents Application Developers to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc.</p> <p>Furthermore, Welkin comes with various guardrails to make it hard to Application Developers to do the wrong thing, like running containers as root. This ensures both system integrity and data confidentiality, e.g., Application Developers cannot take over the operating system on a Node.</p>"},{"location":"ciso-guide/isa-iec-62443/#fr-5-restricted-data-flow-microsegmentation","title":"FR 5: Restricted Data Flow (Microsegmentation)","text":"<p>Welkin restricts data flows at three levels:</p> <ul> <li>Infrastructure: Provided the infrastructure supports this, the servers composing Welkin should always be put on a private network, fronted by a load-balancer.   The load-balancer should restrict communication to port 80 (TCP ACME) and 443 (HTTPS).</li> <li>Platform: Within the platform, most Welkin components have NetworkPolicies in place.   NetworkPolicies are roughly equivalent to firewall in a containerized environment.   This ensures that components can only communicate to one-another on an \"as-needed\" basis, and severely restricts the ability to exploit certain vulnerabilities, such as the infamous Log4j vulnerability.</li> <li>Application: Application Developers should ship their application with NetworkPolicies to restrict data flows, as described on the Network Model page.   By default, Welkin warns Application Developer if NetworkPolicies are missing.</li> </ul>"},{"location":"ciso-guide/isa-iec-62443/#fr-6-timely-response-to-events-incident-management","title":"FR 6: Timely Response to Events (Incident Management)","text":"<p>Welkin issues alerts when the application or platform requires human attention. Welkin comes with many built-in alerts to allow the Platform Administrator to start troubleshooting before an incident happens.</p> <p>The Application Developer is empowered to configure alerts for their application either via log-based alerting or via metrics.</p>"},{"location":"ciso-guide/isa-iec-62443/#fr-7-resource-availability","title":"FR 7: Resource Availability","text":"<p>Welkin comes with a go-live checklist to ensure that the environment is provided with sufficient capacity even in case of Node (single server) or Zone (whole data-center) failure.</p> <p>Furthermore, Welkin comes with capacity-related alerts to alert the Platform Administrator days in advance when capacity needs to be added to the environment.</p>"},{"location":"ciso-guide/isa-iec-62443/#further-reading","title":"Further Reading","text":"<ul> <li>MSB, Standard: ISA/IEC 62443</li> <li>Wikipedia, Operational technology</li> <li>Wikipedia, Industrial control systems</li> </ul>"},{"location":"ciso-guide/log-review/","title":"Log Review","text":"<p>This document highlights the risks that can be mitigated by regularly reviewing logs and makes concrete recommendations on how to do log review.</p>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art, the costs of implementation and the nature, scope, context and purposes of processing as well as the risk of varying likelihood and severity for the rights and freedoms of natural persons, the controller and the processor shall implement appropriate technical and organisational measures to ensure a level of security appropriate to the risk, including inter alia as appropriate: [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40</p> <p>2 \u00a7 V\u00e5rdgivaren ska genom ledningssystemet s\u00e4kerst\u00e4lla att [...] 4. \u00e5tg\u00e4rder kan h\u00e4rledas till en anv\u00e4ndare (sp\u00e5rbarhet) i informationssystem som \u00e4r helt eller delvis automatiserade.</p>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.4.1 \"Event Logging\"</li> <li>A.12.4.3 \"Administrator and Operator Logs\"</li> </ul>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#purpose","title":"Purpose","text":"<p>Welkin captures application logs and audit logs in a tamper-proof logging environment, which we call the Management Cluster. By \"tamper-proof\", we mean that even a complete compromise of production infrastructure does not allow an attacker to erase or change existing log entries, as would be required to hide their activity and avoid suspicion.</p> <p>Note</p> <p>Attackers can, however, inject new \"weird\" logs entries. However, that wouldn't remove their tracks and would only trigger more suspicion.</p> <p>However, said logs only help with information security if they are regularly reviewed for suspicious activity. Prefer to use logs for catching \"unknown unknowns\". For known bad failures -- e.g., a Fluentd Pod restarting -- prefer alerts.</p>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#risks","title":"Risks","text":"<p>Periodically reviewing logs can mitigate the following information security risks:</p> <ul> <li>Information disclosure: Regularly reviewing logs can reveal an attack attempt or an ongoing attack.</li> <li>Downtime: Regularly reviewing logs can reveal misbehaving components (e.g., Pod restarts, various errors) and inform fixes before it leads to downtime.</li> <li>Silent corruption: Regularly reviewing logs can reveal data corruption.</li> </ul>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#how-to-do-log-review","title":"How to do log review","text":"<p>By review period, we mean the time elapsed since the last review of the logs, e.g., 30 days.</p> <p>Aim for a review which is both wide and deep. By wide we mean that you should vary the time interval, time point, filters, etc., when reviewing log entries. By deep we mean that you should actually read and try to understand a sample of logs.</p> <ol> <li>Open up a browser and open the Welkin logs of the Cluster you are reviewing. This functionality is currently offered by OpenSearch.</li> <li>Search for the following keywords on all indices -- i.e., search over each index pattern -- over the last review period: <code>error</code>, <code>failed</code>, <code>failure</code>, <code>deny</code>, <code>denied</code>, <code>blocked</code>, <code>invalid</code>, <code>expired</code>, <code>unable</code>, <code>unauthorized</code>, <code>bad</code>, <code>401</code>, <code>403</code>, <code>500</code>, <code>unknown</code>. Sample a few keywords you recently encountered during your work, e.g., <code>already installed</code> or <code>not found</code>; be creative and unpredictable.</li> <li>Vary the time point, the time interval, filters, etc.</li> <li>Include the total amount of logs in each log category in your review (set the time interval bigger than retention). Is it the same, significantly less or significantly more logs compared to the last check? If there is a major difference, it could be worth investigating further to figure out why that is.</li> <li>Go wide: For each query (index pattern, keyword, timepoint, time interval and filter combination), look at the timeline and see if there is an unexpected increase or decrease in the count of log lines. If you find any, focus your attention on those.</li> <li>Go deep: For each query, sample at least 10 log entries, read them and make sure you understand what they mean. Think about the following:<ul> <li>What are potential causes?</li> <li>What are potential implications?</li> <li>Time: Do the entries appear periodically or randomly?</li> <li>Space: Does a specific component trigger them? Is the entry generated by the platform or the application?</li> </ul> </li> <li>If anything catches your attention vary the time point, time interval and various filters to understand if the log entry is a risk indicator or not. Look for unknown unknowns. Any failures, especially authentication failures, which feature a significant increase are risk indicators.</li> <li>Contact the person owning the component, e.g., the Application Developer or Welkin architect, to better understand if the entry is suspicious or not. Perhaps it is due to a recent change -- as indicated by an operator log -- and indicates no risk.</li> </ol>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#log-review-dashboard","title":"Log review dashboard","text":"<p>There is a dashboard made to get a overview of the log landscape.</p>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/log-review/#possible-resolutions","title":"Possible resolutions","text":"<ol> <li>If you found a suspicious activity, escalate.</li> <li>If the log entry is due to a bug in Welkin, file an issue.</li> </ol>","tags":["HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","MSBFS 2020:7 4 kap. 17 \u00a7","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","NIST SP 800-171 3.3.3","NIST SP 800-171 3.3.5","NIST SP 800-171 3.3.6"]},{"location":"ciso-guide/network-security/","title":"Network Security Dashboard","text":"","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] encryption of personal data;</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>Utv\u00e4rdering av skyddet mot olovlig \u00e5tkomst</p> <p>18 \u00a7 V\u00e5rdgivaren ska \u00e5rligen utv\u00e4rdera skyddet mot s\u00e5v\u00e4l intern som extern olovlig \u00e5tkomst till datorn\u00e4tverk och informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter.</p>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.13 Communications Security</li> </ul>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#welkin-network-security-dashboard","title":"Welkin Network Security Dashboard","text":"<p>The Welkin Network Security Dashboard allows to audit violations of NetworkPolicies (i.e., \"firewall rules\"). In the best case, denied traffic indicates a misconfiguration. In worst case, denied traffic indicates an ongoing security attack.</p> <p>Significant or unexpected increases of allowed traffic should also be closely monitored. In best case, these may indicate inefficient application code which may cause capacity issues later. In worst case, these may indicate an attempt to exfiltrate large amounts of data or to use the Cluster as a reflector for an amplification attack.</p> <p>Therefore, this dashboard should be regularly reviewed, perhaps even daily.</p>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Make sure you have a proper incident management policy in place. If an attack is ongoing, it might be better to take the system offline to protect data from getting in the wrong hands. Platform Administrator need to be trained on what events justify such an extreme action, otherwise, escalating the issue along the reporting chain may add delays that favor the attacker.</p> <p>In less severe cases, simply contact the developers to investigate their code, fix needless communication attempts or update their NetworkPolicies accordingly to fix any potential misconfiguration.</p>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/network-security/#further-reading","title":"Further Reading","text":"<ul> <li>Network Policies</li> </ul>","tags":["ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"]},{"location":"ciso-guide/pdl-audit-logs/","title":"How do I comply with HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter?","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p>","tags":["HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/pdl-audit-logs/#how-do-i-comply-with-hslf-fs-201640-4-kap-9-kontroll-av-atkomst-till-uppgifter","title":"How do I comply with HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter?","text":"<p>Here is how we recommend to comply with Swedish Patient Data Laws, specifically HSLF-FS 2016:40 4 kap. 9 \u00a7 if you host your application on Welkin.</p>","tags":["HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/pdl-audit-logs/#legal-context","title":"Legal Context","text":"<p>Swedish Patient Data Laws mandate having audit logs (who did what and when) for accesses to healthcare data. Socialstyrelsen has rulemaking power. Their regulation, HSLF-FS 2016:40, states the following:</p> <p>9 \u00a7 V\u00e5rdgivaren ska ansvara f\u00f6r att</p> <ol> <li>det av dokumentationen av \u00e5tkomsten (loggar) framg\u00e5r vilka \u00e5tg\u00e4rder som har vidtagits med uppgifter om en patient,</li> <li>det av loggarna framg\u00e5r vid vilken v\u00e5rdenhet eller v\u00e5rdprocess \u00e5tg\u00e4rderna vidtagits,</li> <li>det av loggarna framg\u00e5r vid vilken tidpunkt \u00e5tg\u00e4rderna vidtagits,</li> <li>anv\u00e4ndarens och patientens identitet framg\u00e5r av loggarna,</li> <li>systematiska och \u00e5terkommande stickprovskontroller av loggarna g\u00f6rs,</li> <li>kontroller av loggarna dokumenteras, och</li> <li>loggarna sparas minst fem \u00e5r f\u00f6r att m\u00f6jligg\u00f6ra kontroll av \u00e5tkomsten till uppgifter om en patient.</li> </ol>","tags":["HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/pdl-audit-logs/#technical-context","title":"Technical Context","text":"<p>In IT systems used in healthcare, we usually have two types of accesses:</p> <ul> <li>\"Front-door\" access used by the caregiver, e.g., via a web browser or mobile app.</li> <li>\"Back-door\" access used by the technician, e.g., via the Kubernetes API, SSH-ing to the server, walking into the server room.</li> </ul> <p>It is clear that HSLF-FS 2016:40 4 kap. 9 \u00a7 demands a 5-year retention period for access logs generated by caregiver access. (They also want said logs to be regularly reviewed, but that's a different story.) Application Developer can read this page for details on how to correctly implement this part.</p> <p>For access logs generated by technicians -- i.e., application developers, platform administrators and the administrators of infrastructure providers -- the answer is complex. We had discussions with IMY, and they seem to want us to observe both:</p> <ul> <li>Art. 5 GDPR \"purpose limitation\", which pushes to shorter platform audit logs;</li> <li>Art. 32 GDPR \"security of processing\", which pushes to longer platform audit logs; in fact, IMY gave a reprimand for missing access logs in some cases.</li> </ul> <p>We talked to some industry experts. Some interpreted this as \"retain platform access logs for exactly 30 days and no more\". Others interpreted this to be 90 days.</p> <p>We recommend you set up long-term log retention (5 years) for relevant application logs according to the link above, but retain platform audit logs for 30 days.</p>","tags":["HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/pdl-audit-logs/#further-reading","title":"Further Reading","text":"<ul> <li>Audit Logs</li> <li>Long-term Log Retention</li> </ul> <ul> <li>Senaste version av HSLF-FS 2016:40 Socialstyrelsens f\u00f6reskrifter och allm\u00e4nna r\u00e5d om journalf\u00f6ring och behandling av personuppgifter i h\u00e4lso- och sjukv\u00e5rden</li> </ul>","tags":["HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"]},{"location":"ciso-guide/policy-as-code/","title":"Policy-as-Code Dashboard","text":"","tags":["ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security"]},{"location":"ciso-guide/policy-as-code/#relevant-regulations","title":"Relevant Regulations","text":"<p>Although \"policy-as-code\" is not explicit in any regulation, enforcing policies in a consistent technical manner (\"policy-as-code\") is seen as an important strategy to reduce compliance violations, as well as reduce the overhead of complying.</p>","tags":["ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security"]},{"location":"ciso-guide/policy-as-code/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.18.2.2 Compliance with Security Policies &amp; Standards</li> <li>A.18.2.3 Technical Compliance Review</li> </ul>","tags":["ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security"]},{"location":"ciso-guide/policy-as-code/#welkin-policy-as-code-dashboard","title":"Welkin Policy-as-Code Dashboard","text":"<p>Some of your policies are best enforced in code, e.g., Ingress resources do not have encryption set up or PersistentVolumeClaims do not have the necessary backup annotations. Setting up such policies as code is highly dependent on your organization, your risk appetite and your operations. Policies that make sense enforcing by code may be required in some organizations, whereas others might see it as unnecessary and prefer simply treat codified policies as aspirational.</p> <p>Whatever your situation, the Welkin Policy-as-Code Dashboard allows to quickly audit what Kubernetes resources are set up in a non-compliant way or how many policy violations were avoided by Welkin.</p>","tags":["ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security"]},{"location":"ciso-guide/policy-as-code/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>If an application or user keeps violating a policy, start by reviewing the policy. If the policy seems well codified, contact the developer or the application owner to determine why policy violations occurs or need to be prevented by Welkin.</p> <p>If a policy is missing or too strict, contact the Welkin administrators.</p>","tags":["ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security"]},{"location":"ciso-guide/vulnerability/","title":"Vulnerability Dashboard","text":"","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#relevant-regulations","title":"Relevant Regulations","text":"","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#gdpr","title":"GDPR","text":"<p>GDPR Article 32:</p> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#swedish-patient-data-law","title":"Swedish Patient Data Law","text":"<p>Note</p> <p>This regulation is only available in Swedish. To avoid confusion, we decided not to produce an unofficial translation.</p> <p>HSLF-FS 2016:40:</p> <p>10 \u00a7 V\u00e5rdgivaren ska vid utveckling, idrifttagande och \u00e4ndring av informationssystem som anv\u00e4nds f\u00f6r behandling av personuppgifter s\u00e4kerst\u00e4lla att personuppgifternas tillg\u00e4nglighet, riktighet, konfidentialitet och sp\u00e5rbarhet inte riskeras.</p>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#mapping-to-iso-27001-controls","title":"Mapping to ISO 27001 Controls","text":"<ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#welkin-vulnerability-dashboard","title":"Welkin Vulnerability Dashboard","text":"<p>The Welkin Vulnerability Dashboard (Trivy Operator Dashboard) allows to audit what vulnerable container images are running in production. The dashboard allows to asses increase or decrease of exposure over time. It also allows to prioritize vulnerabilities based on CVE score (CVSS).</p> <p>Therefore, this dashboard should be regularly reviewed, perhaps even daily. A vulnerability management process should be in place to decide how to systematically handle vulnerabilities.</p>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#handling-non-compliance","title":"Handling Non-Compliance","text":"<p>Containers should preferably be redeployed with an image that received the necessary security fixes. In case the security fix cannot be deployed in a timely manner -- e.g., due to a slow fix from the vendor -- then the affected containers should be terminated. In all cases, isolating a container using NetworkPolicies, non-root user accounts, no service account token, etc. can make a vulnerability more difficult to exploit.</p>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/vulnerability/#further-reading","title":"Further Reading","text":"<ul> <li>Vulnerability management</li> <li>CVE</li> <li>CVSS</li> <li>Trivy-Operator</li> </ul>","tags":["HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.7.4","NIST SP 800-171 3.7.5","NIST SP 800-171 3.11.2","NIST SP 800-171 3.14.4","NIST SP 800-171 3.14.5","NIS2 Minimum Requirement (e) Vulnerability Handling","ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities"]},{"location":"ciso-guide/information-security/","title":"Information security and DevSecOps best practices","text":"<p>As per the project's mission and vision, Welkin is designed from the ground up to embody security best practices. These pages showcase how this commitment relates to industry best practices.</p> <ul> <li>US Department of Defense Enterprise DevSecOps Reference Design for Kubernetes</li> </ul>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/","title":"DoD Enterprise DevSecOps Reference Design for Kubernetes","text":"<p>This page highlights the project's commitment to security best practices by comparing it to the US Department of Defense's DevSecOps Reference design. The section is based on the document entitled \"DoD Enterprise DevSecOps Reference Design: CNCF Kubernetes\", version 2.0, published in March 2021 (PDF download link).</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#disclaimer","title":"Disclaimer","text":"<p>The DoD Enterprise DevSecOps Reference Design (henceforth referred to as the Reference Design) is a publicly available document, and has had a profound impact on the field. However, it also makes references to DoD-specific systems and other DoD or US Military-internal systems, such as:</p> <ul> <li>\"Iron Bank\" container registry and associated requirements</li> <li>\"Platform One\" Kubernetes platform</li> </ul> <p>The exact properties and the functionality of these systems are not public.</p> <p>This section is therefore written from the perspective of a typical Welkin application developer that wishes to adhere to the best practices that are properly and publicly available from the Reference Design.</p> <p>The rest of this page will go section-by-section through the Reference Design and map it to Welkin.</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#section-1-introduction","title":"Section 1, Introduction","text":"<p>The first section of the Reference Design defines the background, purpose, and scope of the document. It also lists the intended audience and how the document relates to others in the same suite.</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#section-2-assumptions-and-principles","title":"Section 2, Assumptions and Principles","text":"<p>The second section of the Reference Design clarifies assumptions and principles. These are addressed in the following list:</p> <ul> <li>Welkin is a Certified Kubernetes distribution.</li> <li>Welkin has no vendor-specific tooling, instead opting for pure open source that is portable and avoids vendor lock-in (as per ADR-0015).</li> <li>Welkin does not dictate a particular definition of \"hardened containers\". Instead, Welkin puts multiple important guardrails in place, such as:<ul> <li>forbidding containers to run as root</li> <li>mandating adherence to the Restricted Pod Security Standard;</li> <li>requiring microsegmentation via Network Policies;</li> <li>vulnerability scanning;\\     intrusion detection for running applications; and</li> <li>requiring container images to be pulled from only allowlisted container registries.</li> </ul> </li> </ul> <p>Taken together, these measures provide comprehensive defense in depth.</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#section-3-software-factory-interconnects","title":"Section 3, Software Factory Interconnects","text":"<p>Note that the majority of Section 3 of the Reference Design describes the software distribution that DoD has in place, which is not public.</p> <p>Table 1 (Page 15) makes it clear that the DoD and the Welkin have divergent views on what is appropriate to run inside a Pod. The features listed there make it clear that the sidecar that DoD issues in their environments must run with very high privilege. Otherwise, features such as rerouting all network traffic via service mesh, scanning the underlying Node for vulnerabilities, and adaptively restricting the main container's runtime behavior would simply not be possible.</p> <p>In the view of Welkin, this means that such a sidecar would itself become an attack vector. By design, it is reachable by the application running within the main container in the Pod. Given its high level of privilege, it is going to be a highly valuable target.</p> <p>Instead, Welkin takes a more restrictive approach to containers that run within it. We add layers upon layers of security on top of it, and keep it running with low privileges. This means that all guardrails put in place work for us, rather than against us. And we do not circumvent them by putting all our faith in a privileged sidecar.</p> <p>Table 1 calls out the following named security features, and we describe what we do within that space as follows:</p> <ul> <li>Logging Agent, for which we use Fluentd to forward all logs into the logging system. Going beyond the requirements of the Reference Design, our setup with two Clusters ensures that no application in the Workload Cluster can overwrite or modify the logs that are stored in the OpenSearch instance in the Management Cluster. Thereby, we offer tamper-proof logging, which the DoD Reference Design does not.</li> <li>Logging Storage and Retrieval, which is what OpenSearch is for.</li> <li>Log Visualization and Analysis, which is what OpenSearch Dashboards offer.</li> <li>Container Policy Enforcement, which in the DoD Reference Design is about ensuring compliance with the US military's Security Content Automation Protocol (SCAP). In Welkin, the following guardrails provide defence in depth, which is similar in scope:<ul> <li>forbidding containers to run as root;</li> <li>mandating adherence to the Restricted Pod Security Standard;</li> <li>requiring microsegmentation via Network Policies providing both vulnerability scanning and intrusion detection for applications; and</li> <li>enforcing container images pulls from only allowlisted container registries.</li> </ul> </li> <li>Runtime Defense, which in Welkin is offered via the intrusion detection system Falco.</li> <li>Service Mesh Proxy and Service Mesh, which the document later spells out as offering in-Cluster workload identity and network traffic encryption features. In all Kubernetes-integrated service meshes, the workload identity ultimately comes from the Kubernetes API, which means that they are from a security perspective equivalent to the Pod selectors used in e.g. Network Policies: if an attacker can fool or impersonate the Kubernetes API server, no amount of additional use of cryptography will help because the Kubernetes API as source of truth is regarded as authoritative. Network traffic can be encrypted in Welkin by enabling Node-to-Node encryption in Calico, the CNI of choice. So while Welkin does not ship with a service mesh, it has equivalent workload identity and over-the-wire network security features as one.</li> <li>Vulnerability Management and CVE Service, for which Welkin includes the Trivy vulnerability scanner for both the Harbor container registry and for scanning the running workloads using the Trivy Operator.</li> <li>Host Based Security, which Welkin addresses by making it easy to apply upgrades and to reboot Nodes with Kured or to replace them, depending on if the Cluster is installed via Kubespray or Cluster API.</li> <li>Artifact Repository, which in Welkin is offered by the Harbor container registry.</li> <li>Zero Trust Model Down to the Container Level, which is poorly defined in the Reference Design document, but the table contents suggests that it relates to the features of a Service Mesh, so see that bullet for the answer.</li> </ul>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#section-4-software-factory-kubernetes-reference-design","title":"Section 4, Software Factory Kubernetes Reference Design","text":"<p>Section 4 of the Reference Design is highly DoD-specific, but Application Developers in Welkin are recommended to take inspiration from it in setting up their own CI/CD systems. As for the final step, deploying to Welkin, it is recommended to either use the Argo CD additional managed service or to set up Deployment as a final step in a CI system, as per our instructions.</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#section-5-additional-tools-and-activities","title":"Section 5, Additional Tools and Activities","text":"<p>Section 5 of the Reference Design summarizes requirements from other sections and concretizes what is written in a related document, the \"DevSecOps Tools and Activities Guidebook\" (version 2.0 PDF from March 2021 download link).</p>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#container-hardening","title":"Container Hardening","text":"<p>Application Developers may be interested in extracting the steps and learnings that are not DoD-specific from the \"Container Hardening Processing Guide\" by DISA and DoD (version 1, release 2 PDF from August 2022 download link). Disregarding all references to DoD-internal systems the following useful information remains for public consumption:</p> <ul> <li>Define a base image that your organization issues and can properly secure.</li> <li>Carry out vulnerability management on the base image. Let changes ripple to all other container images.</li> <li>Ensure code dependencies are available through some means that does not require Internet access, if ability to build cannot be allowed to be compromised by a non-working Internet connection or server availability. Take care to also avoid downloading assets within Dockerfiles, for the same reason.</li> <li>Understand default configuration, because it will be in effect in addition to your explicit configuration. Consider setting all configuration values explicitly, just like it is common practice to set dependencies explicitly. You would rather get an error at startup than strange behavior during runtime.</li> <li>Emit logs to stdout, to make it possible to collect them via a log forwarder.</li> <li>Always run containers as a non-root user.</li> <li>Do not hard-code configuration or secret values, instead opting to get such values via ConfigMaps or Secrets in Kubernetes, injected using one of the standard options: environment variables, arguments, or as files in the Pod.</li> <li>Remove all packages that are not required, since they just add to the attack surface of the container.</li> <li>Consider removing setuid and setgid flags from binaries in your container, in a paranoid addition to requiring Kubernetes Pods to have <code>allowPrivilegeEscalation</code> set to false.</li> <li>Scan containers for known vulnerabilities, and have a process in place to prevent ones with unacceptably high scores from being deployed to production. What that means for your organization is up to you.</li> </ul>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#general-tools-and-activity-advice","title":"General Tools and Activity Advice","text":"<ul> <li>Using a GitOps-based Deployment tool and process is advised, because of three reasons:<ul> <li>Eliminates the need to open ports and to share keys with externally running CI/CD services, such as GitHub Actions.</li> <li>Eliminates environment configuration drift.</li> <li>Ensures the desired state is always represented in Git, which (although not mentioned in the Reference Design document) also brings the benefits of clear and non-repudiable auditability, thanks to signed commits.</li> </ul> </li> <li>Analyze network flows between components. In Welkin, this is done using the Jaeger additional managed service for applications deployed onto the environment. This is not the same as a full network flow analyzer, because it doesn't work on the low level of capturing all traffic across the entire network. However, it does show to a fine level of detail how applications interact with each other. The fact that it's more focused on application-only traffic may both be a strength or a weakness, depending on perspective.</li> <li>Section 5.1 lists the common advanced Deployment strategies: Blue/Green, Canary, Rolling, and Continuous (which is a special case of Rolling). All these can be implemented in Welkin in Argo and with a bit more effort in a manual or scripted way using normal Kubernetes primitives.</li> <li>Section 5.2 is concerned with monitoring, logging, and alerting based on monitoring and logging. These are supported in Welkin, thanks to the Prometheus monitoring stack which includes Alert Manager, and log-based alerting in OpenSearch.</li> </ul>"},{"location":"ciso-guide/information-security/dod-enterprise-devsecops-reference-design-kubernetes/#summary","title":"Summary","text":"<p>This page has shown how Welkin incorporates a large number of the best practices that are described in the \"DoD Enterprise DevSecOps Reference Design: CNCF Kubernetes\", version 2.0, published in March 2021 (PDF download link). The most common source of deviation from them is the fact that the Reference Design refers to DoD-internal systems and practices, which are not publicly available, and hence not applicable to organizations outside of the US military.</p> <p>Applications deployed on Welkin benefit from a comprehensive defense in depth approach on the platform and application runtime level. Application developers are recommended to harden their containers as described in Section 5, and to release applications using fully automated GitOps.</p>"},{"location":"contributor-guide/","title":"Contributor guide","text":"","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#definition-of-done","title":"Definition of Done","text":"<p>When working in regulated industries, it is really important to have the bar high for when something can be called \"done\". In Welkin, we use the following definition of done:</p> <ol> <li>Code and documentation is merged on the main branch of upstream projects. This may cause time delays which are outside your control. However, if we cannot convince upstream projects to take our contributions, then we better know about this as soon as possible. A Welkin relying on an abandoned upstream branch is unsustainable.</li> <li>Code is merged in the Welkin project.</li> <li>Documentation is up-to-date. IT systems used in regulated industries need to have documentation. (See ISO 27001 A.12.1.1 \"Documented Operating Procedures\").     You may either point to upstream documentation -- if Welkin does not add any specifics -- or write a dedicated section/page. Prefer to refer to upstream documentation -- potentially updating that one -- instead of duplicating it in Welkin.</li> <li>You provide evidence for completion. This can be terminal output, screenshot or -- even better, but more time consuming -- a screencast with voice-over explanations. Ideally, these should be attached in the PR to convince the reviewer that the code and documentation are as intended.</li> </ol>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#submitting-prs","title":"Submitting PRs","text":"<p>To make the review process as smooth as possible for everyone we have some steps that we'd like you to follow</p> <ul> <li> <p>Look through our DEVELOPMENT.md</p> </li> <li> <p>The pre-commit hook will run on all PRs to <code>main</code>, so either make sure to have it installed by running:</p> </li> </ul> <pre><code>pre-commit install\n</code></pre> <p>Or manually run it before committing</p> <pre><code>pre-commit run\n</code></pre> <ul> <li>Make sure to follow the PR template, see this for more details.   Alternatively start a PR and you'll see it there.</li> </ul>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#setting-up-your-environment","title":"Setting up your environment","text":"<p>To install all required tools, please follow the instructions here.</p>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#tips-and-tricks","title":"Tips and tricks","text":"<p>To make your life easier we suggest to use language server for the language that you're editing.</p> <p>E.g.</p> <ul> <li>Terraform: <code>terraform-ls</code></li> <li>YAML: <code>yaml-language-server</code></li> </ul> <p>To catch pre-commit errors early, direct in your editor, it's also suggested to install plugins for these tools.</p> <ul> <li><code>markdownlint</code></li> <li><code>shellcheck</code></li> </ul> <p>When developing and you only working on a single application it will be faster to only deploy that application instead of applying all charts. This can be done by figuring out the app label for the application in question by running:</p> <pre><code>bin/ck8s ops helmfile {wc|sc} list\n</code></pre> <p>When you figured out the app label (lets say it's <code>dex</code> in this case) you can check the diff of your work by running:</p> <pre><code>bin/ck8s ops helmfile {wc|sc} -l app=dex diff\n</code></pre> <p>Instead of running <code>helmfile apply</code>, it might be useful to run <code>helmfile sync</code>. This will do a 3-way upgrade and make sure that the Helm state matches the objects actually running in Kubernetes. This will make sure that you haven't manually edited something for debugging and forgot about it.</p> <pre><code>bin/ck8s ops helmfile {wc|sc} -l app=dex sync\n</code></pre>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#object-storage","title":"Object storage","text":"<p>To make creating and deletion of buckets easy, we've a script to help you with that, see here (the quickstart has instructions on how to use it).</p>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#dns","title":"DNS","text":"<p>These following snippets can be used to setup/remove all DNS records required for Welkin using Exoscale's CLI.</p> <p>Start by setting up some variables:</p> <pre><code>DOMAIN=\"example.com\"\nIP=\"203.0.113.123\" # IP to LB/ingress endpoint for the Management Cluster\nCK8S_ENVIRONMENT_NAME=\"my-cluster-name\"\n\nSUBDOMAINS=(  \"*.ops.${CK8S_ENVIRONMENT_NAME}\"\n              \"grafana.${CK8S_ENVIRONMENT_NAME}\"\n              \"harbor.${CK8S_ENVIRONMENT_NAME}\"\n              \"kibana.${CK8S_ENVIRONMENT_NAME}\"\n              \"dex.${CK8S_ENVIRONMENT_NAME}\"\n              \"notary.harbor.${CK8S_ENVIRONMENT_NAME}\" )\n</code></pre> <pre><code># Adding the A records\nfor SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do\n  exo dns add A \"${DOMAIN}\" -a \"${IP}\" -n \"${SUBDOMAIN}\"\ndone\n</code></pre> <pre><code># Removing the records\nfor SUBDOMAIN in \"${SUBDOMAINS[@]}\"; do\n  exo dns remove \"${DOMAIN}\" \"${SUBDOMAIN}\"\ndone\n</code></pre>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"contributor-guide/#reusing-clusters","title":"Reusing Clusters","text":"<p>If you for some reason need to reinstall Welkin from scratch, we have some scripts that removes all objects created by this repository. The scripts can be found here (clean-sc.sh and clean-wc.sh).</p>","tags":["ISO 27001 Annex A 8.25 Secure Development Life Cycle"]},{"location":"operator-manual/","title":"Platform Administrator Manual Overview","text":"<p>Welkin abstracts away a lot of complexity from Application Developers. Unfortunately, someone somewhere needs to take care of that complexity. That person is you, the Platform Administrator.</p> <p>Setting up Welkin is not for the faint hearted. We recommend you to proceed in 4 steps as shown below:</p>        Phase 1: Try out the end result               Validate that the application team's needs will be satisfied.         Start with Welkin managed by an experienced platform team on a trusted EU cloud infrastructure.      Go to Managed Services        Phase 2: Infrastructure audit             Clarify the capabilities of your infrastructure and ways of working with your infrastructure team.      Go to Provider Audit        Phase 3: Welkin setup             Now that the infrastructure is in place, you're ready to configure and install Welkin.      Setup Welkin        Phase 4: Deepen your knowledge and operate             Practice maintenance, capacity management and troubleshooting.      Learn how to operate a Kubernetes platform"},{"location":"operator-manual/access-control/","title":"Access control","text":"","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#group-claims","title":"Group claims","text":"<p>This guide describes how to set up and make use of group claims for applications.</p> <p>Note</p> <p>This guide assumes your group claim name is <code>groups</code>.</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#kubernetes","title":"Kubernetes","text":"<p>To set up kubelogin to fetch and use groups make sure that your kubeconfig looks something like this.</p> <pre><code>users:\n  - name: user@my-cluster\n    user:\n      exec:\n        apiVersion: client.authentication.k8s.io/v1beta1\n        args:\n          - oidc-login\n          - get-token\n          - --oidc-issuer-url=https://dex.my-cluster-domain.com\n          - --oidc-client-id=my-client-id\n          - --oidc-client-secret=my-client-secret\n          - --oidc-extra-scope=email,groups # Make sure groups are here\n        command: kubectl\n</code></pre> <p>Tips</p> <p>Your token can be found in <code>~/.kube/cache/oidc-login/</code>. This is useful if you're trying to debug your claims since you can just paste the token to jwt.io and check it.</p> <p>Example:</p> <pre><code>$ ls ~/.kube/cache/oidc-login/\n\n$ kubectl get pod\n&lt;log in&gt;\n\n$ ls ~/.kube/cache/oidc-login/\n13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4\n\n$ cat ~/.kube/cache/oidc-login/13b165965d8e80749ce3b8d442da3e4e9f5ff5e38900ef104eee99fde85a39d4 | jq -r .id_token\neyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJodHRwczovL2RleC5teS1jbHVzdGVyLWRvbWFpbi5jb20iLCJpYXQiOjE2MjE1MTUxNzcsImV4cCI6MTY1MzEzNzU3NywiYXVkIjoibXktY2xpZW50LWlkIiwic3ViIjoiSGlVUE92S1BKMmVwWUkwR1R1U0JYWGRxYTJTV2ZxRnc1ZjBXNVBQeThTWSIsIm5vdW5jZSI6IkNoVXhNRFk0TVRZNE1qRXpORFUzTURVM01ERXlNREFTQm1kdmIyZHNaUSIsImF0X2hhc2giOiI1aUZjbF9Sc1JvblhHekZaMU0xQ2JnIiwiZW1haWwiOiJ1c2VyQG15LWRvbWFpbi5jb20iLCJlbWFpbF92ZXJpZmllZCI6InRydWUiLCJncm91cHMiOlsibXktZ3JvdXAtb25lIiwibXktZ3JvdXAtdHdvIl19.s65Aowfn6B1PiyQvRGPRu9KgX7G39nkLtx6yCAEElao\n</code></pre> <p>Copy the token to jwt.io and ensure that the payload includes the expected groups claim.</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#opensearch","title":"OpenSearch","text":"<p>To enable OpenSearch to use the groups for OpenSearch Dashboards access.</p> <pre><code>opensearch:\n  sso:\n    scope: \"... groups\" # Add groups to existing\n  extraRoleMappings:\n    - mapping_name: kibana_user\n      definition:\n        backend_roles:\n          - my-group-name\n    - mapping_name: kubernetes_log_reader\n      definition:\n        backend_roles:\n          - my-group-name\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#harbor","title":"Harbor","text":"<p>Set correct group claim name since the default scopes includes groups already. This groups can be assigned to projects or as admin group.</p> <pre><code>harbor:\n  oidc:\n    groupClaimName: groups\n</code></pre> <p>Note</p> <p>When OIDC (e.g. DeX) is enabled we cannot create static users using the Harbor web interface. But when anyone logs in via DeX they automatically get a user and we can promote that user to admin. Once there is one admin, they can set specific permissions for other users (there should be at least a few users promoted to admins).</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#grafana","title":"Grafana","text":"","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#ops-grafana","title":"OPS Grafana","text":"<pre><code>grafana:\n  ops:\n    oidc:\n      enabled: true\n      userGroups:\n        grafanaAdmin: my-admin-group\n        grafanaEditor: my-editor-group\n        grafanaViewer: my-viewer-group\n      scopes: \".... groups\" # Add groups to existing\n      allowedDomains:\n        - my-domain.com\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#user-grafana","title":"User Grafana","text":"<pre><code>grafana:\n  user:\n    oidc:\n      userGroups:\n        grafanaAdmin: my-admin-group\n        grafanaEditor: my-editor-group\n        grafanaViewer: my-viewer-group\n      scopes: \"... groups\" # Add groups to existing\n      allowedDomains:\n        - my-domain.com\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#users-onboarding","title":"Users onboarding","text":"<p>This describes how to configure Welkin with the Application Developers who should be OpenSearch, Grafana or Harbor Administrators.</p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#opensearch_1","title":"OpenSearch","text":"<p>This is configured via <code>sc-config.yaml</code></p> <pre><code>opensearch:\n  extraRoleMappings:\n    # Application developer access\n    - mapping_name: kibana_user\n      definition:\n        users:\n          - user@domain.tld\n    # Extra permissions for Application developer\n    - mapping_name: kubernetes_log_reader\n      definition:\n        users:\n          - user@domain.tld\n    - mapping_name: alerting_ack_alerts\n      definition:\n        users:\n          - user@domain.tld\n    - mapping_name: alerting_read_access\n      definition:\n        users:\n          - user@domain.tld\n    - mapping_name: alerting_full_access\n      definition:\n        users:\n          - user@domain.tld\n    # Administrator access\n    - mapping_name: all_access\n      definition:\n        users:\n          - user@domain.tld\n        backend_roles:\n          - group@domain.tld\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#grafana_1","title":"Grafana","text":"<ol> <li> <p>Application Developer logs in to Grafana via OpenID</p> </li> <li> <p>Administrator logs in to Grafana via static admin user.</p> <p>Note</p> <p>To get the static admin username and password you need to have access to the SC cluster and then run</p> <p><code>kubectl get secret user-grafana-env -n monitoring -o json | jq '.data | map_values(@base64d)'</code></p> </li> <li> <p>Administrator promotes the OpenID user to Grafana admin at <code>grafana.domain.tld/admin/users</code></p> </li> </ol>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#harbor_1","title":"Harbor","text":"<ol> <li> <p>Application Developer logs in to Harbor via OpenID</p> </li> <li> <p>Administrator logs in to Harbor via static admin user.</p> <p>Note</p> <p>To get the static admin username and password you need to have access to the SC cluster and then run</p> <p><code>kubectl get secret harbor-init-secret -n harbor -o json | jq '.data.\"harbor-password\" | @base64d'</code></p> <p>Username is: admin</p> </li> <li> <p>Administrator promotes the OpenID user to Harbor admin at <code>harbor.domain.tld/harbor/users</code></p> </li> </ol>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#opensearch-index-per-namespace","title":"OpenSearch Index Per Namespace","text":"<p>This section defines how to enable indices for Kubernetes namespaces.</p> <p>To enable logging and control indices per namespace in OpenSearch, this is configured in <code>common-config.yaml</code> as follows:</p> <pre><code>opensearch:\n  indexPerNamespace: true\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#defining-access-control-per-index","title":"Defining Access Control Per Index","text":"<p>This section describes how to define access control for specific indices in OpenSearch.</p> <p>This is configured via <code>sc-config.yaml</code></p> <pre><code>opensearch:\n  extraRoles:\n    - role_name: namespace1_reader\n      definition:\n        index_permissions:\n          - index_patterns:\n              - \"namespace1-*\"\n            allowed_actions:\n              - read\n    - role_name: namespace2_reader\n      definition:\n        index_permissions:\n          - index_patterns:\n              - \"namespace2-*\"\n            allowed_actions:\n              - read\n  extraRoleMappings:\n    - mapping_name: kibana_user # needed to be able to view index patterns\n      definition:\n        users:\n          - user1@domain.tld\n          - user2@domain.tld\n    - mapping_name: namespace1_reader\n      definition:\n        users:\n          - user1@domain.tld\n        backend_roles: []\n    - mapping_name: namespace2_reader\n      definition:\n        users:\n          - user2@domain.tld\n        backend_roles: []\n# we also need to configure retention period in curator to delete the indices\n  curator:\n    retention:\n      - pattern: ^[^.].*\n        sizeGB: 5000\n        ageDays: 30\n</code></pre> <p>Do note that the configured users needs to create the index patterns manually in OpenSearch Dashboards under <code>Dashboards Management</code> -&gt; <code>Index patterns</code> -&gt; <code>Create index pattern</code>:</p> <p></p>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/access-control/#opensearch-document-level-security","title":"OpenSearch Document-level security","text":"<p>This section describes how to configure Document-level security in order to restrict access to a subset of documents within an index. This makes it possible to limit access to certain logs of an application based on the contents of fields.</p> <p>The following example describes a role that will not be able to display documents where the field <code>level=audit</code> or where <code>message</code> contains <code>level=audit</code></p> <pre><code>opensearch:\n  extraRoles:\n   - role_name: kubernetes-without-audit\n      definition:\n        index_permissions:\n          - index_patterns:\n              - \"kubernetes*\"\n            allowed_actions:\n              - read\n            dls: |-\n              {\n                \"bool\": {\n                  \"must_not\": [\n                    {\n                      \"match_phrase\": {\n                        \"message\": \"level=audit\"\n                      }\n                    },\n                    {\n                      \"match_phrase\": {\n                        \"level\": \"audit\"\n                      }\n                    }\n                  ]\n                }\n              }\n  extraRoleMappings:\n    - mapping_name: kubernetes-without-audit\n      definition:\n        users:\n          - less-privileged-user@domain.tld\n    - mapping_name: kibana_user # needed to be able to view index patterns\n      definition:\n        users:\n          - less-privileged-user@domain.tld\n</code></pre>","tags":["HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"]},{"location":"operator-manual/additional-services/","title":"Additional Managed Services","text":"<p>Welkin recommends the following Operators or Helm Charts:</p> <ul> <li>Argo CD Helm Chart</li> <li>Jaeger operator</li> <li>Zalando operator for PostgreSQL</li> <li>RabbitMQ Cluster Operator for Kubernetes</li> <li>Spotahome operator for Redis</li> </ul>"},{"location":"operator-manual/air-gapped/","title":"Welkin in Air-gapped Network","text":"<p>Warning</p> <p>Air-gapped networks differ widely in how strictly they are isolated. Before proceeding, see Types of air-gapped networks to understand which level applies to your setup.</p> <p>Contact Elastisys Support if you need help.</p> <p>For Welkin Enterprise Customers</p> <p>Please start by running these commands.</p> <p>If you are struggling, don't hesitate to file a ticket.</p> <p>You can run the following command from the compliantkubernetes-apps repository to collect diagnostic information that will help us support you. Ensure that you have put fingerprints received from Elastisys in a file named <code>${CK8S_CONFIG_PATH}/diagnostics_receiver.gpg</code>.</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt;\n</code></pre> <p>Show more examples on using the diagnostics command The command <code>ck8s diagnostics</code> can be provided with different flags to gather additional information from your environment, to see all available options run: <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --help\n</code></pre> <p>Some example use cases:</p> <ul> <li> <p>To include config files found in <code>CK8S_CONFIG_PATH</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --include-config\n</code></pre> </li> <li> <p>To retrieve more information such as YAML manifests for resources in a specific namespace, in this example <code>ingress-nginx</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> <li> <p>It is also possible to set which GPG keys should be used by setting <code>CK8S_PGP_FP</code>:</p> <pre><code>export CK8S_PGP_FP=&lt;gpg-fingerprint1&gt;,&lt;gpg-fingerprint2&gt;\n./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> </ul> <p>Please also provide us with your terminal in a text format. We need to look both at the commands you typed and their output.</p>"},{"location":"operator-manual/air-gapped/#background","title":"Background","text":"<p>In an air-gapped network, machines are isolated from insecure networks such as the public Internet. Air-gapping is used for networks that handle highly confidential data such as military or governmental systems, or in life-critical systems, for example, in nuclear power plants or for medical equipment. This document provides guidelines on how to configure Welkin Apps to work inside an air-gapped network.</p>"},{"location":"operator-manual/air-gapped/#types-of-air-gapped-networks","title":"Types of air-gapped networks","text":"<p>The term air-gapped is used with different meanings in different contexts. The table below summarizes common variants, from least to most restrictive.</p> Level Outbound Internet Inbound Internet Description 1. Firewalled \u2705 Allowed through firewalls \u2705 Allowed through firewalls Sometimes marketed as \"air-gapped\", but not truly isolated. 2. Outbound-only \u2705 Allowed \u274c Blocked (except VPN) Typical for secure enterprise setups with limited inbound access. 3. Proxy-mediated outbound \u2699\ufe0f Through HTTP/HTTPS proxy only \u274c Blocked Common practical setup; controlled outbound for updates and telemetry. 4. Toggle-off default \ud83d\udeab Disabled by default, can be temporarily enabled \u274c Blocked Higher-security networks; outbound enabled only on request. 5. Fully air-gapped \ud83d\udeab None \ud83d\udeab None Complete physical or logical isolation; on-premises access only."},{"location":"operator-manual/air-gapped/#maintenance-access-via-vpn","title":"Maintenance access via VPN","text":"<p>Remote maintenance access is often treated as a separate channel from general inbound Internet traffic. Different organizations apply different policies depending on risk tolerance and operational needs. The table below summarizes common variants, from least to most restrictive.</p> Access Policy Description 1. Permanent Always-on VPN access limited to a small set of authorized users from a limited set of IP ranges. 2. Temporary VPN access can be enabled on request for specific maintenance windows. 3. None (on-site only) Administrators must be physically present; no remote access permitted. <p>Note</p> <p>Elastisys Welkin can be adapted to any type of air-gapped network scenario listed above. Unless otherwise noted, this guide is written with the \"proxy-mediated outbound\" type in mind, due to both its prevalence in the industry and to keep this guide readable.</p> <p>Contact Elastisys Support for assistance with other configurations.</p>"},{"location":"operator-manual/air-gapped/#system-context-diagram","title":"System Context Diagram","text":"<p>The following is a generic system context diagram over an air-gapped network consisting of a Welkin Environment. The diagram shows the Platform Administrator Machine that has access to both the Internet and the air-gapped network. Inside the air-gapped network, Welkin Clusters can access other services residing on the same private network such as an object storage for long-term storage, a container registry mirror, DNS and NTP servers, etc.</p> <p></p> <p>Note</p> <p>* For Certificate Provisioning, you may want to allow DNS01 or HTTP01 challenges to a public certificate authority, such as Let's Encrypt. However, if you want the whole network to be truly air-gapped, then you need to figure out an air-gapped solution for PKI.</p>"},{"location":"operator-manual/air-gapped/#configuring-air-gapped-welkin-apps","title":"Configuring Air-gapped Welkin Apps","text":"<p>These guidelines show how to configure Welkin Apps to work in an air-gapped network. For setting up the Kubernetes layer, please refer to the Kubespray air-gap installation documentation or Cluster-API documentation for further instructions.</p> <p>This guide will assume that you have a Platform Administrator Machine that can access both the Internet and the air-gapped network over SSH, this includes SSH access to both the Cluster Nodes and hosts for the different offline services described in the system context diagram above. This \"Platform Administrator Machine\" can be your local machine or a bastion host.</p> <p>Start by initializing your Welkin Apps configuration (see the quickstart section here for more info on initializing and deploying Welkin Apps) with the <code>air-gapped</code> flavor by setting the <code>CK8S_FLAVOR</code> environment variable:</p> <pre><code>export CK8S_FLAVOR=air-gapped\n</code></pre>"},{"location":"operator-manual/air-gapped/#container-images-and-registry-mirroring","title":"Container Images and Registry Mirroring","text":"<p>All container images used for the Kubernetes layer and for Welkin Apps needs to be available in the air-gapped network. This can be done by setting up a private container registry (e.g. <code>Harbor</code>) that can act as a registry mirror or registry cache. But first these images need to be added to the private container registry. The following sections describes ways of getting images used by Welkin Apps.</p>"},{"location":"operator-manual/air-gapped/#migrating-images-from-a-running-environment-with-script","title":"Migrating Images From a Running Environment with Script","text":"<p>There is a useful script in the upstream Kubespray repository that you can use to get all images from existing Welkin Clusters by simply pointing the <code>KUBECONFIG</code> environment variable to the correct Cluster(s). If possible it is recommended to use this script for migrating images to a container registry mirror.</p>"},{"location":"operator-manual/air-gapped/#migrating-images-manually","title":"Migrating Images Manually","text":"<p>If getting images from a running Cluster is not desirable, use the following commands to get most of the images used in Welkin Apps:</p> <pre><code>export KUBE_VERSION=# e.g. 1.27.5\n./bin/ck8s ops helmfile sc template --kube-version=$KUBE_VERSION | yq '..|.image? | select(.)' | sort -u &gt; images-sc.list\n./bin/ck8s ops helmfile wc template --kube-version=$KUBE_VERSION | yq '..|.image? | select(.)' | sort -u &gt; images-wc.list\n</code></pre> <p>Note</p> <p>We use the word most here since some images can only be known after deployment, since some pods are created by operators. These images will need to be managed manually.</p> <p>You can then copy over the images to the host running the container registry mirror, after which the images can be tagged and pushed to. To copy over images, you can store them as tarballs, for example, you can run the following if you have a container runtime installed such as <code>nerdctl</code>, <code>docker</code> or <code>podman</code> (see the note regarding using <code>docker</code> and <code>podman</code>):</p> <pre><code>RUNTIME=nerdctl\nREGISTRY= # set this to the public registry containing the image, e.g. ghcr.io\nIMAGE_NAME= # include repository, image name and tag, e.g. aquasecurity/trivy-operator:0.17.1\nIMAGE_FILE_NAME=image_file.tar\nsudo ${RUNTIME} pull ${REGISTRY}/${IMAGE_NAME}\nsudo ${RUNTIME} save -o ${IMAGE_FILE_NAME}.tar ${REGISTRY}/${IMAGE_NAME}\n</code></pre> <p>Note</p> <p>Be wary of images with digests as tags, as when saving the container image when using the <code>docker</code> or <code>podman</code> runtime and then loading it, the image will lose its digest (see issue discussing this here). If you use this approach, you may need to override the digest or image tag used in the Welkin Apps configuration. Using a tool like <code>nerdctl</code> will however preserve the correct image digest.</p> <p>Tip</p> <p>If your Platform Administrator Machine has access to the air-gapped container registry mirror, you can use a tool like <code>regctl</code> or <code>regsync</code> which can help syncing images between repositories. This way, images will keep their digest and you do not have to save the images locally and move them to the air-gapped registry before pushing them.</p> <p>Then copy the image tarballs to the container registry mirror host using <code>scp</code>. SSH to the host and load, tag and push the images:</p> <pre><code>PRIVATE_REGISTRY= # set this to an air-gapped registry, e.g. registry.air-gapped.internal:5000\nsudo ${RUNTIME} load -i ${IMAGE_FILE_NAME}\nsudo ${RUNTIME} tag ${REGISTRY}/${IMAGE_NAME} ${PRIVATE_REGISTRY}/${IMAGE_NAME}\nsudo ${RUNTIME} push ${PRIVATE_REGISTRY}/${IMAGE_NAME}\n</code></pre> <p>Configure the container registry mirror for the container runtime. If you are using Kubespray and <code>containerd</code> you can configure mirrors as follows:</p> <pre><code>registry_addr: \"registry.air-gapped.internal\"\nregistry_host: \"https://registry.air-gapped.internal\" # change to http:// if not available over HTTPS\nregistry_insecure: false # set to true if HTTP\n\ncontainerd_registries_mirrors:\n  - prefix: \"{{ registry_addr }}\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n  - prefix: \"docker.io\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n  - prefix: \"gcr.io\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n  - prefix: \"ghcr.io\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n  - prefix: \"quay.io\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n  - prefix: \"registry.k8s.io\"\n    mirrors:\n      - host: \"{{ registry_host }}\"\n        capabilities: [\"pull\", \"resolve\"]\n        skip_verify: {{ registry_verify }}\n</code></pre> <p>Warning</p> <p>If using e.g. Harbor as private container registry mirror, you may run into issues with non-namespaced images (e.g. <code>registry.k8s.io/pause</code>) as this is currently not supported with Harbor. In Kubespray it is possible to override some images, such as the <code>pause</code> image by setting <code>pod_infra_image_repo: \"{{ registry_addr }}/library/pause\"</code> and pushing the <code>pause</code> image to the <code>library</code> namespace/project in the Harbor registry.</p> <p>Note</p> <p>Depending on your setup, it might not be possible to run <code>ck8s update-ips</code> unless you configure the Platform Administrator Machine to be able to resolve domains configured in the air-gapped DNS server, or use some sort of proxy. Otherwise, configure NetworkPolicies for apps manually in the <code>*-config.yaml</code> files.</p> <p>Some of the apps needs special configurations to work properly in an air-gapped network, the following sections will describe how to configure these more closely. Some of these values will already be set to <code>set-me</code> if you used the <code>CK8S_FLAVOR=air-gapped</code> which you need to configure before deploying the apps, however, there are some additional configurations you can make which you might want or need to configure depending on your setup.</p>"},{"location":"operator-manual/air-gapped/#node-local-dns","title":"Node-local-dns","text":"<p>If you have configured a DNS server inside the air-gapped network that you want to use for resolving domains used for services running inside the Clusters (e.g. <code>grafana.air-gapped.internal</code>), you can configure it as an additional forwarder for <code>node-local-dns</code> in <code>common-config.yaml</code>:</p> <pre><code>nodeLocalDns:\n  customConfig: |-\n    example.local:53 {  # &lt;- set the zone name of your air-gapped network here\n        log\n        errors {\n          consolidate 5m \".* i/o timeout$\" warning\n        }\n        forward . 10.65.131.137 # &lt;- change this to match your air-gapped network\n        bind 169.254.20.10 10.233.0.3\n        loadbalance\n        cache 5\n        reload\n        }\n</code></pre>"},{"location":"operator-manual/air-gapped/#falco","title":"Falco","text":"<p>Falco is configured by default to retrieve and update plugins, e.g. Falco rules, from the Internet. In an air-gapped network you can disable the Falco artifact installer to avoid this, and keep the Falco rules that are packaged with the Falco container image. To disable Falco artifact installer, configure the following in <code>common-config.yaml</code>:</p> <pre><code>falco:\n  artifact:\n    install:\n      enabled: false\n</code></pre> <p>It is also possible to host Falco artifact indexes yourselves and configure Falco to point to the URL of these (see here for an overview on falcoctl indexes):</p> <pre><code>falco:\n  artifact:\n    install:\n      enabled: true\n  customIndexes:\n    - fileserver.air-gapped.internal:8080/falcoctl/index.yaml\n</code></pre> <p>The default Falco driver in Welkin is <code>module</code>. With this driver, Falco will attempt to download the driver from the internet, and if it fails to do so, it will build the module as a fallback, which would be the case in an air-gapped network. Unless you host driver modules yourself and configure Falco to use this file server instead:</p> <pre><code>falco:\n  driver:\n    kind: module\n    module:\n      repoURL: \"fileserver.air-gapped.internal:8080\"\n</code></pre> <p>Important</p> <p>With this setup it is your responsibility to ensure that any Falco modules, rules and/or plugins are kept up to date.</p> <p>If Nodes in the Clusters have a kernel version &gt;=5.8, you can use <code>modern-bpf</code> instead to avoid Falco having to download drivers from the Internet, as everything is already embedded into Falco (as long as this is supported, see requirements here:</p> <pre><code>falco:\n  driver:\n    kind: modern-bpf\n</code></pre>"},{"location":"operator-manual/air-gapped/#opensearch","title":"OpenSearch","text":"<p>Welkin is configured to store OpenSearch data in Object storage which is configured with plugins. Normally, OpenSearch attempts to download plugins from the Internet. In an air-gapped network, you can download plugins for OpenSearch manually and host them on a file server inside the air-gapped network. To download the S3 plugin run:</p> <pre><code>OPENSEARCH_VERSION=2.8.0  # set the opensearch version of the cluster\nwget \"https://artifacts.opensearch.org/releases/plugins/repository-s3/${OPENSEARCH_VERSION}/repository-s3-${OPENSEARCH_VERSION}.zip\"\n</code></pre> <p>To configure OpenSearch to download the S3 plugin from a file server instead of downloading from the Internet, set the following in <code>sc-config.yaml</code>:</p> <pre><code>opensearch:\n  plugins:\n    installExternalObjectStoragePlugin: false\n    additionalPlugins:\n      - fileserver.air-gapped.internal:8080/repository-s3-${OPENSEARCH_VERSION}.zip\n</code></pre>"},{"location":"operator-manual/air-gapped/#trivy","title":"Trivy","text":"<p>Trivy checks for vulnerabilities from a vulnerability database as well as a Java index database which are usually downloaded directly from GitHub (<code>ghcr.io</code>). In an air-gapped network, you can download these databases manually and push them to a private registry inside the air-gapped network. Please read the air-gapped documentation for Trivy on how to download and copy over this database. The tool <code>oras</code> can be useful for working with and managing OCI repositories.</p> <p>Once the databases are available in the air-gapped private-registry, you need to configure the following variables in <code>common-config.yaml</code> for Trivy to download from the private registry:</p> <pre><code>trivy:\n  scanner:\n    offlineScanEnabled: true\n    dbRegistry: registry.air-gapped.internal\n    dbRepository: aquasecurity/trivy-db\n    dbRepositoryInsecure: false # set to true if the private registry is not configured with HTTPS\n    javaDbRegistry: registry.air-gapped.internal\n    javaDbRepository: aquasecurity/trivy-java-db\n\n    # add registries that should be mirrored to private registry\n    registry:\n      mirror:\n        \"docker.io\": registry.air-gapped.internal\n        \"gcr.io\": registry.air-gapped.internal\n        \"ghcr.io\": registry.air-gapped.internal\n        \"index.docker.io\": registry.air-gapped.internal\n        \"quay.io\": registry.air-gapped.internal\n        \"registry.k8s.io\": registry.air-gapped.internal\n\n    # if authorization is required to the private registry, create a pull\n    # secret in the monitoring namespace and configure the secret name\n    imagePullSecret:\n      name: \"pull-secret\"\n</code></pre> <p>Important</p> <p>With this setup it is your responsibility to ensure that vulnerability databases are kept up to date.</p>"},{"location":"operator-manual/air-gapped/#networkpolicies","title":"NetworkPolicies","text":"<p>A note about configuring NetworkPolicies, when using self-hosted file servers for the different services, remember to configure NetworkPolicies accordingly, i.e. if a self-hosted file server runs on a different port than 443, you will need to override it in the configuration for each respective service, e.g. for OpenSearch to get plugins from <code>fileserver.air-gapped.internal:8080</code>:</p> <pre><code>networkPolicies:\n  opensearch:\n    plugins:\n      ips:\n        - 10.65.131.137/32 # set this to the IP of fileserver.air-gapped.internal\n      ports:\n        - 8080 # the file-server is hosted on port 8080\n</code></pre>"},{"location":"operator-manual/air-gapped/#custom-alerting-receiver","title":"Custom alerting receiver","text":"<p>It is possible to configure your own alerting receivers for Alertmanager, which you can use to configure alerting to a on-call service that is available in your air-gapped network. The following shows a simplified example on how to configure a custom email receiver in <code>sc-config.yaml</code>:</p> <pre><code>alerts:\n  alertsTo: email\n  customReceivers:\n    - name: \"email\"\n      email_configs:\n        - to: \"admin@example.com\"\n          from: \"prometheus@example.com\"\n          require_tls: false\n          send_resolved: true\n</code></pre>"},{"location":"operator-manual/air-gapped/#demo","title":"Demo","text":"<p>Following are some screenshots of an air-gapped Welkin Environment, accessing Service Endpoints on the local air-gapped domain by using a SOCKS proxy in Firefox over an SSH tunnel. Accessing Grafana Dashboards:</p> <p></p> <p>Signing in with Dex will redirect to the Dex Service Endpoint for authentication (in this case a static user is used):</p> <p></p> <p>Checking the endpoint of the user demo application:</p> <p></p> <p>Seeing HTTP metrics for the <code>/users</code> endpoint of the user demo in Grafana Dashboards:</p> <p></p> <p>Seeing logs from the user demo Pods in OpenSearch Dashboards:</p> <p></p>"},{"location":"operator-manual/air-gapped/#catalog-of-requirements-for-an-air-gapped-environment-hosting-welkin","title":"Catalog of Requirements for an Air-gapped Environment Hosting Welkin","text":"<p>Note</p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p> <ol> <li>NTP system:<ol> <li>The NTP system MUST provide accurate time without relying on external time-sources.</li> <li>The NTP system SHOULD fulfill requirements in BSI IT-Grundschutz OPS.1.2.6 NTP Time Synchronisation. Requirements which are not fulfilled MUST have a clear justification for exclusion or a list of compensatory security measures.</li> </ol> </li> <li>DNS system:<ol> <li>The DNS system MUST be air-gapped and MUST NOT use external DNS servers, neither advertising nor resolving.</li> <li>The DNS system SHOULD fulfill requirements in BSI IT-Grundschutz APP.3.6 DNS Servers. Requirements which are not fulfilled MUST have a clear justification for exclusion or a list of compensatory security measures.</li> <li>The DNS system MUST resolve all domain in <code>.internal</code>.</li> <li>The DNS system MAY resolve <code>in-addr.arpa</code> (see RFC 1035) and <code>ip6.arpa</code> (see RFC 3152).</li> <li>The DNS system MUST NOT recurs and MUST respond to all other requests with SERVFAIL.</li> </ol> </li> <li>Certificate provisioning: You have several options:<ul> <li>Option A: Run your own Certificate Authority which issues intermediate Certificate Authority (CA) certificates. Configure Welkin environments with an intermediate CA. Welkin will issue server certificates for Ingress resources as documented on this page. See CA in cert-manager Documentation.</li> <li>Option B: Run your own Certificate Authority which issues server certificates. Install each server certificate in Welkin via Secrets of type <code>kubernetes.io/tls</code>.</li> <li>Option C: Run your Automatic Certificate Management Environment (ACME). Configure Welkin to use your ACME server instead of LetsEncrypt.</li> </ul> </li> <li>Identity Provider (IdP):<ol> <li>The IdP MUST be compatible with OpenID.</li> <li>The IdP MUST NOT depend on resources outside the air-gapped environment.</li> </ol> </li> <li>Object storage:<ol> <li>The object storage MUST be compatible with S3.</li> <li>The object storage SHOULD support object locking.</li> <li>The object storage MUST NOT depend on resources outside the air-gapped environment.</li> </ol> </li> <li>Registry mirror:<ol> <li>The registry mirror MUST be compatible with OCI.</li> <li>The object storage MUST NOT depend on resources outside the air-gapped environment.</li> </ol> </li> <li>File server<ol> <li>The file server MUST be able to expose files via HTTP.</li> <li>The file server MUST allows upload via SSH, SCP, rsync or SFTP.</li> </ol> </li> <li>On-call management tool:<ol> <li>The on-call management tool MUST be compatible with a Prometheus Alertmanager client.<ul> <li>Note: Currently, Welkin only supports OpsGenie, Slack and Teams. Please contact Elastisys if you need support for other Alertmanager clients.</li> </ul> </li> </ol> </li> </ol>"},{"location":"operator-manual/air-gapped/#implications","title":"Implications","text":"<p>Note</p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p> <p>To successfully run in an air-gapped environment, Welkin and the underlying operating system MUST be configured as follows:</p> <ul> <li>All servers in the air-gapped environment MUST be configured to:<ul> <li>use the NTP system above (see systemd-timesyncd);</li> <li>use the DNS system above (see systemd-resolved);</li> <li>trust your Certificate Authority (see update-ca-certificates);</li> <li>use the file server for OS repositories;</li> <li>use the registry mirror for container images (see description above).</li> </ul> </li> <li>Welkin MUST be configured to:<ul> <li>use the object storage above;</li> <li>use the identity provider above.</li> </ul> </li> <li>Containers MUST trust your Certificate Authority. This can be accomplished as follows:<ul> <li>Note: Currently, Welkin only supports Option A. Please contact Elastisys, if you prefer Option B or C.</li> <li>Option A: Build containers so as to trust your Certificate Authority (see update-ca-certificates).<ul> <li>This solution solves the trust problem at build-time. Some people see this as the most robust, as it can be more robustly tested and rolled out.</li> </ul> </li> <li>Option B: Inject the certificate of your Certificate Authority with trust-manager.</li> <li>Option C: Inject the certificate of your Certificate Authority with Kyverno.</li> </ul> </li> </ul>"},{"location":"operator-manual/air-gapped/#references","title":"References","text":"<ul> <li>Air gap (networking) on Wikipedia</li> <li>Kubespray offline environment documentation</li> </ul>"},{"location":"operator-manual/break-glass/","title":"Break-glass","text":"<p>In this section we describe a workaround when access to the environment is broken for the Platform Administrators/operators and/or users.</p>","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/break-glass/#platform-administrator-access","title":"Platform Administrator Access","text":"<p>When Dex or the OpenID provider is malfunctioning, the Platform Administrator might be unable to access the Cluster. The following steps will give you temporary access sufficient for troubleshooting and recovery:</p> <ol> <li> <p><code>SSH</code> to one of the control-plane Nodes.</p> </li> <li> <p>Use <code>/etc/kubernetes/admin.conf</code> and run <code>kubectl</code> commands to check the problem</p> <pre><code>export KUBECONFIG=/etc/kubernetes/admin.conf\n#run kubectl command\nsudo kubectl get po -A\n</code></pre> </li> </ol>","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/break-glass/#kubernetes-user-access","title":"Kubernetes User Access","text":"<p>Note</p> <p>This is a temporary solution and access should be disabled once the issue with Dex is resolved.</p> <p>If Dex is broken, you can manually create a <code>kubeconfig</code> file for a user. While there are different ways to create <code>kubeconfig</code> files, we will use the X.509 client certificates with OpenSSL. Follow the steps below to create a user <code>kubeconfig</code> file.</p> <ol> <li> <p>Create a private key:</p> <pre><code>openssl genrsa -out user1.key 2048\n</code></pre> </li> <li> <p>Create a certificate signing request (CSR). <code>CN</code> is the username and <code>O</code> the group.</p> <pre><code>openssl req -new -key user1.key \\\n-out user1.csr \\\n-subj \"/CN=user1/O=companyname\"\n</code></pre> </li> <li> <p>Get the Base64 encoding for the generated CSR file.</p> <pre><code>cat user1.csr | base64 | tr -d '\\n'\n</code></pre> </li> <li> <p>Create a Certificate Signing Request with Kubernetes</p> <pre><code>cat &lt;&lt;EOF | kubectl  apply -f -\napiVersion: certificates.k8s.io/v1\nkind: CertificateSigningRequest\nmetadata:\n    name: user1\nspec:\n    groups:\n    - system:authenticated\n    request: # put here the Base64 encoded text for the CRS that you get in step 3\n    signerName: kubernetes.io/kube-apiserver-client\n    usages:\n    - client auth\nEOF\n</code></pre> </li> <li> <p>Approve the CSR</p> <pre><code>kubectl certificate approve user1\n</code></pre> </li> <li> <p>Get the certificate.     Retrieve the certificate from the CSR:</p> <pre><code>kubectl get csr/user1 -o yaml\n</code></pre> <p>The certificate value is in Base64-encoded format under <code>status.certificate</code>. Put the content under <code>client-certificate-data:</code>. And also get the Base64 encoded content for the private key and put it under <code>client-key-data:</code>. To get the Base64 encoded content <code>cat user1.key | base64 | tr -d '\\n'</code>.</p> <p>The kubeconfig file for <code>user1</code> user looks like:</p> <pre><code>apiVersion: v1\nclusters:\n- cluster:\n    certificate-authority-data: &lt;CA&gt;\n    server: https://control-node-ip:6443 # ip address of one of the control nodes\n  name: &lt;cluster-name&gt;\ncontexts:\n- context:\n    cluster: &lt;cluster-name&gt;\n    user: user1 # &lt;USER&gt;\n  name: &lt;USER&gt;@&lt;CLUSTER-NAME&gt;\nkind: Config\nusers:\n- name: user1\n  user:\n    client-certificate-data: &lt;CLIENT-CRT-DATA&gt;\n    client-key-data: &lt;CLIENT-KEY-DATA&gt;\n</code></pre> </li> <li> <p>Add the user and namespaces that s/he has access to in wc-config.yaml file.</p> <pre><code>user:\n  # This only controls if the namespaces should be created, user RBAC is always created.\n  createNamespaces: true\n  namespaces:\n    - namespace1 # namespaces that the user is allowed to access\n  adminUsers:\n    - user1 # the user\n</code></pre> </li> </ol>","tags":["HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)"]},{"location":"operator-manual/capacity-management/","title":"Capacity Management","text":"<p>Our users trust us -- the Welkin administrators -- to keep applications up and secure. Keeping the application up means that there is sufficient capacity in the environment, both for headroom -- in case the application suddenly gets popular -- and resilience to Node or Zone failure. Keeping the application secure means having sufficient capacity in the environment to allow rolling Node restarts -- as required for keeping the base OS up-to-date and secure -- without causing downtime.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#types-of-failure","title":"Types of Failure","text":"<p>Welkin environments are set up to withstand either:</p> <ul> <li>a single Node failure per Node Group (Node resilient); or</li> <li>a single Zone failure (Zone resilient).</li> </ul> <p>Node resilient environments are set up with enough resources allocated to withstand a single Node failure on a per Node Group basis (e.g., control-plane, worker). In other words, the environment should be able to withstand a control-plane Node and worker Node failing simultaneously.</p> <p>Zone resilient environments are set up over three Zones. All components that have HA capabilities will be spread across all three Zones, such that the environment can withstand a complete Zone failure (e.g., Kubernetes Control Plane, Ceph Mon, OpenSearch etc.).</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#defining-node-groups","title":"Defining Node Groups","text":"<p>Node Groups are meant to represent a logical grouping of Nodes, for example the worker Nodes in a Cluster. In practice, these Node Groups are defined by labeling all Nodes with the name of the Node Group that they belong to:</p> <pre><code>kubectl label node &lt;node-name&gt; elastisys.io/node-group=&lt;node-group&gt;\n</code></pre> <p>These labels are required for the monitoring and alerting described on this page to function.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#upscaling","title":"Upscaling","text":"<p>Welkin uses a combination of alerts for both individual Nodes as well as Node Groups, which are also monitored and visualized in a Grafana dashboard.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#when","title":"When?","text":"<p>Welkin triggers a P1 alert when:</p> <ul> <li>The average CPU usage for a Node or Node Group, over one hour, is above 95%.</li> <li> <p>The average memory usage for a Node or Node Group, over one hour, is above 85%.</p> </li> <li> <p>Why a P1 alert? P1 alerts are events that need immediate attention, which makes them suitable for scenarios with a higher usage threshold over a shorter time-span. If the alert is triggered for a single Node, the administrator can attempt to redistribute the workload more evenly across the Node Group. If the alert is triggered for a Node Group, that Node Group needs to be scaled up.</p> </li> </ul> <p>Welkin triggers a P2 alert when:</p> <ul> <li> <p>The average CPU or memory usage for a Node Group, over 24 hours, is above 75%.</p> </li> <li> <p>Why a P2 alert? P2 alerts are events that need to be dealt with within a business day. This makes them suitable for scenarios with a lower usage threshold over a longer time-span, giving administrators enough time to take action. Excess capacity is cheaper than frustrated administrators. There is no need to disturb anyone's sleep.</p> </li> </ul>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#metrics","title":"Metrics","text":"<ul> <li>Why memory and CPU usage?<ul> <li>These are the resource metrics that are directly tied to the Nodes, and represent how much of the resource is actually used and how much is available. If usage gets close to 100% of capacity, it will start impacting applications.</li> <li>That isn't to say that these are the only capacity metrics to take into account. Other metrics are useful too, but are often not cause for an immediate scale-up and instead require further investigation. These other metrics include:<ul> <li>CPU:<ul> <li>sum of (Kubernetes) CPU requests to CPU allocatable;</li> <li>load average;</li> </ul> </li> <li>Memory:<ul> <li>sum of (Kubernetes) memory request to memory allocatable;</li> </ul> </li> <li>Storage:<ul> <li>host disk used to size;</li> <li>PersistentVolumeClaim used to size;</li> <li>Rook/Ceph OSD used to size;</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Note</p> <p>Welkin can be configured to require resource requests for all Pods.</p> <p>Important</p> <p>Nodes dedicated for data services, such as PostgreSQL, are excluded from Kubernetes requests to allocatable calculation.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#how","title":"How?","text":"<p>Add a new Node of the same type as the other Nodes in the Cluster.</p> <p>If the Cluster has 6 Nodes, consider consolidating to 3 Nodes of twice-the-size -- in number of CPUs or memory or both. This may have several benefits:</p> <ul> <li>it may reduces infrastructure cost;</li> <li>it reduces platform administrator burden;</li> <li>it reduces the risk of hitting the maximum VMs per anti-affinity group limit; for some infrastructure providers, this may be as low as 5.</li> </ul> <p>Before doing this, get in touch with Application Developers to ensure they don't have Kubernetes scheduling constraints that would cause issues on the consolidated environment.</p> <p>If you are about to double the number of Nodes, get in touch with Application Developers to ensure their application is not misbehaving, before upscaling.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#downscaling","title":"Downscaling","text":"<p>We hope that the applications we host will only grow in popularity and that downscaling is never needed. Nevertheless, Application Developer trust us to keep infrastructure costs down, if their application hasn't gone viral -- yet!</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#when_1","title":"When?","text":"<p>The capacity of the environment should be regularly reviewed, for example, after a maintenance window.</p> <p>Important</p> <p>Downscaling may put application uptime at risk. Therefore, be conservative when downscaling.</p> <p>Before downscaling you should:</p> <ul> <li>Evaluate the capacity trends in last 3 to 6 months and take decision based on that. Notice that capacity usage may be smaller during weekends, at the beginning or end of the month, during vacation periods, etc.</li> <li>Ask the user if the reduction in capacity usage is a real trend, and not just sporadic due to quiet periods or vacation periods. E.g., an EdTech app won't be used as intensively during school holidays.</li> <li>Ask the user if they foresee any increase in capacity due to new app releases or new apps additions or something that will require more resources.</li> </ul>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#how_1","title":"How?","text":"<p>If a decision has been made to downscale, make sure to drain and cordon the Node before decommissioning it.</p> <p>If you are about to go below 3 Nodes, consider replacing the Nodes with 6 Nodes of half-the-size before downscaling. Before doing this, get in touch with Application Developers to ensure they don't have Kubernetes scheduling constraints that would cause issues on the consolidated environment.</p> <p>If you are about to half the number of Nodes, get in touch with Application Developers to ensure their application is not misbehaving, before downscaling.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/capacity-management/#optimization","title":"Optimization","text":"<p>Removing capacity is more dangerous than having extra capacity, when it comes to application uptime. Furthermore, we need to avoid oscillations: Removing capacity to only add it back a few days later is no fun for the administrator. Therefore, downscaling should only be performed periodically, whereas upscaling should be performed as soon as predictions show it is needed.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"]},{"location":"operator-manual/clean-up/","title":"Removing Welkin Apps from your Cluster","text":"<p>To remove the applications added by Welkin you can use the two scripts <code>clean-sc.sh</code> and <code>clean-wc.sh</code>, they are located here in the scripts folder.</p> <p>They perform the following actions:</p> <ol> <li>Delete the added Helm charts</li> <li>Delete the added namespaces</li> <li>Delete any remaining PersistentVolumes</li> <li>Delete the added CustomResourceDefinitions</li> </ol> <p>Note</p> <p>If user namespaces are managed by Welkin apps then they will also be deleted if you clean up the Workload Cluster.</p>"},{"location":"operator-manual/clock-synchronization/","title":"Clock Synchronization","text":"<p>Welkin follows best practices from ntp.se regarding clock synchronization by default. This ensures reliable synchronization with at least two clock sources using NTP servers.</p> <p>Clock synchronization is important for the following reasons:</p> <ul> <li>Several Kubernetes components, in particular etcd, Rook/Ceph, do not work correctly if Nodes' clock drifts by more than 100ms;</li> <li>Several data protection regulations require it.</li> </ul> <p>Be aware of the following:</p> <ul> <li>Some regulations require synchronization with at least two clock sources. This could be two different NTP servers which can be traced to two different atomic clocks.</li> <li>Some regulations require synchronization with a specific time source. For example, MSBFS 2020:7 4 kap. 13 \u00a7 specifically requires synchronization with ntp.se.</li> </ul>","tags":["MSBFS 2020:7 4 kap. 13 \u00a7","NIST SP 800-171 3.3.7","ISO 27001 Annex A 8.17 Clock Synchronization"]},{"location":"operator-manual/cluster-sizing/","title":"Cluster Sizing","text":"<p>Welkin requires 26 CPUs and 72 GB of memory in addition to the capacity needed by your applications. Which for example could be 2 CPU and 8 GB of memory.</p>"},{"location":"operator-manual/cluster-sizing/#monitoring","title":"Monitoring","text":"<p>Monitoring stack (Thanos) can handle 6000 metrics per second in our standard configuration. This can be increased on demand.</p>"},{"location":"operator-manual/cluster-sizing/#logging","title":"Logging","text":"<p>Logging stack (OpenSearch) can take 100 records per second while provisioned with 12 CPUs and 24 GB of memory.</p>"},{"location":"operator-manual/common/","title":"Common","text":""},{"location":"operator-manual/common/#deploy-rook","title":"Deploy Rook","text":"<p>To deploy Rook, go to the <code>welkin-rook</code> repository and follow the instructions here for each Cluster.</p> <p>Note</p> <p>If the kubeconfig files for the Clusters are encrypted with SOPS, you need to decrypt them before using them:</p> <pre><code>sops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\nexport KUBECONFIG=$CLUSTER.yaml\n</code></pre> <p>Please restart the operator Pod, <code>rook-ceph-operator*</code>, if some Pods stalls in initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Warning</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>"},{"location":"operator-manual/common/#test-rook","title":"Test Rook","text":"<p>Note</p> <p>If the Workload Cluster kubeconfig is configured with authentication to Dex running in the Management Cluster, part of apps needs to be deployed before it is possible to run the commands below for <code>wc</code>.</p> <p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default apply -f https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/csi/rbd/pvc.yaml\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default apply -f https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/csi/rbd/pod.yaml\ndone\n\nfor CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default get pvc rbd-pvc\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default get pod csirbd-demo-pod\ndone\n</code></pre> <p>You should see PVCs in Bound state, and that the Pods which mounts the volumes are running.</p> <p>Important</p> <p>If you have taints on certain Nodes which should support running Pods that mounts <code>rook-ceph</code> PVCs, you need to ensure these Nodes are tolerated by the rook-ceph DaemonSet <code>csi-rbdplugin</code>, otherwise, Pods on these Nodes will not be able to attach or mount the volumes.</p> <p>If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default delete pvc rbd-pvc\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default delete pod csirbd-demo-pod\ndone\n</code></pre> <p>Now that the Kubernetes Clusters are up and running, we are ready to install Welkin Apps.</p>"},{"location":"operator-manual/common/#clone-compliantkubernetes-apps-and-install-pre-requisites","title":"Clone <code>compliantkubernetes-apps</code> and Install Pre-requisites","text":"<p>If you haven't done so already, clone the <code>compliantkubernetes-apps</code> repository and install pre-requisites.</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps.git\ncompliantkubernetes-apps/bin/ck8s install-requirements\n</code></pre> <p>Export the following variables:</p> <pre><code>export CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# run 'compliantkubernetes-apps/bin/ck8s providers' to list available providers\nexport CK8S_ENVIRONMENT_NAME=my-environment-name\nexport CK8S_FLAVOR=# run 'compliantkubernetes-apps/bin/ck8s flavors' to list available flavors\nexport CK8S_K8S_INSTALLER=# run 'compliantkubernetes-apps/bin/ck8s k8s-installers' to list available k8s-installers\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\nexport CLUSTERS=( \"sc\" \"wc\" )\nexport DOMAIN=example.com # your domain\n</code></pre>"},{"location":"operator-manual/common/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>compliantkubernetes-apps/bin/ck8s init both\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>"},{"location":"operator-manual/common/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/common-config.yaml\n</code></pre> <p>Edit the secrets.yaml file and add the credentials for:</p> <ul> <li>S3 - used for backup storage.</li> <li>Dex - connectors -- check your identity provider.</li> <li>On-call management tool configurations-- Check supported on-call management tools.</li> </ul> <pre><code>sops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>The default configuration for the Management Cluster and Workload Cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the Cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p>"},{"location":"operator-manual/common/#install-welkin-apps","title":"Install Welkin Apps","text":"<p>Start with the Management Cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s apply sc\n</code></pre> <p>Then the Workload Cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s apply wc\n</code></pre>"},{"location":"operator-manual/common/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from Let's Encrypt, perhaps as much as 20 minutes.</p> <p>Check if all Helm charts succeeded.</p> <pre><code>compliantkubernetes-apps/bin/ck8s ops helm wc list -A --all\n</code></pre> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in sc wc; do\n    compliantkubernetes-apps/bin/ck8s ops kubectl ${CLUSTER} get --all-namespaces pods\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in sc wc; do\n    compliantkubernetes-apps/bin/ck8s ops kubectl ${CLUSTER} get --all-namespaces issuers,clusterissuers,certificates\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>"},{"location":"operator-manual/common/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below:</p> <pre><code>for CLUSTER in sc wc; do\n  compliantkubernetes-apps/bin/ck8s test ${CLUSTER}\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Welkin's features.</p>"},{"location":"operator-manual/common/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following script to create required S3 buckets. The script uses <code>s3cmd</code> in the background and gets configuration and credentials for your S3 provider from <code>${HOME}/.s3cfg</code> file.</p> <pre><code># Use your default s3cmd config file: ${HOME}/.s3cfg\nscripts/S3/entry.sh create\n</code></pre> <p>Warning</p> <p>You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider.</p>"},{"location":"operator-manual/common/#test-s3","title":"Test S3","text":"<p>To ensure that you have configured S3 correctly, run the following snippet:</p> <pre><code>(\n    access_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq \".objectStorage.s3.accessKey\" {}')\n    secret_key=$(sops exec-file ${CK8S_CONFIG_PATH}/secrets.yaml 'yq \".objectStorage.s3.secretKey\" {}')\n    sc_config=$(yq eval-all '. as $item ireduce ({}; . * $item )' ${CK8S_CONFIG_PATH}/defaults/sc-config.yaml ${CK8S_CONFIG_PATH}/sc-config.yaml)\n    region=$(echo ${sc_config} | yq '.objectStorage.s3.region')\n    host=$(echo ${sc_config} | yq '.objectStorage.s3.regionEndpoint')\n\n    for bucket in $(echo ${sc_config} | yq '.objectStorage.buckets.*'); do\n        s3cmd --access_key=${access_key} --secret_key=${secret_key} \\\n            --region=${region} --host=${host} \\\n            ls s3://${bucket} &gt; /dev/null\n        [ ${?} = 0 ] &amp;&amp; echo \"Bucket ${bucket} exists!\"\n    done\n)\n</code></pre> <p>This page is out of date</p> <p>We are currently working on internal documentation to streamline Welkin onboarding for selected Infrastructure Providers. Until those documents are ready, and until we have capacity to make parts of that documentation public, this page is out-of-date.</p> <p>Nevertheless, parts of it are useful. Use at your own risk and don't expect things to work smoothly.</p> <p>For Welkin Enterprise Customers</p> <p>Please start by running these commands.</p> <p>If you are struggling, don't hesitate to file a ticket.</p> <p>You can run the following command from the compliantkubernetes-apps repository to collect diagnostic information that will help us support you. Ensure that you have put fingerprints received from Elastisys in a file named <code>${CK8S_CONFIG_PATH}/diagnostics_receiver.gpg</code>.</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt;\n</code></pre> <p>Show more examples on using the diagnostics command The command <code>ck8s diagnostics</code> can be provided with different flags to gather additional information from your environment, to see all available options run: <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --help\n</code></pre> <p>Some example use cases:</p> <ul> <li> <p>To include config files found in <code>CK8S_CONFIG_PATH</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --include-config\n</code></pre> </li> <li> <p>To retrieve more information such as YAML manifests for resources in a specific namespace, in this example <code>ingress-nginx</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> <li> <p>It is also possible to set which GPG keys should be used by setting <code>CK8S_PGP_FP</code>:</p> <pre><code>export CK8S_PGP_FP=&lt;gpg-fingerprint1&gt;,&lt;gpg-fingerprint2&gt;\n./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> </ul> <p>Please also provide us with your terminal in a text format. We need to look both at the commands you typed and their output.</p>"},{"location":"operator-manual/configure/","title":"Advanced Configuration","text":"<p>You have already been exposed to some of Welkin's configuration options while creating a Cluster. If not, read that section first.</p> <p>This section will outline some advanced configuration topics.</p>"},{"location":"operator-manual/configure/#overview","title":"Overview","text":"<p>Welkin is composed of two layers, the Kubernetes layer and the apps layer. Each is configured slightly differently.</p>"},{"location":"operator-manual/configure/#kubernetes-layer","title":"Kubernetes Layer","text":"<p>To find all configuration option of the Kubernetes layer, please read the upstream Kubespray documentation. Welkin overrides some of Kubespray's defaults, as shown here.</p>"},{"location":"operator-manual/configure/#apps-layer","title":"Apps Layer","text":"<p>The configuration of the apps layer is documented here and here.</p>"},{"location":"operator-manual/credentials/","title":"Use of Credentials","text":"<p>Welkin interacts with a lot of credentials. This document captures all of them in an orderly fashion, layer-by-layer.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#terminology","title":"Terminology","text":"<ul> <li>Purpose: Why are these credentials necessary, what can be done with them.</li> <li>Owner: The person (e.g., John Smith) or computing system (e.g., control plane Node, Pod) who controls the credentials, and is responsible for their safe storage and usage.</li> <li>Type: Individual credentials identify a person, while service accounts identify a computing system.</li> <li>Use for: What should these credentials be used for.</li> <li>Do not use for: When should these credentials NOT be used, although they technically could.</li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#single-sign-on-sso-credentials","title":"Single Sign-On (SSO) Credentials","text":"<ul> <li>Example: Company Google Accounts</li> <li>Purpose: authenticate a person with various systems, in particular<ul> <li>Kubernetes API via Dex</li> <li>Grafana via Dex</li> <li>OpenSearch Dashboards via Dex</li> <li>Harbor via Dex</li> </ul> </li> <li>Owner: individual person (user or administrator)</li> <li>Type: individual credentials</li> <li>Use for: identifying yourself</li> <li>Do not use for:<ul> <li>These credentials are super valuable and should not be shared with anyone, not even family, friends, workmates, etc., even if requested. Report such sharing requests.</li> </ul> </li> <li>Misc:<ul> <li>Protect using 2FA</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#infrastructure-provider-credentials","title":"Infrastructure Provider Credentials","text":"<ul> <li>Purpose: create infrastructure, e.g., VMs, load balancers, networks, buckets.</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Terraform layer in Kubespray</li> <li>Creating and destroying buckets via helper scripts</li> </ul> </li> <li>Do not use for:<ul> <li>Kubernetes cloud-controller integration, use Cloud Controller Credentials instead.</li> <li>Access to object storage / S3 bucket, use backup credentials instead.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#ssh-keys","title":"SSH Keys","text":"<ul> <li>Purpose: access Nodes for setup, break glass or disaster recovery</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Accessing Nodes via SSH</li> </ul> </li> <li>Do not use for:<ul> <li>Giving a system access to a Git repository. Create a separate SSH key only for that purpose instead.</li> </ul> </li> <li>Important considerations:<ul> <li>When generating an SSH key, see Cryptography.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#pgp-keys","title":"PGP Keys","text":"<ul> <li>Purpose: encrypt/decrypt sensitive information, e.g., service account credentials, Application Developer names, incident reports, financial information, etc.</li> <li>Owner: administrator</li> <li>Type: individual credentials</li> <li>Use for:<ul> <li>Encrypting/decrypting sensitive information</li> </ul> </li> <li>Do not use for:<ul> <li>Encrypting/decrypting individual credentials. These are meant to be individual and never shared.</li> <li>Encrypting/decrypting SSH key. These are meant to be individual and never shared. Prefer protecting your SSH key with a passphrase or storing it on a YubiKey.</li> <li>Encrypting non-sensitive information. This leads to a culture of \"security by obscurity\" in which people over-rely on encryption. Prefer being mindful about what data you store and why. If unsure, prefer not storing credentials, as Infrastructure Provider Credentials and SSH keys should be enough to restore any access.</li> </ul> </li> <li>Important considerations:<ul> <li>When generating a GPG key, see Cryptography.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#cloud-controller-integration-credentials","title":"Cloud Controller (Integration) Credentials","text":"<ul> <li>Purpose: allow Kubernetes control Nodes, specifically the cloud-controller-manager, to create LoadBalancers and PersistentVolumes</li> <li>Owner: each Kubernetes Cluster should have their own</li> <li>Type: service account</li> <li>Use for:<ul> <li>Configuring Kubespray to set up a Kubernetes Cluster with cloud integration</li> </ul> </li> <li>Do not use for:<ul> <li>AWS. Use AWS IAM Node Roles instead.</li> <li>Exoscale. We currently don't integrate with Exoscale for LoadBalancer or PersistentVolumes.</li> <li>Terraform layer in Kubespray</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#backup-and-long-term-logging-credentials","title":"Backup and Long-Term Logging Credentials","text":"<ul> <li>Purpose:<ul> <li>Allow backup of various components, e.g., PVCs via Velero, Thanos metrics, OpenSearch Indexes, PostgreSQL databases.</li> <li>Allow long-term logging, e.g., Management Cluster logs</li> </ul> </li> <li>Owner: each Welkin Cluster should have their own</li> <li>Type: service account</li> <li>Use for:<ul> <li>Backup</li> <li>Logging</li> </ul> </li> <li>Do not use for:<ul> <li>Other object storage, e.g., Harbor container images</li> <li>Disaster recovery, investigations. Use Infrastructure Provider credentials instead.</li> </ul> </li> <li>Misc:<ul> <li>Ensure these credentials are write-only, if supported by the underlying Infrastructure Provider, to comply with ISO 27001 A.12.3.1 Information Backup and ISO 27001 A.12.4.2 Protection of Log Information. As of 2021-05-20, this is supported by AWS S3, Exoscale S3, GCP and Safespring S3.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#opsgenie-credentials","title":"OpsGenie Credentials","text":"<ul> <li>Purpose:<ul> <li>Allow the Cluster to issue alerts to OpsGenie.</li> </ul> </li> <li>Owner: each Welkin Cluster should have their own</li> <li>Type: service account</li> <li>Use for: alerting</li> <li>Do not use for:<ul> <li>Operator access to OpsGenie. Prefer Single Sign-On (SSO).</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#dex-openid-client-secret","title":"Dex OpenID Client Secret","text":"<ul> <li>Purpose:<ul> <li>Complete the \"OAuth dance\" between Grafana, OpenSearch Dashboard, Harbor and kubectl, on one side, and Dex, on the other side.</li> <li>Used both by administrators and users.</li> </ul> </li> <li>Owner: each Welkin Cluster should have their own</li> <li>Type: not secret</li> <li>Misc:<ul> <li>We have determined that the OpenID client secret should not be treated as a secret. See risk analysis here and here.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#kubeconfig-with-openid-authentication","title":"Kubeconfig with OpenID Authentication","text":"<ul> <li>Purpose: access the Kubernetes API in normal situations</li> <li>Owner: shared between administrators and users</li> <li>Type: not secret</li> <li>Use for:<ul> <li>Routine checks</li> <li>Routine maintenance</li> <li>Investigations</li> <li>\"Simple\" recovery</li> </ul> </li> <li>Misc:<ul> <li>If these credentials become unusable, you are in a \"break glass\" situation. Use Infrastructure Provider credentials or SSH keys to initiate disaster recovery.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/credentials/#kubeconfig-with-client-certificate-key","title":"Kubeconfig with Client Certificate Key","text":"<ul> <li>Purpose: access the Kubernetes API for disaster recovery, break glass or initial setup</li> <li>Owner: shared between administrators</li> <li>Type: special</li> <li>Use for:<ul> <li>Initial setup</li> <li>Break glass</li> <li>Disaster recovery</li> </ul> </li> <li>Do not use for:<ul> <li>Routine maintenance or investigation. Use Kubeconfig with OpenID Authentication</li> </ul> </li> <li>Misc:<ul> <li>Such a Kubeconfig is available on all control plane Nodes at <code>/etc/kubernetes/admin.conf</code>. SSH into a control plane Node then type <code>sudo su</code> and you can readily use <code>kubectl</code> commands.</li> <li>Unless absolutely necessary, avoid storing this file outside the control plane Nodes.</li> <li>If, for some good reason, you downloaded this file, <code>shred</code> it after usage.</li> </ul> </li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter"]},{"location":"operator-manual/cryptography/","title":"Use of Cryptography","text":"<p>Welkin recommends the ECRYPT-CSA \"near term use\". The key cryptographic parameters are listed below.</p>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/cryptography/#recommended-strengths","title":"Recommended Strengths","text":"Cryptographic Structure Size Symmetric 128 Factoring Modulus 3072 Discrete Logarithm 256/3072 Elliptic Group 256 Hash 256","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/cryptography/#recommended-algorithms","title":"Recommended Algorithms","text":"Function Algorithm Block Ciphers AESCamelliaSerpent Hash Functions SHA-2 (256, 384, 512, 512/256)SHA-3 (256, 384, 512, SHAKE128, SHAKE256)Whirlpool (512)BLAKE (256, 584, 512) Public Key Primitive RSA (&gt;3072)  DSA (&gt;256/3072)  ECDSA (&gt;256)","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/cryptography/#recommended-implementation","title":"Recommended Implementation","text":"<p>Ubuntu 22.04 already generates SSH and GPG keys conforming to this recommendation, as evidenced below:</p> <pre><code>$ ssh-keygen\nGenerating public/private rsa key pair.\n[...]\n+---[RSA 3072]----+\n|           o+.=++|\n|           +o..= |\n|        = =...o  |\n|       O @.    o |\n|      . S +.  . .|\n|       + B  .. .E|\n|      . O o ..o  |\n|       o + +o... |\n|          +oo=o  |\n+----[SHA256]-----+\n$ gpg --generate-key\ngpg (GnuPG) 2.2.27; Copyright (C) 2021 Free Software Foundation, Inc.\n[...]\npub   rsa3072 2023-03-24 [SC] [expires: 2025-03-23]\n      41E32D8838ADA81B4D57333E79797753D349F087\nuid                      Cristian Klein &lt;cristian.klein@example.com&gt;\nsub   rsa3072 2023-03-24 [E] [expires: 2025-03-23]\n</code></pre>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/cryptography/#notes-on-https-traffic","title":"Notes on HTTPS Traffic","text":"<p>For HTTPS traffic, Welkin allows either TLS 1.2 or TLS 1.3. TLS 1.3 mandates forward secrecy. TLS 1.2 makes forward secrecy optional, however, the default cipher list in Welkin prioritizes algorithms that provide perfect forward secrecy. In brief, you can rely on forward secrecy with most browsers in use today.</p> <p>Forward secrecy addresses the \"store now, decrypt later\" attack. In essence, an attacker cannot decrypt past HTTPS transmissions even if the TLS certificate (private key) is compromised.</p> <p>Welkin uses RSA 2048 when provisioning HTTPS certificates, which is lower than the present recommendation. However, these certificates have a short expiration time of 3 months. Hence, with short certificate expiration time and forward secrecy, usage of RSA 2048 for HTTPS certificates does not add a security risk.</p> <p>We recommend you to regularly run the Qualys SSL Server Test against the application HTTPS endpoints to make sure encrypted-in-transit sufficiently protects your data. At the time of this writing, Welkin receives A+ overall rating.</p>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/cryptography/#further-reading","title":"Further Reading","text":"<ul> <li>ECRYPT\u2013CSA D5.4 Algorithms, Key Size and Protocols Report (2018)</li> <li>BlueCrypt Cryptographic Key Length Recommendation</li> <li>Ingress NGINX SSL Ciphers</li> <li>Mozilla SSL recommendations</li> <li>Forward secrecy</li> </ul>","tags":["MSBFS 2020:7 4 kap. 9 \u00a7","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","NIST SP 800-171 3.13.10","NIST SP 800-171 3.13.11","NIS2 Minimum Requirement (h) Cryptography","ISO 27001 Annex A 8.24 Use of Cryptography"]},{"location":"operator-manual/disaster-recovery/","title":"Disaster Recovery","text":"<p>This document details disaster recovery procedures for Welkin. These procedures must be executed by the administrator. Most commands found in these instructions are expected to be run from the compliantkubernetes-apps repository.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#compliance-need","title":"Compliance Need","text":"<p>Disaster recovery is mandated by several regulations and information security standards. For example, in ISO 27001:2013, the annexes that mostly concerns disaster recovery are:</p> <ul> <li>A.12.3.1 Information Backup</li> <li>A.17.1.1 Planning Information Security Continuity</li> </ul>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#off-site-backups","title":"Off-site backups","text":"<p>Backups can be set up to be replicated off-site using CronJobs.</p> <p>If these are encrypted then these off-site backups must first be restored themselves before they can be used to restore other services.</p> <p>If these are unencrypted then these off-site backups can be used directly to restore other services by reconfiguring which object storage service they are using.</p> <p>See the instructions in <code>compliantkubernetes-apps</code> for how to restore off-site backups.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#when-a-new-regioninfrastructure-provider-is-used","title":"When a new region/Infrastructure Provider is used","text":"<ul> <li>Configure and set base ck8s-configs:</li> </ul> <p>sc-config.yaml:</p> <p><code>harbor.persistence.swift.*</code>,<code>objectStorage.sync.*</code></p> <p>common-config.yaml:</p> <p><code>objectStorage.s3.region</code>, <code>objectStorage.s3.regionEndpoint</code> if S3 is used.</p> <p>secrets.yaml:</p> <p><code>dex.connectors.*</code>, <code>harbor.persistence.swift.username</code>, <code>harbor.persistence.swift.password</code>.</p> <p><code>objectStorage.azure.storageAccountKey</code> if Azure is used.</p> <p><code>objectStorage.s3.accessKey</code>, <code>objectStorage.s3.secretKey</code> if S3 is used.</p> <p>.state/s3cfg.ini: (only if S3 is used)</p> <p><code>access_key</code>, <code>secret_key</code>, <code>host_base</code>, <code>host_bucket</code></p> <ul> <li>Configure and set custom ck8s-configs:</li> </ul> <p>Examples can be files containing Identity Provider, Infrastructure Provider, or DNS critical information.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#opensearch","title":"OpenSearch","text":"","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#backup","title":"Backup","text":"<p>OpenSearch is set up to store snapshots in object storage. There is a Snapshot Management (SM) policy in OpenSearch which is responsible for the lifecycle management of snapshots, i.e. creation and retention. Note that any manually created snapshot will not be handled by the SM policy, and should be manually cleaned up when no longer needed. If you need to create a snapshot on-demand, you may do so through the OpenSearch API.</p> <p>Configure the following variables:</p> <pre><code>user=admin\npassword=$(sops -d ${CK8S_CONFIG_PATH}/secrets.yaml | yq '.opensearch.adminPassword')\nos_url=https://opensearch.$(yq '.global.opsDomain' ${CK8S_CONFIG_PATH}/common-config.yaml)\n</code></pre> <p>Get the name of the snapshot repository:</p> <pre><code>curl -L -u \"${user}:${password}\" \"${os_url}/_cat/repositories?v\"\n</code></pre> <p>Set variable for the snapshot repository:</p> <pre><code>snapshot_repo=&lt;name/id from previous step&gt;\n</code></pre> <p>Take snapshot:</p> <pre><code>curl -L -u \"${user}:${password}\" -X PUT \"${os_url}/_snapshot/${snapshot_repo}/manual-snapshot\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"*,-.opendistro_security\",\n  \"include_global_state\": false\n}\n'\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#optional-start-new-cluster-from-snapshot","title":"Optional: Start new Cluster from snapshot","text":"<p>Note</p> <p>Only perform the steps in this section if you are starting a new Cluster from a snapshot. Otherwise, skip ahead to the Restore section.</p> <p>Before you install OpenSearch you should disable the initial index creation to make the restore process leaner by setting the following configuration option:</p> <pre><code>opensearch.createIndices: false\n</code></pre> <p>Install the OpenSearch suite:</p> <pre><code>./bin/ck8s ops helmfile sc -l app=opensearch apply\n</code></pre> <p>Snapshots are by default taken regularly. To avoid uploading empty snapshots consider suspending this automatic process during recovery:</p> <pre><code>curl -L -u \"${user}:${password}\" -X POST \"${os_url}/_plugins/_sm/policies/snapshot_management_policy/_stop\"\n</code></pre> <p>After the installation, continue to the Restore section to proceed with the restore. If you want to restore all indices, use the following <code>indices</code> variable:</p> <pre><code>indices=\"kubernetes-*,kubeaudit-*,other-*,authlog-*\"\n</code></pre> <p>Note</p> <p>This process assumes that you are using the same bucket as your previous Cluster. If you aren't:</p> <ul> <li>Register a new snapshot repository to the old bucket as described here</li> <li>Use the newly registered snapshot repository in the restore process</li> </ul>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restore","title":"Restore","text":"<p>Set the following variables:</p> <ol> <li>OpenSearch user with permissions to manage snapshots, usually <code>admin</code></li> <li>The password for the above user</li> <li>The URL to OpenSearch</li> </ol> <pre><code>user=admin\npassword=$(sops -d ${CK8S_CONFIG_PATH}/secrets.yaml | yq '.opensearch.adminPassword')\nos_url=https://opensearch.$(yq '.global.opsDomain' ${CK8S_CONFIG_PATH}/common-config.yaml)\n</code></pre> <p>Restoring from off-site backup</p> <ul> <li> <p>To restore using Rclone:</p> <p>If buckets were restored from an off-site object store with Rclone after OpenSearch was installed, remove the new snapshot repository created with the fresh install, then sync OpenSearch to let the configurer re-run, making you able to use the restored snapshots.</p> </li> <li> <p>To restore from an encrypted off-site backup:</p> <p>First import the backup into the main object storage and register the restored bucket as a new snapshot repository:</p> <p> For S3-compatible storage <pre><code>curl -L -u \"${user}:${password}\" -X PUT \"${os_url}/_snapshot/backup-repository?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"type\": \"s3\",\n  \"settings\": {\n    \"bucket\": \"&lt;restored-bucket&gt;\",\n    \"readonly\": true\n  }\n}\n'\n</code></pre> </p> <p> For Azure-based storage <pre><code>curl -L -u \"${user}:${password}\" -X PUT \"${os_url}/_snapshot/backup-repository?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"type\": \"azure\",\n  \"settings\": {\n    \"container\": \"&lt;restored-bucket&gt;\",\n    \"client\": \"default\",\n    \"readonly\": true\n  }\n}\n'\n</code></pre> </p> <p>Then restore from this snapshot repository (<code>backup-repository</code>) in OpenSearch.</p> </li> <li> <p>To restore from an unencrypted off-site backup:</p> <p>Configure the off-site object storage as the main object storage for apps and OpenSearch respectively, then update the OpenSearch Helm releases and perform the restore. It is recommended to either suspend or remove the OpenSearch backup CronJob to prevent it from running while restoring.</p> <p>Remember to revert to the regular object storage afterwards and reactivate the backup CronJob!  Replace the previous snapshot repository if it is unusable.</p> </li> </ul> <p>List snapshot repositories:</p> <pre><code>curl -L -u \"${user}:${password}\" \"${os_url}/_cat/repositories?v\"\n</code></pre> <p>Output should be similar to:</p> <pre><code>id                   type\nopensearch-snapshots   s3\n</code></pre> Detailed output <pre><code>curl -L -u \"${user}:${password}\" \"${os_url}/_snapshot/?pretty\"\n{\n  \"opensearch-snapshots\" : {\n    \"type\" : \"s3\",\n    \"settings\" : {\n      \"bucket\" : \"opensearch-backup\",\n      \"client\" : \"default\"\n    }\n  }\n}\n</code></pre> <p>List available snapshots:</p> <pre><code>snapshot_repo=&lt;name/id from previous step&gt;\n\ncurl -L -u \"${user}:${password}\" \"${os_url}/_cat/snapshots/${snapshot_repo}?v&amp;s=id\"\n</code></pre> <p>Output should be similar to:</p> <pre><code>id                         status start_epoch start_time end_epoch  end_time duration indices successful_shards failed_shards total_shards\nsnapshot-20211231_120002z SUCCESS 1640952003  12:00:03   1640952082 12:01:22     1.3m      54                54             0           54\nsnapshot-20220101_000003z SUCCESS 1640995203  00:00:03   1640995367 00:02:47     2.7m      59                59             0           59\nsnapshot-20220101_120002z SUCCESS 1641038403  12:00:03   1641038533 12:02:13     2.1m      57                57             0           57\n...\n</code></pre> Detailed output <pre><code># Detailed list of all snapshots\ncurl -L -u \"${user}:${password}\" \"${os_url}/_snapshot/${snapshot_repo}/_all?pretty\"\n\n# Detailed list of specific snapshot\ncurl -L -u \"${user}:${password}\" \"${os_url}/_snapshot/${snapshot_repo}/snapshot-20220104_120002z?pretty\"\n{{\n  \"snapshots\" : [\n    {\n      \"snapshot\" : \"snapshot-20220104_120002z\",\n      \"uuid\" : \"oClQdNAyTeiEmZb5dVh0SQ\",\n      \"version_id\" : 135238127,\n      \"version\" : \"1.2.3\",\n      \"indices\" : [\n        \"authlog-default-2021.12.20-000001\",\n        \"authlog-default-2021.12.30-000011\",\n        \"authlog-default-2022.01.03-000015\",\n        \"other-default-2021.12.30-000011\",\n        ...\n      ],\n      \"data_streams\" : [ ],\n      \"include_global_state\" : false,\n      \"state\" : \"SUCCESS\",\n      \"start_time\" : \"2022-01-04T12:00:02.596Z\",\n      \"start_time_in_millis\" : 1641297602596,\n      \"end_time\" : \"2022-01-04T12:01:07.833Z\",\n      \"end_time_in_millis\" : 1641297667833,\n      \"duration_in_millis\" : 65237,\n      \"failures\" : [ ],\n      \"shards\" : {\n        \"total\" : 66,\n        \"failed\" : 0,\n        \"successful\" : 66\n      }\n    }\n  ]\n}\n</code></pre> <p>You usually select the latest snapshot containing the indices you want to restore. Restore one or multiple indices from a snapshot</p> <p>Note</p> <p>You cannot restore a write index (the latest index) if you already have a write index connected to the same index alias (which will happen if you have started to receive logs).</p> <pre><code>snapshot_name=&lt;Snapshot name from previous step&gt;\n# Use \"-.*\" if index per namespace is enabled\nindices=\"kubernetes-*,kubeaudit-*,other-*,authlog-*\"\n\ncurl -L -u \"${user}:${password}\" -X POST \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"'${indices}'\"\n}\n'\n</code></pre> <p>To monitor the progress of the restore process and make sure that the index or indices are restored as expected you can watch the indices details:</p> <pre><code>watch -n 2 curl -Ls -u \"${user}:${password}\" \"${os_url}/_cat/indices/${indices}?v\"\n</code></pre> <p>The output will update every 2 seconds and show a list of the restored indices. The health of all indices should eventually transition to <code>green</code> and other columns such as <code>docs.count</code> (Total documents) and <code>store.size</code> (Total size) will be populated with their expected values.</p> <p>Confirm the Cluster health status:</p> <pre><code>curl -Ls -u \"${user}:${password}\" \"${os_url}/_cluster/health\" | jq -r '.status'\n</code></pre> <p>Output should say <code>green</code> if the Cluster is healthy.</p> <p>Note</p> <p>If you previously suspended the automatic creation of snapshots via the <code>snapshot_management_policy</code>, you should now enable it once again:</p> <pre><code>curl -L -u \"${user}:${password}\" -X POST \"${os_url}/_plugins/_sm/policies/snapshot_management_policy/_start\"\n</code></pre> <p>Read the documentation to see the API, all parameters and their explanations.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restoring-opensearch-dashboards-data","title":"Restoring OpenSearch Dashboards data","text":"<p>Data in OpenSearch Dashboards (saved searches, visualizations, dashboards, etc) is stored in the indices pointed to by the alias <code>.kibana</code>. To restore that data you first need to delete the index and then do a restore.</p> <p>This will overwrite anything in the current <code>.kibana_x</code> index. If there is something new that should be saved, then export the saved objects and import them after the restore.</p> <p>There can be multiple <code>.kibana</code> indices in OpenSearch, the current index should be the one you want to restore. To view your dashboard indices, follow these steps.</p> <pre><code>snapshot_name=&lt;Snapshot name from previous step&gt;\n\ncurl -L -u \"${user}:${password}\" -X GET ${os_url}'/.kibana*?pretty' | jq 'keys'\n</code></pre> <p>If multiple <code>.kibana_x</code> indices show up, run this to see the index that the alias is currently looking at.</p> <pre><code>curl -L -u \"${user}:${password}\" -X GET ${os_url}'/_alias/.kibana*?pretty' | jq 'keys'\n</code></pre> <p>Make sure that the index you want to restore also exists on the snapshot. (May be an issue if you are using an old snapshot)</p> <pre><code>curl -L -u \"${user}:${password}\" -X GET \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}?pretty\" | jq '.snapshots[].indices' | grep .kibana\n</code></pre> <p>Note</p> <p>If you visit the <code>\"&lt;os_url&gt;/app/dashboards\"</code> page in the OpenSearch GUI after deleting the index and before restoring the index, another empty index <code>.kibana</code> will be created. You need to delete this manually, which can be done with:</p> <pre><code>curl -L -u \"${user}:${password}\" -X DELETE \"${os_url}/.kibana?pretty\"\n</code></pre> <pre><code>index_to_restore=&lt;Index name from previous step&gt;\n\ncurl -L -u \"${user}:${password}\" -X DELETE \"${os_url}/${index_to_restore}?pretty\"\n\ncurl -L -u \"${user}:${password}\" -X POST \"${os_url}/_snapshot/${snapshot_repo}/${snapshot_name}/_restore?pretty\" -H 'Content-Type: application/json' -d'\n{\n  \"indices\": \"'${index_to_restore}'\"\n}\n'\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#harbor","title":"Harbor","text":"","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#backup_1","title":"Backup","text":"<p>Harbor is set up to store backups of the database in object storage (note that this does not include the actual images, since those are already stored in object storage by default). There is a CronJob called <code>harbor-backup-cronjob</code> in the Cluster that is taking a database dump and uploading it.</p> <p>To take a backup on-demand, execute</p> <pre><code>./bin/ck8s ops kubectl sc -n harbor create job --from=cronjob/harbor-backup-cronjob &lt;name-of-job&gt;\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restore_1","title":"Restore","text":"<p>Restoring from off-site backup</p> <p>Since Harbor stores both database backups and images in the same bucket it is recommended to restore the off-site backup into the main object storage first, reconfigure Harbor to use it, then restore the database from it.</p> <p>Instructions for how to restore Harbor can be found in <code>compliantkubernetes-apps</code>: https://github.com/elastisys/compliantkubernetes-apps/blob/main/restore/harbor/README.md</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#velero","title":"Velero","text":"<p>These instructions focuses on backups for the Workload Cluster using the Velero CLI. For instructions on using Velero in the Management Cluster see the Grafana section.</p> <p>Read more about Velero here.</p> <p>Note</p> <p>This documentation uses the Velero CLI, as opposed to Velero CRDs, since that is what is encouraged by upstream documentation.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#backup_2","title":"Backup","text":"<p>Velero is set up to take daily backups and store them in object storage. The daily backup will not take backups of everything in a Kubernetes Cluster, it will instead look for certain labels and annotations. Read more about those labels and annotations here.</p> <p>It is also possible to take on-demand backups. Then you can freely chose what to backup and do not have to base it on the same labels. Here is a basic example of how to use Velero to take a backup of all Kubernetes resources (though not the data in the volumes by default):</p> <pre><code>./bin/ck8s ops velero wc backup create manual-backup\n</code></pre> <p>If you want to create a backup from existing schedule you can run the following:</p> <pre><code>./bin/ck8s ops velero wc backup create --from-schedule velero-daily-backup --wait\n</code></pre> <p>Tip</p> <p>Check which arguments you can use by running <code>./bin/ck8s ops velero wc backup create --help</code>.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restore_2","title":"Restore","text":"<p>Note</p> <p>If you are restoring an environment under a new domain name then there is a possibility to reconfigure image references with Velero, but other resources that might contain domain names such as Ingresses, ConfigMaps and Secrets must be updated manually.</p> <p>If you are restoring an environment and want or need to change the StorageClass of PersistentVolumes then it is possible to configure a StorageClass mapping, see the Velero documentation.</p> <p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>The default daily Velero backup in the Workload Cluster is used to backup everything in application developer namespaces and the Alertmanager configuration secret. If you want to restore the entire Workload Cluster, first you should delete the Alertmanager secret in WC, then all you need to run to restore the state from the latest daily backup is:</p> <pre><code># Delete the alertmanager secret so that it can be restored from backup\n./bin/ck8s ops kubectl wc delete secret -n alertmanager alertmanager-kube-prometheus-stack-alertmanager\n# Make sure application dependencies like Harbor, Postgres, RabbitMQ\n# and Redis are restored/installed before restoring wc with velero.\n# If this environment has managed ArgoCD, then that should be restored later.\n./bin/ck8s ops velero wc restore create --from-schedule velero-daily-backup --exclude-namespaces argocd-system --wait\n</code></pre> <p>Velero in the Management Cluster is used to backup user Grafana. See the section <code>Grafana</code> further down on this page for instructions on how to restore that.</p> <p>Tip</p> <p>Use <code>./bin/ck8s ops velero wc restore create --help</code> to see available flags and some examples. If a backup has a status of PartiallyFailed, the argument <code>--allow-partially-failed</code> can be used to restore from such a backup. If a backup or restore gets stuck or has other issues, refer to this guide.</p> <p>This command will wait until the restore has finished. You can also do partial restorations, e.g. just restoring one namespace, by using different arguments. You can also restore from manual backups by using the flag <code>--from-backup &lt;backup-name&gt;</code></p> <p>Persistent Volumes are only restored if a Pod with the backup annotation is restored. Multiple Pods can have an annotation for the same Persistent Volume. When restoring the Persistent Volume it will overwrite any existing files with the same names as the files to be restored. Any other files will be left as they were before the restoration started. So a restore will not wipe the volume clean and then restore. If a clean wipe is the desired behavior, then the volume must be wiped manually before restoring.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#example-restoring-a-volume-in-waitforfirstconsumer-mode","title":"Example restoring a volume in WaitForFirstConsumer mode","text":"<p>Restoring volumes with the volume binding mode <code>WaitForFirstConsumer</code> requires some extra steps, as Velero will not restore the data until a Pod binds the volume.</p> <p>So by either manually creating a Pod of the kind that normally binds the volume or creating a special purpose Pod that only binds the volume, we can let Velero complete the restoration.</p> <p>For the second case, the following method can be used to get the restore to proceed to completion without starting the actual Pods that use them.</p> <p>Check if there are any volumes stuck in Pending.</p> <pre><code>kubectl get pvc -A | sed -n '1p;/Pending/p'\n</code></pre> <p>If there were no stuck volumes then you are done.</p> <p>Save the following as: <code>volume-restorer-pod.yaml</code></p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: volume-restore-helper\n  namespace: default\nspec:\n  automountServiceAccountToken: false\n  containers:\n    - image: busybox:stable\n      imagePullPolicy: IfNotPresent\n      name: sleeper\n      resources:\n        limits:\n          cpu: 10m\n          memory: 5Mi\n        requests:\n          cpu: 10m\n          memory: 5Mi\n      securityContext:\n        allowPrivilegeEscalation: false\n        capabilities:\n          drop:\n            - ALL\n        privileged: false\n        runAsGroup: 1\n        runAsNonRoot: true\n        runAsUser: 10000\n        runAsGroup: 10000\n        seccompProfile:\n          type: RuntimeDefault\n      volumeMounts:\n        # Adjust this `mountPath` to match the real pod.\n        - mountPath: /mnt\n          name: restore-me\n      args: # Do nothing while Velero restores data.\n        - sleep\n        - inf\n  tolerations:\n    - effect: NoExecute\n      key: node.kubernetes.io/not-ready\n      operator: Exists\n      tolerationSeconds: 300\n    - effect: NoExecute\n      key: node.kubernetes.io/unreachable\n      operator: Exists\n      tolerationSeconds: 300\n  volumes:\n    - name: restore-me\n      persistentVolumeClaim:\n        claimName: some-data # Enter the volume you want to restore\n  securityContext:\n    fsGroup: 10000\n</code></pre> <ol> <li>Fill in the volume based on <code>kubectl get pvc</code> or <code>kubectl get pv</code>.</li> <li>Adjust the <code>mountPath</code> to match what the original Pod uses.</li> <li>Create Pod using <code>kubectl apply -f volume-restorer-pod.yaml</code>.</li> <li>Watch <code>velero restore get</code> for Velero to finish.</li> <li>Finally, delete the restorer Pod using <code>kubectl delete -f volume-restorer-pod.yaml</code>.</li> </ol>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#example-restoring-a-partially-failed-backup","title":"Example restoring a partially failed backup","text":"<p>A backup that has status <code>PartiallyFailed</code> can be restored by using <code>--allow-partially-failed</code> flag:</p> <pre><code>./bin/ck8s ops velero wc restore create &lt;restore-name&gt; --allow-partially-failed --from-schedule velero-daily-backup --wait\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#example-restoring-a-single-resource","title":"Example restoring a single resource","text":"<p>You can explore a <code>Completed</code> backup as follows</p> <pre><code>./bin/ck8s ops velero wc backup describe --details &lt;name-of-backup&gt;\n</code></pre> <p>and you can then use the following to handpick resources from the backup you want restored:</p> <pre><code>./bin/ck8s ops velero wc restore create &lt;restore-name&gt;  --include-resources pod,volume --from-backup &lt;backup-name&gt; --include-namespaces &lt;namespace-name&gt; --selector &lt;resource-selector&gt; --wait\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restore-from-off-site-backup","title":"Restore from off-site backup","text":"<ul> <li>Restoring from encrypted off-site backup:</li> </ul> <p>Recover the encrypted bucket into the main object storage and reconfigure Velero to use this bucket, then follow the regular instructions.</p> <p>The references in Kubernetes might need to be deleted so Velero can resync from the bucket:</p> <pre><code># Note that this is only backup metadata\n./bin/ck8s ops kubectl sc -n velero delete backups.velero.io --all\n\n./bin/ck8s ops kubectl wc -n velero delete backups.velero.io --all\n</code></pre> <ul> <li>Restoring from unencrypted off-site backup:</li> </ul> <p>To recover directly from off-site backup the backup-location must be reconfigured.   Follow the instructions for the relevant storage type below.</p> S3-compatible object storage <pre><code>export CLUSTER=\"&lt;sc|wc&gt;\"\nexport S3_BUCKET=\"&lt;off-site-s3-bucket&gt;\" # Do not include s3:// prefix\nexport S3_PREFIX=\"&lt;service-cluster|workload-cluster&gt;\"\nexport S3_ACCESS_KEY=$(sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"accessKey\"]' \"$CK8S_CONFIG_PATH/secrets.yaml\")\nexport S3_SECRET_KEY=$(sops -d --extract '[\"objectStorage\"][\"sync\"][\"s3\"][\"secretKey\"]' \"$CK8S_CONFIG_PATH/secrets.yaml\")\nexport S3_REGION=$(yq \".objectStorage.sync.s3.region\" \"$CK8S_CONFIG_PATH/sc-config.yaml\" )\nexport S3_ENDPOINT=$(yq \".objectStorage.sync.s3.regionEndpoint\" \"$CK8S_CONFIG_PATH/sc-config.yaml\")\nexport S3_PATH_STYLE=$(yq \".objectStorage.sync.s3.forcePathStyle\" \"$CK8S_CONFIG_PATH/sc-config.yaml\")\n\n# Delete backups from default backup location, note that this is only the backup metadata\n./bin/ck8s ops kubectl \"${CLUSTER}\" -n velero delete backups.velero.io --all\n\n# Delete default backup location\n./bin/ck8s ops velero \"${CLUSTER}\" backup-location delete default\n\n# Create off-site credentials\nkubectl -n velero create secret generic velero-backup \\\n  --from-literal=cloud=\"$(echo -e \"[default]\\naws_access_key_id: ${S3_ACCESS_KEY}\\naws_secret_access_key: ${S3_SECRET_KEY}\\n\")\"\n\n# Create off-site backup location\n./bin/ck8s ops velero \"${CLUSTER}\" backup-location create backup \\\n    --access-mode ReadOnly \\\n    --provider aws \\\n    --bucket \"${S3_BUCKET}\" \\\n    --prefix \"${S3_PREFIX}\" \\\n    --config=\"region=${S3_REGION},s3Url=${S3_ENDPOINT},s3ForcePathStyle=${S3_PATH_STYLE}\" \\\n    --credential=velero-backup=cloud\n</code></pre> Azure-based storage <pre><code>export CLUSTER=&lt;sc|wc&gt;\nexport AZURE_CONTAINER=&lt;off-site-azure-blob-container&gt;\nexport AZURE_PREFIX=&lt;service-cluster|workload-cluster&gt;\nexport AZURE_RESOURCE_GROUP=$(yq \".objectStorage.sync.azure.resourceGroup\" \"$CK8S_CONFIG_PATH/sc-config.yaml\")\nexport AZURE_STORAGE_ACCOUNT=$(yq \".objectStorage.sync.azure.storageAccountName\" \"$CK8S_CONFIG_PATH/sc-config.yaml\")\nexport AZURE_STORAGE_ACCOUNT_KEY=$(sops -d --extract '[\"objectStorage\"][\"sync\"][\"azure\"][\"storageAccountKey\"]' \"$CK8S_CONFIG_PATH/secrets.yaml\")\n\n# Create secret for credentials\n./bin/ck8s ops kubectl \"${CLUSTER}\" -n velero create secret generic velero-backup --from-literal=cloud=\"$(echo -e \"AZURE_STORAGE_ACCOUNT_KEY=${AZURE_STORAGE_ACCOUNT_KEY}\\nAZURE_CLOUD_NAME=AzurePublicCloud\")\"\n\n# Create off-site backup location\n./bin/ck8s ops velero \"${CLUSTER}\" backup-location create backup \\\n    --access-mode ReadOnly \\\n    --provider azure \\\n    --bucket \"${AZURE_CONTAINER}\" \\\n    --prefix \"${AZURE_PREFIX}\" \\\n    --config=\"resourceGroup=${AZURE_RESOURCE_GROUP},storageAccount=${AZURE_STORAGE_ACCOUNT},storageAccountKeyEnvVar=AZURE_STORAGE_ACCOUNT_KEY\" \\\n    --credential=velero-backup=cloud\n</code></pre> <p>Check that the backup-location becomes available:</p> <pre><code>$ velero backup-location get\nNAME     PROVIDER   BUCKET/PREFIX       PHASE       LAST VALIDATED   ACCESS MODE   DEFAULT\nbackup   aws        &lt;bucket&gt;/&lt;prefix&gt;   Available   &lt;timestamp&gt;      ReadOnly\n</code></pre> <p>Then check that the backups becomes available using:</p> <pre><code>./bin/ck8s ops velero \"${CLUSTER}\" backup get\n</code></pre> <p>When they are available restore one of them using:</p> <pre><code>./bin/ck8s ops velero \"${CLUSTER}\" restore create &lt;name-of-restore&gt; --from-backup &lt;name-of-backup&gt;\n</code></pre> <p>After the restore is complete Velero should be reconfigured to use the main object storage again, with a new bucket if the previous one is unusable.   Updating or syncing the Helm chart:</p> <pre><code>./bin/ck8s ops helmfile \"${CLUSTER}\" -f helmfile -l app=velero -i apply\n</code></pre> <p>The secret and the backup metadata from the off-site backups can be deleted:</p> <pre><code>./bin/ck8s ops kubectl \"${CLUSTER}\" -n velero delete secret velero-backup\n./bin/ck8s ops kubectl \"${CLUSTER}\" -n velero delete backups.velero.io --all\n./bin/ck8s ops kubectl \"${CLUSTER}\" -n velero delete backupstoragelocations.velero.io backup\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#changing-pvpvc-storage-classes","title":"Changing PV/PVC Storage Classes","text":"<p>Velero can change the storage class of persistent volumes and persistent volume claims during restores. To configure a storage class mapping, create a ConfigMap in the Velero namespace:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  # any name can be used; Velero uses the labels (below)\n  # to identify it rather than the name\n  name: change-storage-class-config\n  # must be in the velero namespace\n  namespace: velero\n  # the below labels should be used verbatim in your\n  # ConfigMap.\n  labels:\n    # this value-less label identifies the ConfigMap as\n    # config for a plugin (i.e. the built-in restore item action plugin)\n    velero.io/plugin-config: \"\"\n    # this label identifies the name and kind of plugin\n    # that this ConfigMap is for.\n    velero.io/change-storage-class: RestoreItemAction\ndata:\n  # add 1+ key-value pairs here, where the key is the old\n  # storage class name and the value is the new storage\n  # class name.\n  &lt;old-storage-class&gt;: &lt;new-storage-class&gt;\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#grafana","title":"Grafana","text":"<p>This section refers to the Management Cluster and specifically to the user Grafana, not the ops Grafana.</p>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#backup_3","title":"Backup","text":"<p>Backups of Grafana dashboards created by Application Developers are included in the daily Velero backup in the Management Cluster. We then include the Grafana Deployment, Pod, and PVC (including the data). Manual backups can be taken using Velero (include the same resources).</p> <p>To manually create a backup run:</p> <pre><code>./bin/ck8s ops velero sc backup create --from-schedule velero-daily-backup --wait\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/disaster-recovery/#restore_3","title":"Restore","text":"<p>To restore the Grafana backup you must:</p> <ul> <li>Have Grafana installed</li> <li>Delete the Grafana Deployment, PVC and PV</li> </ul> <pre><code>./bin/ck8s ops kubectl sc delete deploy -n monitoring user-grafana\n./bin/ck8s ops kubectl sc delete pvc -n monitoring user-grafana\n</code></pre> <ul> <li>Restore the Velero backup</li> </ul> <pre><code>./bin/ck8s ops velero sc restore create --from-schedule velero-daily-backup --wait\n</code></pre>","tags":["HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","MSBFS 2020:7 4 kap. 22 \u00a7","HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","NIST SP 800-171 3.6.3","NIS2 Minimum Requirement (c) Disaster Recovery","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","ISO 27001 Annex A 8.13 Information Backup"]},{"location":"operator-manual/faq/","title":"Platform Administrator FAQ","text":""},{"location":"operator-manual/faq/#i-updated-some-opensearch-options-but-it-didnt-work-now-what","title":"I updated some OpenSearch options but it didn't work, now what?","text":"<p>If you update the OpenSearch <code>securityConfig</code> you will have to make sure that the master Pod(s) are restarted so that they pick up the new Secret and then run the <code>securityadmin.sh</code> script. This happens for example if you switch from non-SSO to SSO.</p> <p>To reload the configuration you need to run the following commands:</p> <pre><code># Make the script executable\nkubectl -n opensearch-system exec opensearch-master-0 -- chmod +x ./plugins/opensearch-security/tools/securityadmin.sh\n# Run the script to update the configuration\nkubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\\n    -f plugins/opensearch-security/securityconfig/config.yml \\\n    -icl -nhnv \\\n    -cacert config/admin/ca.crt \\\n    -cert config/admin/tls.crt \\\n    -key config/admin/tls.key\n</code></pre> <p>Note that the above only reloads the <code>config.yml</code> (as specified with the <code>-f</code>). If you made changes to other parts of the system you will need to point to the relevant file to reload, or reload everything like this:</p> <pre><code># Run the script to update \"everything\" (internal users, roles, configuration, etc.)\nkubectl -n opensearch-system exec opensearch-master-0 -- ./plugins/opensearch-security/tools/securityadmin.sh \\\n    -cd plugins/opensearch-security/securityconfig/ \\\n    -icl -nhnv \\\n    -cacert config/admin/ca.crt \\\n    -cert config/admin/tls.crt \\\n    -key config/admin/tls.key\n</code></pre> <p>When you update things other than <code>config.yml</code> you will also need to rerun the Configurer Job by syncing the <code>opensearch-configurer</code> chart.</p>"},{"location":"operator-manual/getting-started/","title":"Getting Started","text":"<p>Setting up Welkin consists of two parts: setting up at least two vanilla Kubernetes Clusters and deploying <code>compliantkubernetes-apps</code> on top of them.</p>"},{"location":"operator-manual/getting-started/#pre-requisites-for-creating-vanilla-kubernetes-clusters","title":"Pre-requisites for Creating Vanilla Kubernetes Clusters","text":"<p>In theory, any vanilla Kubernetes Cluster can be used for Welkin. We suggest the Kubespray way. To this end, you need:</p> <ul> <li>Git</li> <li>Python3 pip</li> <li>Terraform</li> <li>Ansible</li> <li><code>pwgen</code></li> </ul> <p>Ansible is best installed as follows:</p> <pre><code>git clone --recursive https://github.com/elastisys/compliantkubernetes-kubespray\ncd compliantkubernetes-kubespray\npip3 install -r kubespray/requirements.txt\n</code></pre> <p>Optional: For debugging, you may want CLI tools to interact with your chosen Infrastructure Provider:</p> <ul> <li>AWS CLI</li> <li>Exoscale CLI</li> <li>OpenStack Client</li> <li>VMware vSphere CLI (govmomi)</li> </ul>"},{"location":"operator-manual/getting-started/#pre-requisites-for-welkin-apps","title":"Pre-requisites for Welkin Apps","text":"<p>Install pre-requisites for Welkin Apps:</p> <pre><code>git clone https://github.com/elastisys/compliantkubernetes-apps\ncd compliantkubernetes-apps\n./bin/ck8s install-requirements\n</code></pre>"},{"location":"operator-manual/getting-started/#misc","title":"Misc","text":"<p>Welkin relies on SSH for accessing Nodes. If you haven't already done so, generate an SSH key as follows:</p> <pre><code>ssh-keygen\n</code></pre> <p>Configuration secrets in Welkin are encrypted using SOPS. We currently only support using PGP when encrypting secrets. If you haven't already done so, generate your own PGP key as follows:</p> <pre><code>gpg --full-generate-key\n</code></pre>"},{"location":"operator-manual/infrastructure-requirements/","title":"Welkin Infrastructure Requirements","text":"<p>Note</p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p> <p>This page lists infrastructure requirements to run Welkin in production. For setting up a development environment, see this guide instead.</p> <p>You should always perform a provider audit to get a stable and secure Welkin environment. This page serve primarily as an initial assessment -- to determine whether it's even worth getting started. This ensures that the fundamental conditions are met before investing further effort.</p>"},{"location":"operator-manual/infrastructure-requirements/#how-much-infrastructure-capacity-does-welkin-need","title":"How much infrastructure capacity does Welkin need?","text":"<p>The required capacity depends on the performance of your infrastructure. This, in turn, is strongly influenced by the CPU model, memory bandwidth, network bandwidth, disk bandwidth, etc. that you invested in.</p> <p>We found the following to be a good starting point:</p> <ul> <li>Management Cluster:<ul> <li>3x 2 CPUs, 4 GB RAM, 60 GB local disk for control plane Nodes</li> <li>5x 2 CPUs, 8 GB RAM, 80 GB local disk for worker Nodes</li> </ul> </li> <li>Workload Cluster:<ul> <li>3x 2 CPUs, 4 GB RAM, 60 GB local disk for control plane Nodes</li> <li>2x 2 CPUs, 6 GB RAM, 60 GB local disk for Welkin worker Nodes</li> <li>Add additional capacity as required by your application and Additional Managed Services.</li> </ul> </li> </ul> <p>Before going live, you should adjust capacity as needed to preserve stability and security. See the Go-Live Checklist.</p>"},{"location":"operator-manual/infrastructure-requirements/#what-infrastructure-capabilities-do-i-need","title":"What infrastructure capabilities do I need?","text":"<p>This depends a lot on your goals. For example:</p> <ul> <li>If you want to tolerate one data center failure, then you MUST have three data centers, due to Kubernetes quorum requirements (further reading).</li> <li>If you want to tolerate one Node failure, then you MUST have a load-balancer with health-checks which can route traffic away from the failed Node.</li> </ul> <p>Ideally, your infrastructure SHOULD provide:</p> <ul> <li>VMs or physical servers running Ubuntu 24.04 LTS</li> <li>Firewall to protect the VMs</li> <li>Two private networks, one for the Management Cluster and one for the Workload Cluster</li> <li>Two load-balancers</li> <li>CSI-compatible block storage</li> <li>S3-compatible object storage</li> <li>An OpenTofu or Terraform Provider to configure all above via an API</li> </ul> <p>Furthermore, your infrastructure SHOULD not block:</p> <ul> <li>Internal traffic between the VMs</li> <li>Access to the Swedish NTP servers (outgoing packets on UDP port 123).</li> </ul> <p>Note that the Swedish NTP servers are mandated by MSBFS 2020:7 \"Myndigheten f\u00f6r samh\u00e4llsskydd och beredskaps f\u00f6reskrifter om s\u00e4kerhets\u00e5tg\u00e4rder i informationssystem f\u00f6r statliga myndigheter\" 3 kap \u00a7 13.</p> <ul> <li>Access to Let's Encrypt (outgoing and incoming packets on TCP port 80)</li> </ul> <p>For initial setup, platform administrators need SSH access to the VMs. This can be done in a few ways:</p> <ul> <li>Expose the VMs on the public Internet and allowlist the platform administrators' VPNs.</li> <li>Expose a Bastion Host running Ubuntu on the public Internet and allowlist the platform administrators' VPNs.</li> <li>Provide the platform administrators with a Linux-compatible VPN client to access the VMs via SSH.</li> </ul>"},{"location":"operator-manual/infrastructure-requirements/#what-if-i-lack","title":"What if I lack ...?","text":"<ul> <li>... a policy which allows SSH access to the VMs:    Platform administrators needs to be empowered to use the tools which make them most efficient.    Doing initial setup via \"desktop share\", such as VNC, Remote Desktop or Citrix DaaS, is tedious and error-prone.    Let us help you by reading your Exception Policy, fill out your Exception Request Form and provide you with detailed Compensating Controls to mitigate risk associated with the exception.</li> <li>... an API to configure my infrastructure:    Platform administrators will communicate with your infrastructure team via email or service tickets.    In fact, we recommend that initial setup includes a detailed architecture diagram of the infrastructure, both to reduce misunderstandings, but also to serve as documentation, as required by ISO 27001 Annex A 5.37 Documented Operating Procedures.</li> <li>... access to the Swedish NTP servers:     Welkin can be configured with alternative NTP servers.     This will ensure you conform with ISO 27001 Annex A 8.17 Clock Synchronization.</li> <li>... access to LetsEncrypt:     This page discusses alternative strategy for certificate provisioning.     Note that this may reduce automation, hence is more human time intensive and error prone.</li> <li>... an S3-compatible object storage:     Consider using a public cloud-based fully-managed S3-compatible object storage.</li> <li>... CSI-compatible block storage:     If you have Rook/Ceph experts, then you can set up a Rook/Ceph Cluster.     Note that this will increase infrastructure footprint and the skills needed for  troubleshooting from your team.     Always prefer using your existing block storage solution, such as your existing NFS server.</li> <li>... load-balancers:     If Welkin may speak BGP with your routers, then you may use kube-vip -- currently not part of Welkin.     If not, then this will make it significantly harder to tolerate Node failures. We could potentially compensate by using DNS with health-checks, as provided by AWS Route53, however fail-over times become a lot larger, e.g., up to 5 minutes. If DNS failover is unsuitable for you, then we recommend a manual failover process.</li> <li>... private networks:     You can use public IP addresses for the VMs and protect them using your firewall.</li> <li>... three data centers:     To tolerate one data center failure, you need three data centres.     This requirement is due to Kubernetes needing a quorum.     Some Additional Managed Services also need quorum for tolerating failure.     You also need to have a load-balancer which is \"stretched\" across all data centers.     This means that if one data center goes down, your load balancer needs to be able to direct traffic to the healthy data centers.     This is usually achieved via BGP Anycast.</li> <li>... Internet access:     See air-gapped.</li> </ul>"},{"location":"operator-manual/ingress/","title":"Ingress","text":"<p>Welkin uses the Ingress NGINX Controller to route external traffic to the correct Service inside the Cluster. Welkin can configure the Ingress Controller in two different ways depending on the underlying infrastructure.</p>"},{"location":"operator-manual/ingress/#using-a-service-of-type-loadbalancer","title":"Using a Service of type LoadBalancer","text":"<p>When using a Infrastructure Provider with a Kubernetes cloud integration such as AWS, Azure and Google cloud the Ingress controller can be exposed with a Service of type LoadBalancer. This will create an external load balancer in the cloud provider with an external ip-address. Any DNS records should be pointed to the IP address of the load balancer.</p> <p>Note</p> <p>This is only currently supported in Welkin for AWS. It is however possible to configure this for Azure and Google cloud as well but it's not done by default</p>"},{"location":"operator-manual/ingress/#using-the-host-network","title":"Using the host network","text":"<p>For any Infrastructure Provider (or bare metal) not supporting these kind of public load balancers the Ingress Controller uses the host network instead. This is done by configuring the Ingress Controller as a DaemonSet so one Pod is created on each Node. The Pods are configured to use the host network, so all traffic received on the Node on port 80 and 443 will be intercepted by the Ingress Controller Pod and then routed to the desired Service.</p> <p>On some Infrastructure Providers there is load balancing available for the worker Nodes. For example Exoscale uses an \"elastic IP\" which provides one external IP which load balances to the available worker Nodes. For these Infrastructure Providers this external IP of the load balancers should be used as the entry point in the DNS.</p> <p>For the Infrastructure Providers where this is not available the easiest option is to just point the DNS to the IP of any, or all, of the worker Nodes. This is of course not a optimal solution because it adds a single point of failure on the worker Node which is selected by the DNS. Another option is to use any existing load balancer service if this is available.</p>"},{"location":"operator-manual/ingress/#installation","title":"Installation","text":"<p>The Ingress NGINX Controller is currently configured and installed by the compliantkubernetes-apps repository. The configuration is set in sc-config.yaml and wc-config.yaml under:</p> <pre><code>ingressNginx:\n  useHostPort: \"\"\n  service:\n    enabled: \"\"\n    type: \"\"\n</code></pre> <p>If the apps repository is initiated with the correct Infrastructure Provider these configuration options will get the correct defaults.</p> <p>For more ways to install the Ingress NGINX Controller see the upstream documentation.</p>"},{"location":"operator-manual/ingress/#ingress-resource","title":"Ingress resource","text":"<p>The Ingress resource is used to later route traffic to the desired Service. For more information about this see the official documentation.</p>"},{"location":"operator-manual/maintenance/","title":"Maintaining and Upgrading your Welkin environment","text":"<p>In order to keep your Welkin environment running smoothly, and to assure that you are up to date with the latest patches you need to perform regular maintenance on it.</p> <p>This guide assumes that:</p> <ul> <li>Your Welkin environment is running normally, if not, please see the Troubleshooting guide.</li> <li>Your Welkin environment is properly sized</li> <li>You have performed the actions in the Go-live Checklist as failure to do so might cause downtime during maintenance.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#compliance-needs","title":"Compliance needs","text":"<p>Many regulations require you to secure your information system against unauthorized access, data loss, and breaches. An important part of this is keeping your Welkin environment up to date with the latest security patches to not run outdated versions of components that are no longer supported. This maps to objectives in ISO Annex A.12.6.1 Management of Technical Vulnerabilities.</p>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#what-maintenance-do-i-need-to-do-and-how","title":"What maintenance do I need to do and how?","text":"<p>In short, there are three levels of maintenance that should be performed on a regular basis.</p> <ul> <li>Patching the underlying OS on the Nodes</li> <li>Upgrading the Welkin application stack</li> <li>Upgrading Welkin Core (Kubespray or Cluster-API)</li> </ul> <p>Note</p> <p>These docs only include maintenance steps for Kubespray and not Cluster-API.</p> <p>Let's go through them one by one.</p> <p>Important</p> <p>Welkin advises against rollbacks and instead argues for performing staged rollouts and, in the case that a defective patch made it all the way to production, perform a rollforward instead. Performing a rollback for distributed and stateful applications can be a very complex process, is prone to errors and can lead to data- loss, inconsistencies, corruption etc. To avoid such issues, we recommend the following:</p> <ul> <li>It is recommended to have separate staging and production environments, so that each new release is first tested in the staging environment before upgrading production.</li> <li>If issues are encountered in production, it is recommended to address and patch those issues instead of doing a rollback.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#patching-the-nodes","title":"Patching the Nodes","text":"<p>Security patches for the underlying OS on the Nodes are constantly being released, and to ensure your environment is secured, the Nodes that run Welkin must be updated with these patches. We recommend that you use the AutomaticSecurityUpdates feature that is available in Ubuntu (similar feature exist in other Linux distributions) to install these updates. Note that the Nodes still need to be rebooted for some of these updates to be applied. In order to reboot the Nodes, you can either use a tool like kured or you can do it manually by logging on to the Nodes and rebooting them manually. When doing that, reboot one Node at the time and make sure that the rebooted Node is 'Ready' and that Pods are scheduled to it before you move on to the next, or you risk downtime.</p> <p>There is a playbook in the compliantkubernetes-kubespray repository that can assist with the reboot of Nodes. It will cordon and reboot the Nodes one by one.</p> <pre><code>./bin/ck8s-kubespray reboot-nodes &lt;wc|sc&gt; [--extra-vars manual_prompt=true] [&lt;options&gt;]\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#upgrading-the-welkin-application-stack","title":"Upgrading the Welkin application stack","text":"<p>Welkin consists of a multitude of open source components that interact to form a smooth End User experience. In order to free you of the burden of keeping track of when to upgrade the various components, new versions of Welkin are regularly released. When a new version is released, it becomes available as a tagged release in the GitHub repository.</p> <p>Note</p> <p>Before upgrading to a new release, please review the changelog and if possible, apply the upgrade to a staging environment before upgrading any environments with production data.</p>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#prerequisites","title":"Prerequisites","text":"<ul> <li> Notify the users (if any) before the upgrade starts;</li> <li> Check if there are any pending changes to the environment;</li> <li> Check the state of the environment, Pods, Nodes and backup jobs:</li> </ul> <p>Note</p> <p>the below steps should be run from compliantkubernetes-apps root directory.</p> <pre><code>./bin/ck8s test sc|wc\n./bin/ck8s ops kubectl sc|wc get pods -A -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,READY-false:status.containerStatuses[*].ready,REASON:status.containerStatuses[*].state.terminated.reason | grep false | grep -v Completed\n./bin/ck8s ops kubectl sc|wc get nodes\n./bin/ck8s ops kubectl sc|wc get jobs -A\n./bin/ck8s ops velero sc|wc get backup\n</code></pre> <ul> <li> Silence the notifications for the alerts. e.g you can use Alertmanager silences;</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#upgrading-compliantkubernetes-apps","title":"Upgrading compliantkubernetes-apps","text":"<p>For security, compliance, and support reasons, environments should stay up to date with the latest version of compliankubernetes-apps.</p> <p>Note what version of compliantkubernetes-apps that is currently used and the version that you want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. You should never upgrade more than one minor version of compliantkubernetes-apps at a time.</p> <p>Check and follow the migration document for the release you want to upgrade to: https://github.com/elastisys/compliantkubernetes-apps/tree/main/migration</p>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#upgrading-kubespraykubernetes","title":"Upgrading Kubespray/Kubernetes","text":"<p>All Clusters should stay up to date with the latest Kubespray version used in compliantkubernetes-kubespray.</p> <p>Note what version of Kubespray that is currently used in the Cluster and the Kubespray version you want to upgrade to. Then check the release notes for each version in between to see if there are anything that might cause any problems, if so then consult the rest of the operations team before proceeding. Also check if the newer Kubespray version would upgrade Kubernetes to a new minor version, if so then Application Developers should get a notice of x weeks before proceeding to let them check for any deprecated APIs that they might be using. You should never upgrade more than one minor version of compliantkubernetes-kubespray at a time. Read more about Kubespray upgrades in their documentation.</p> <p>Check and follow the migration document for the release you want to upgrade to: https://github.com/elastisys/compliantkubernetes-kubespray/tree/main/migration</p>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/maintenance/#after-doing-any-upgrades-or-maintenance","title":"After doing any upgrades or maintenance","text":"<ul> <li> Check the state of the environment, Pods and Nodes:</li> </ul> <p>Note</p> <p>the below steps should be run from compliantkubernetes-apps root directory.</p> <pre><code>./bin/ck8s test sc|wc\n./bin/ck8s ops kubectl sc|wc get pods -A -o custom-columns=NAMESPACE:metadata.namespace,POD:metadata.name,READY-false:status.containerStatuses[*].ready,REASON:status.containerStatuses[*].state.terminated.reason | grep false | grep -v Completed\n./bin/ck8s ops kubectl sc|wc get nodes\n</code></pre> <ul> <li> Check if any alerts generated by the upgrade didn't close;</li> <li> Check if you can login to Grafana, OpenSearch or Harbor;</li> <li> Enable the notifications for the alerts;</li> <li> Notify the users (if any) when the upgrade is complete;</li> <li> Check that you can see fresh metrics and logs.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A21","HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","MSBFS 2020:7 4 kap. 12 \u00a7","NIST SP 800-171 3.4.5","NIST SP 800-171 3.7.1"]},{"location":"operator-manual/on-prem-standard/","title":"Standard Template for on-prem Environment","text":"<p>This document contains instructions on how to set-up a new Welkin on-prem environment.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#prerequisites","title":"Prerequisites","text":"<p>Important</p> <p>Decisions regarding the following items should be made before venturing on deploying Welkin.</p> <ul> <li>Overall architecture, i.e., VM sizes, load-balancer configuration, storage configuration, etc.</li> <li>Identity Provider (IdP) choice and configuration. See this page.</li> <li>On-call Management Tool (OMT) choice and configuration.</li> </ul> <ol> <li> <p>Make sure you install all prerequisites on your computer.</p> </li> <li> <p>Prepare Ubuntu-based VMs:     If you are using public clouds, you can create VMs using the scripts included in Kubespray:</p> <ul> <li>For Azure, use AzureRM scripts.</li> <li>For other clouds, use their respective Terraform scripts.</li> </ul> </li> <li> <p>Create a git working folder to store Welkin configurations in a version-controlled manner. Run the following commands from the root of the configuration repository.</p> <pre><code>export CK8S_CONFIG_PATH=~/.ck8s/my-cluster-path\nexport CK8S_CLOUD_PROVIDER=# run 'compliantkubernetes-apps/bin/ck8s providers' to list available providers\nexport CK8S_ENVIRONMENT_NAME=my-environment-name\nexport CK8S_FLAVOR=# run 'compliantkubernetes-apps/bin/ck8s flavors' to list available flavors\nexport CK8S_K8S_INSTALLER=# run 'compliantkubernetes-apps/bin/ck8s k8s-installers' to list available k8s-installers\nexport CK8S_PGP_FP=&lt;your GPG key fingerprint&gt;  # retrieve with gpg --list-secret-keys\n\nexport CLUSTERS=( \"sc\" \"wc\" )\nexport DOMAIN=example.com # your domain\n</code></pre> </li> <li> <p>Add the Welkin Kubespray repository as a <code>git submodule</code> to the configuration repository and install pre-requisites as follows:</p> <p>Note</p> <p>Remember to switch to the desired version of <code>compliantkubernetes-kubespray</code>.</p> <pre><code>git submodule add https://github.com/elastisys/compliantkubernetes-kubespray.git\ngit submodule update --init --recursive\ncd compliantkubernetes-kubespray\ngit switch -d $(git tag --sort=committerdate | tail -1) # this will switch to the latest release tag\npip3 install -r kubespray/requirements.txt  # this will install Ansible\nansible-playbook -e 'ansible_python_interpreter=/usr/bin/python3' --ask-become-pass --connection local --inventory 127.0.0.1, get-requirements.yaml\n</code></pre> </li> <li> <p>Add the Welkin Apps repository as a <code>git submodule</code> to the configuration repository and install pre-requisites as follows:</p> <p>Note</p> <p>Remember to switch to the desired version of <code>compliantkubernetes-apps</code>.</p> <pre><code>git submodule add https://github.com/elastisys/compliantkubernetes-apps.git\ncd compliantkubernetes-apps\ngit switch -d $(git tag --sort=committerdate | tail -1) # this will switch to the latest release tag\n./bin/ck8s install-requirements\n</code></pre> </li> <li> <p>Create the domain name.     You need to create a domain name to access the different services in your environment. You will need to set up the following DNS entries.</p> <ul> <li>Point these domains to the Workload Cluster Ingress Controller (this step is done during Welkin Apps installation):</li> <li><code>*.$DOMAIN</code></li> <li>Point these domains to the Management Cluster Ingress Controller (this step is done during Welkin Apps installation):</li> <li><code>*.ops.$DOMAIN</code></li> <li><code>dex.$DOMAIN</code></li> <li><code>grafana.$DOMAIN</code></li> <li><code>harbor.$DOMAIN</code></li> <li><code>opensearch.$DOMAIN</code></li> </ul> If both Management and Workload Clusters are in the same subnet <p>If both the Management and Workload Clusters are in the same subnet, it would be great to configure the following domain names to the private IP addresses of Management Cluster's worker nodes.</p> <ul> <li><code>*.thanos.ops.$DOMAIN</code></li> <li><code>*.opensearch.ops.$DOMAIN</code></li> </ul> </li> <li> <p>Create S3 credentials and add them to <code>.state/s3cfg.ini</code>.</p> </li> <li> <p>Set up load balancer</p> <p>You need to set up two load balancers, one for the Workload Cluster and one for the Management Cluster.</p> </li> <li> <p>Make sure you have all necessary tools.</p> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#deploying-welkin-using-kubespray","title":"Deploying Welkin using Kubespray","text":"How to change Default Kubernetes Subnet Address <p>If  the default IP block ranges used for Docker and Kubernetes are the same as the internal IP ranges used in the company, you can change the values  to resolve the conflict as follows. Note that you can use any valid private IP address range, the values below are put as an example.</p> For KubernetesFor Docker <pre><code>* For Management Cluster: Add `kube_service_addresses: 10.178.0.0/18` and `kube_pods_subnet: 10.178.120.0/18` in `${CK8S_CONFIG_PATH}/sc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml` file.\n* For Workload Cluster:  Add `kube_service_addresses: 10.178.0.0/18` and `kube_pods_subnet: 10.178.120.0/18` in `${CK8S_CONFIG_PATH}/wc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml` file.\n</code></pre> <pre><code>* For Management Cluster: Add `docker_options: \"--default-address-pool base=10.179.0.0/24,size=24\"` in `${CK8S_CONFIG_PATH}/sc-config/group_vars/all/docker.yml` file.\n* For Workload Cluster:  Add `docker_options: \"--default-address-pool base=10.179.4.0/24,size=24\"` in `${CK8S_CONFIG_PATH}/wc-config/group_vars/all/docker.yml` file.\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#init-kubespray-configuration-in-your-configuration-path","title":"Init Kubespray configuration in your configuration path","text":"<pre><code>for CLUSTER in ${CLUSTERS[@]}\"; do\n    compliantkubernetes-kubespray/ck8s-kubespray init $CLUSTER $CK8S_CLOUD_PROVIDER $CK8S_PGP_FP\ndone\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#configure-oidc","title":"Configure OIDC","text":"<p>To configure OpenID access for Kubernetes API and other services, Dex should be configured with your identity provider (IdP). Check what Dex needs from your identity provider.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#configure-oidc-endpoint","title":"Configure OIDC endpoint","text":"<p>The Management Cluster is recommended to be configured with an external OIDC endpoint provided by the IdP of your choice. This can be configured in <code>${CK8S_CONFIG_PATH}/sc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml</code> by setting the following variables:</p> <ul> <li><code>kube_oidc_auth</code> should be set to true, this enables OIDC authentication for the api-server</li> <li><code>kube_oidc_url</code> should be set to an OIDC endpoint from your IdP (e.g. for Google this would be <code>https://accounts.google.com</code>)</li> <li><code>kube_oidc_client_id</code> should be retrieved from your IdP</li> <li><code>kube_oidc_client_secret</code> should be retrieved from your IdP</li> </ul> <p>To configure the Workload Cluster to use Dex running in the Management Cluster for authentication you will also need to configure the following in <code>${CK8S_CONFIG_PATH}/wc-config/group_vars/k8s_cluster/ck8s-k8s-cluster.yaml</code>:</p> <ul> <li><code>kube_oidc_auth</code> should be set to true, this enables OIDC authentication for the api-server</li> <li><code>kube_oidc_url</code> should be set to <code>https://dex.$DOMAIN</code></li> <li><code>kube_oidc_client_id</code> should be set to <code>kubelogin</code></li> <li><code>kube_oidc_client_secret</code> should be set to a Dex client secret generated with the apps configuration, it can be found in <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> under the key <code>dex.kubeloginClientSecret</code> after running <code>ck8s init</code> (see instructions on deploying apps).</li> </ul> <p>To generate kubeconfigs that use OIDC for authentication, the following variables should be set in the configuration files for both Clusters (both can't be true):</p> <pre><code>create_oidc_kubeconfig: true\nkubeconfig_localhost: false\n</code></pre> <p>For more information on managing OIDC kubeconfigs and RBAC, or on running without OIDC, see the Welkin Kubespray documentation.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#copy-the-vms-information-to-the-inventory-files","title":"Copy the VMs information to the inventory files","text":"<p>Add the host name, user and IP address of each VM that you prepared above in <code>${CK8S_CONFIG_PATH}/sc-config/inventory.ini</code> for Management Cluster and <code>${CK8S_CONFIG_PATH}/wc-config/inventory.ini</code> for Workload Cluster. Moreover, you also need to add the host names of the master Nodes under <code>[kube_control_plane]</code>, etcd Nodes under <code>[etcd]</code> and worker Nodes under <code>[kube_node]</code>.</p> <p>Note</p> <p>Make sure that the user has SSH access to the VMs.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#run-kubespray-to-deploy-the-kubernetes-clusters","title":"Run Kubespray to deploy the Kubernetes Clusters","text":"<pre><code>for CLUSTER in \"${CLUSTERS[@]}\"; do\n    compliantkubernetes-kubespray/bin/ck8s-kubespray apply $CLUSTER --flush-cache\ndone\n</code></pre> <p>Note</p> <p>The kubeconfig for the Workload Cluster (<code>.state/kube_config_wc.yaml</code>) will not be usable until you have installed Dex in the Management Cluster (by deploying apps).</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#rook-block-storage","title":"Rook Block Storage","text":"<p>Normally, we want to use block storage solutions provided by the infra provider. However, this is not always available, especially for on-prem environments. In such cases we can partition separate volumes on Nodes in the Cluster for Rook-Ceph and use that as a block storage solution.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#deploy-rook","title":"Deploy Rook","text":"<p>To deploy Rook, go to the <code>welkin-rook</code> repository and follow the instructions here for each Cluster.</p> <p>Note</p> <p>If the kubeconfig files for the Clusters are encrypted with SOPS, you need to decrypt them before using them:</p> <pre><code>sops --decrypt ${CK8S_CONFIG_PATH}/.state/kube_config_$CLUSTER.yaml &gt; $CLUSTER.yaml\nexport KUBECONFIG=$CLUSTER.yaml\n</code></pre> <p>Please restart the operator Pod, <code>rook-ceph-operator*</code>, if some Pods stalls in initialization state as shown below:</p> <pre><code>rook-ceph     rook-ceph-crashcollector-minion-0-b75b9fc64-tv2vg    0/1     Init:0/2   0          24m\nrook-ceph     rook-ceph-crashcollector-minion-1-5cfb88b66f-mggrh   0/1     Init:0/2   0          36m\nrook-ceph     rook-ceph-crashcollector-minion-2-5c74ffffb6-jwk55   0/1     Init:0/2   0          14m\n</code></pre> <p>Warning</p> <p>Pods in pending state usually indicate resource shortage. In such cases you need to use bigger instances.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#test-rook","title":"Test Rook","text":"<p>Note</p> <p>If the Workload Cluster kubeconfig is configured with authentication to Dex running in the Management Cluster, part of apps needs to be deployed before it is possible to run the commands below for <code>wc</code>.</p> <p>To test Rook, proceed as follows:</p> <pre><code>for CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default apply -f https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/csi/rbd/pvc.yaml\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default apply -f https://raw.githubusercontent.com/rook/rook/v1.11.9/deploy/examples/csi/rbd/pod.yaml\ndone\n\nfor CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default get pvc rbd-pvc\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default get pod csirbd-demo-pod\ndone\n</code></pre> <p>You should see PVCs in Bound state, and that the Pods which mounts the volumes are running.</p> <p>Important</p> <p>If you have taints on certain Nodes which should support running Pods that mounts <code>rook-ceph</code> PVCs, you need to ensure these Nodes are tolerated by the rook-ceph DaemonSet <code>csi-rbdplugin</code>, otherwise, Pods on these Nodes will not be able to attach or mount the volumes.</p> <p>If you want to clean the previously created PVCs:</p> <pre><code>for CLUSTER in sc wc; do\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default delete pvc rbd-pvc\n    kubectl --kubeconfig ${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml -n default delete pod csirbd-demo-pod\ndone\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#deploying-welkin-apps","title":"Deploying Welkin Apps","text":"How to change local DNS IP if you change the default Kubernetes subnet address <p>You need to change the default coreDNS default IP address in <code>common-config.yaml</code> file if  you change the default IP block  used for Kubernetes services above.  To get the coreDNS IP address, run the following commands.</p> <p><pre><code>${CK8S_CONFIG_PATH}/compliantkubernetes-apps/bin/ck8s ops kubectl sc get svc -n kube-system coredns\n</code></pre> Once you get the IP address edit <code>${CK8S_CONFIG_PATH}/common-config.yaml</code> file  and set  the value  to <code>global.clusterDns</code> field.</p> Configure the load balancer IP on the loopback interface for each worker Node <p>The Kubernetes data plane Nodes (i.e., worker Nodes) cannot connect to themselves with the IP address of the load balancer that fronts them. The easiest is to configure the load balancer's IP address on the loopback interface of each Nodes. Create <code>/etc/netplan/20-eip-fix.yaml</code> file and add the following to it. <code>${loadblancer_ip_address}</code> should be replaced with the IP address of the load balancer for each cluster.</p> <p><pre><code>network:\n  version: 2\n  ethernets:\n    lo0:\n      match:\n        name: lo\n      dhcp4: false\n      addresses:\n      - ${loadblancer_ip_address}/32\n</code></pre> After adding the above content, run the following command in each worker Node:</p> <pre><code>sudo netplan apply\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#initialize-the-apps-configuration","title":"Initialize the apps configuration","text":"<pre><code>compliantkubernetes-apps/bin/ck8s init both\n</code></pre> <p>This will initialise the configuration in the <code>${CK8S_CONFIG_PATH}</code> directory. Generating configuration files <code>sc-config.yaml</code> and <code>wc-config.yaml</code>, as well as secrets with randomly generated passwords in <code>secrets.yaml</code>. This will also generate read-only default configuration under the directory <code>defaults/</code> which can be used as a guide for available and suggested options.</p> <pre><code>ls -l $CK8S_CONFIG_PATH\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#configure-the-apps","title":"Configure the apps","text":"<p>Edit the configuration files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code>, <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/secrets.yaml</code> and set the appropriate values for some of the configuration fields. Note that, the latter is encrypted.</p> <pre><code>vim ${CK8S_CONFIG_PATH}/sc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/wc-config.yaml\n\nvim ${CK8S_CONFIG_PATH}/common-config.yaml\n</code></pre> <p>Edit the secrets.yaml file and add the credentials for:</p> <ul> <li>S3 - used for backup storage.</li> <li>Dex - connectors -- check your identity provider.</li> <li>On-call management tool configurations-- Check supported on-call management tools.</li> </ul> <pre><code>sops ${CK8S_CONFIG_PATH}/secrets.yaml\n</code></pre> <p>The default configuration for the Management Cluster and Workload Cluster are available in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code> and can be used as a reference for available options.</p> <p>Warning</p> <p>Do not modify the read-only default configurations files found in the directory <code>${CK8S_CONFIG_PATH}/defaults/</code>. Instead configure the Cluster by modifying the regular files <code>${CK8S_CONFIG_PATH}/sc-config.yaml</code> and <code>${CK8S_CONFIG_PATH}/wc-config.yaml</code> as they will override the default options.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#create-s3-buckets","title":"Create S3 buckets","text":"<p>You can use the following script to create required S3 buckets. The script uses <code>s3cmd</code> in the background and gets configuration and credentials for your S3 provider from <code>${HOME}/.s3cfg</code> file.</p> <pre><code># Use your default s3cmd config file: ${HOME}/.s3cfg\nscripts/S3/entry.sh create\n</code></pre> <p>Warning</p> <p>You should not use your own credentials for S3. Rather create a new set of credentials with write-only access, when supported by the object storage provider.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#install-welkin-apps","title":"Install Welkin Apps","text":"<p>Start with the Management Cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s apply sc\n</code></pre> <p>Then the Workload Cluster:</p> <pre><code>compliantkubernetes-apps/bin/ck8s apply wc\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#settling","title":"Settling","text":"<p>Important</p> <p>Leave sufficient time for the system to settle, e.g., request TLS certificates from Let's Encrypt, perhaps as much as 20 minutes.</p> <p>Check if all Helm charts succeeded.</p> <pre><code>compliantkubernetes-apps/bin/ck8s ops helm wc list -A --all\n</code></pre> <p>You can check if the system settled as follows:</p> <pre><code>for CLUSTER in sc wc; do\n    compliantkubernetes-apps/bin/ck8s ops kubectl ${CLUSTER} get --all-namespaces pods\ndone\n</code></pre> <p>Check the output of the command above. All Pods needs to be Running or Completed.</p> <pre><code>for CLUSTER in sc wc; do\n    compliantkubernetes-apps/bin/ck8s ops kubectl ${CLUSTER} get --all-namespaces issuers,clusterissuers,certificates\ndone\n</code></pre> <p>Check the output of the command above. All resources need to have the Ready column True.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#testing","title":"Testing","text":"<p>After completing the installation step you can test if the apps are properly installed and ready using the commands below:</p> <pre><code>for CLUSTER in sc wc; do\n  compliantkubernetes-apps/bin/ck8s test ${CLUSTER}\ndone\n</code></pre> <p>Done. Navigate to the endpoints, for example <code>grafana.$BASE_DOMAIN</code>, <code>kibana.$BASE_DOMAIN</code>, <code>harbor.$BASE_DOMAIN</code>, etc. to discover Welkin's features.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/on-prem-standard/#operate","title":"Operate","text":"<p>The following endpoints can be probed to ensure Welkin services are up and running:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --head https://harbor.$DOMAIN/healthz\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://grafana.ops.$DOMAIN/healthz\ncurl --head app.$DOMAIN/healthz  # Pokes the WC Ingress Controller\ncurl --head app.ops.$DOMAIN/healthz  # Pokes the SC Ingress Controller\n# All commands above should return 'HTTP/2 200'\n\ncurl --head -k https://kube-apiserver.$DOMAIN\ncurl --head https://thanos-receiver.ops.$DOMAIN\ncurl --head https://opensearch.ops.$DOMAIN\ncurl --head https://opensearch.$DOMAIN/api/status\n# The commands above should return 'HTTP/2 401'\n</code></pre> <p>Note</p> <p>Some of these subdomains can be overwritten in configuration (see example here)</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"]},{"location":"operator-manual/opensearch-mappings/","title":"OpenSearch Mappings","text":"<p>OpenSearch organizes data into indices. Each index is a collection of JSON documents. If you have a set of log lines that you want to add to OpenSearch, you must first convert them to JSON. A simple JSON document for a movie might look like this:</p> <pre><code>{\n  \"title\": \"Forrest Gump\",\n  \"release_date\": \"1994-07-06\"\n}\n</code></pre> <p>When you add the document to an index, OpenSearch adds some metadata, such as the unique document ID:</p> <pre><code>{\n  \"_index\": \"&lt;index-name&gt;\",\n  \"_type\": \"_doc\",\n  \"_id\": \"&lt;document-id&gt;\",\n  \"_version\": 1,\n  \"_source\": {\n    \"title\": \"Forrest Gump\",\n    \"release_date\": \"1994-07-06\"\n  }\n}\n</code></pre> <p>Indexes also contain mappings and settings:</p> <ul> <li>A mapping is the collection of fields that documents in the index have. In this case, those fields are <code>title</code> and <code>release_date</code>.</li> <li>Settings include data like the index name, creation date, and number of shards.</li> </ul> <p>You can define how documents and their fields are stored and indexed by creating a mapping. The mapping specifies the list of fields for a document. Every field in the document has a field type, which corresponds to the type of data the field contains. For example, you may want to specify that the <code>release_date</code> field should be of type <code>date</code>. To learn more, see Supported field types.</p>"},{"location":"operator-manual/opensearch-mappings/#dynamic-mapping","title":"Dynamic mapping","text":"<p>When you index a document, OpenSearch adds fields automatically with dynamic mapping. You can also explicitly add fields to an index mapping.</p> <p>Let's create an index called movies, and add a document:</p> <p></p> <p>We can see the dynamic mapping of all the document fields:</p> <p></p> <p>OpenSearch was able to infer that <code>release_date</code> is of type <code>date</code>, and <code>title</code> is of type <code>text</code>.</p> <p>You should be aware that OpenSearch type detection is based on some Internal rules, which by no means will infer the correct type for every field. Let's take this example:</p> <p>We will recreate the movies index, and add a new field called <code>duration_minutes</code>, and wrap the value in quotes:</p> <p></p> <p>Let's check the new dynamic mapping</p> <p></p> <p>You can see that OpenSearch choose a type of text for duration, which is technically correct, but it won't be of value especially if we were to run custom queries on the <code>duration_minutes</code> field, for example: Get all movies longer than a certain duration .. etc</p> <p>We can instruct OpenSearch to detect numeric values, by configuring the index at creation time. Let's recreate the index and enable numeric detection:</p> <p></p> <p>Checking the dynamic mapping again, we can see that OpenSearch was able to detect the numeric, and set the value of field to long</p> <p></p> <p>Although OpenSearch was able to detect the numeric type, long in our case is not the best data type, as it is an inefficient use of memory space. A better data type here could be short.</p> Field data type Description short A signed 16-bit integer. Minimum is \u22122 <sup>15</sup> . Maximum is 2 <sup>15 \u2212 1</sup>. long A signed 64-bit integer. Minimum is \u22122 <sup>63</sup> . Maximum is 2 <sup>63 \u2212 1</sup> <p>To learn more, see Numeric field types</p> <p>Dynamic mappings are fine when you're getting started with OpenSearch or when you're working with a new dataset. Once you have a more concrete idea of how you want to use the data, you want to be much more deliberate with your mappings, this is where an explicit mapping will be beneficial.</p>"},{"location":"operator-manual/opensearch-mappings/#explicit-mapping","title":"Explicit mapping","text":"<p>Explicit mappings allow us to be more precise with our field definitions taking the creative control away from OpenSearch. We describe everything upfront providing the structure of our data and the relevant properties.</p> <p>Here is an example where we define explicitly the fields type of an index:</p> <p></p> <p>An explicit mapping only bypasses the type inference that OpenSearch does for the fields we provide in the explicit mapping. If we index a document with a field not described in the explicit mapping, OpenSearch will still add that new field to the mapping and infer data type to use.</p> <p>Let's index a new document containing a new field <code>producer</code>:</p> <p></p> <p>The document gets indexed fine, and the mapping now contains the additional field with type text</p> <p></p> <p>But now if start adding all sorts of fields we don't know about, we're going to start having problems, and will probably end up with bad quality data, which will make it more difficult to work with.</p> <p>There are some options on how to deal when documents contains new fields that were not explicitly defined in the mapping. We can tell OpenSearch to either reject the document completely or we can allow the document to be indexed but ignore fields not in the explicit mapping.</p> <p>Let's configure the index to reject documents that contains fields not defined in the explicit mapping, to do this we set dynamic to strict:</p> <p></p> <p>If we try now to index a document with a field not in the explicit mapping, we'll get an error:</p> <p></p> <p>This is really handy as it will prevent field count explosion, especially in production environments, as the fields will always be increasing as clients can add documents with previously unmapped fields. However this might be too strict in some cases, so an alternative is still to allow the document to be indexed but ignore fields not defined in the explicit mapping. To do this we set dynamic to false:</p> <p></p> <p>Now we won't get an error when we index with the same document, and the field is not in the mapping for the index</p> <p></p> <p>One thing worth mentioning is that while the producer field was not indexed, and can't be used for queries, it is still there if we get the document, and the source:</p> <p></p> <p>While this new field is not usable for queries as said before, but if we saw the need, we can create a new index with the producer field in the explicit mapping, then re-index the documents in movies index to where the new index is.</p>"},{"location":"operator-manual/opensearch-mappings/#mapping-constraints","title":"Mapping constraints","text":"<p>There is a limit to the number of fields a mapping can have but it's a limit that can be changed. The default value is a 1000 fields per index. This limit is there to prevent Cluster performance issues, as resource utilization increases a lot when you have a large number of fields in an index.</p> <p></p> <p>Also, note that new fields can be added to an explicit mapping, but field types cannot be changed. The general pattern here is to create a new index with a new mapping and bring documents over in what's called a re-index operation.</p>"},{"location":"operator-manual/provider-audit/","title":"Infrastructure Provider Audit","text":"<p>This page will help you do your due diligence and ensure you choose a Infrastructure Provider that provides a solid foundation for Welkin and your application. Elastisys regularly uses this template to validate cloud partners, as required for ISO 27001 certification.</p>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#rationale","title":"Rationale","text":"<p>Welkin is designed to build upon the security and compliance of the underlying Infrastructure Provider. If you cannot trust the underlying provider with controls such as physical security to the servers, safe disposal of hard drives, access control to infrastructure control plane, then no technical measure will help you achieve your security and compliance goals. Trying to take preventive measures in Welkin -- i.e., at the platform level -- is inefficient at best and downright dangerous at worst. Failing to due your due diligence will end up in security theatre, putting your reputation at risk.</p>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#overview","title":"Overview","text":"<p>The remainder of this page contains open questions that you should ask your Infrastructure Provider. Notice the following:</p> <ul> <li>Make sure you ask open questions and note down the answers. Burden of proof lies with the provider that they do an excellent job with protecting data.</li> <li>Ask all questions, then evaluate the provider's suitability. It is unlikely that you'll find the perfect provider, but you'll likely find one that is sufficient for your present and future needs.</li> <li>The least expected the answer, the more \"digging\" is needed.</li> <li>\"You\" represents the Infrastructure Provider and \"I\" represents the Welkin administrator.</li> </ul>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#technical-capability-questionnaire","title":"Technical Capability Questionnaire","text":"<ol> <li>Availability Zones:<ol> <li>Where are your data centers located?</li> <li>How are they presented, i.e., single API vs. multiple independent APIs?</li> </ol> </li> <li>Services:<ol> <li>What services do you offer? (e.g., VMs, object storage)</li> <li>Are all your services available in all zones?</li> </ol> </li> <li>Identity and Access Management (IAM):<ol> <li>Do you offer IAM?</li> <li>How can I create roles? Via an API? Via a UI? Via a Terraform provider?</li> <li>What services can I configure role-based access control for?</li> <li>Can IAM be configured via API? Can IAM be configured via Terraform?</li> <li>Can one single user be given access to multiple projects?</li> </ol> </li> <li> <p>Infrastructure-aaS:</p> <ol> <li>Which IaaS engine do you use? (e.g., OpenStack, VMware, proprietary)</li> <li>Do you have a Terraform provider for your API?</li> <li>Do you have pre-uploaded Ubuntu images? Which?<ol> <li>Do these images have AutomaticSecurityUpdates by default?</li> <li>Do these images have NTP enabled by default?</li> </ol> </li> <li>Do you have a Kubernetes integration for your IaaS?<ol> <li>Can I use a cloud-controller for automatic discovery of Nodes and labeling Nodes with the right Zone?</li> </ol> </li> <li>Can you handle large diurnal capacity changes, a.k.a., auto-scaling? E.g., 40 VMs from 6.00 to 10.00, but only 10 VMs from 10.00-6.00.<ol> <li>Can I reserve VMs? How do you bill for reserved but unused VMs?</li> <li>What technical implementation do you recommend? E.g., pause/unpause VMs, stop/start VMs, terminate/recreate VMs.</li> </ol> </li> <li>Do you support anti-affinity?<ol> <li>If yes, what is the maximum number of VMs which can be part of an anti-affinity group?</li> <li>If not, how can we ensure that VMs don't end up on the same physical servers?</li> </ol> </li> </ol> </li> <li> <p>Storage capabilities:</p> <ol> <li>Do you offer Object Storage as a Service (OSaaS)?<ol> <li>Can I use the object storage via an S3-compatible API?</li> <li>Can I create buckets via API?</li> <li>Can I create bucket credentials via API?</li> <li>Do you have a Terraform provider for your API?</li> <li>In which zones?</li> <li>Do you have immutable storage or object lock?</li> <li>Is OSaaS stretched across zones?</li> <li>Is object storage replicated across zones?</li> </ol> </li> <li>Do you offer Block storage as a Service (BSaaS)?<ol> <li>Which API (OpenStack, VMware)?</li> <li>In which zones?</li> <li>Can I use a Container Storage Interface (CSI) driver for automatic creating of PersistentVolumes?</li> <li>[For NFS] How did you configure User ID Mapping, specifically <code>root_squash</code>, <code>no_root_squash</code>, <code>all_squash</code>, <code>anonuid</code> and <code>anongid</code>? Mapping the root UID to values typically used by containers, e.g., 1000, will lead to permission denied errors. For example, OpenSearch's init containers do <code>chown 1000</code> which fails with <code>squash_root</code> and <code>anonuid=1000</code>.</li> <li>Is BSaaS stretched across zones?</li> <li>Is block storage replicated across zones?</li> <li>Does the CSI driver support the Snapshot feature? This is needed for more consistent Velero backups.</li> </ol> </li> <li>Do you offer encryption-at-rest?<ol> <li>Encrypted object storage: Do you offer this by default?</li> <li>Encrypted block storage: Do you offer this by default?</li> <li>Encrypted boot discs: Do you offer this by default?</li> <li>If not, how do you dispose of media potentially containing personal data (e.g., hard drivers, backup tapes)?</li> </ol> </li> </ol> </li> <li> <p>Networking capabilities:</p> <ol> <li>Can the VMs be set up on a private network? Do you have a Terraform provider for your API?<ol> <li>Is your private network stretched across zones?</li> <li>Do you trust the network between your data centers?</li> <li>Does the private network overlap:<ol> <li>The default Docker network (<code>172.17.0.0/16</code>)?</li> <li>The default Kubernetes Service network (<code>10.233.0.0/18</code>)?</li> <li>The default Kubernetes Pod network (<code>10.233.64.0/18</code>)?</li> </ol> </li> <li>How can we peer private networks across projects?</li> </ol> </li> <li>Firewall-aaS<ol> <li>Are Firewall-aaS available?</li> <li>What API? (e.g., OpenStack, VMware)</li> <li>Do you have a Terraform provider for your API?</li> </ol> </li> <li>Do you offer Load Balancer-aaS (LBaaS)?<ol> <li>Can I create a LB via API?</li> <li>Do you have a Terraform provider for your API?</li> <li>Can I use a cloud-controller for automatic creation of external LoadBalancers?</li> <li>Can I set up a LB across zones? Via API?</li> <li>Can VMs see themselves via the load-balancers's IP address? (If not, then VMs need a minor fix.)</li> <li>Do your LBs preserve source IP addresses? Usually, this involves clever DNAT or PROXY protocol support.</li> </ol> </li> <li>Do you offer IPv6 support? By default?</li> <li>Do you offer DNS as a Service? Which API?</li> </ol> </li> <li> <p>Network security:</p> <ol> <li>Do you allow NTP (UDP port 123) for clock synchronization to the Internet?<ol> <li>If not, do you have a private NTP server?</li> </ol> </li> <li>Do you allow ACME (TCP port 80) for automated certificate provisioning via Let's Encrypt?<ol> <li>If not, how will you provision certificates?</li> </ol> </li> <li>Do you periodically scan your IP ranges, e.g., via ANTS?</li> </ol> </li> </ol>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#organizational-capabilities","title":"Organizational capabilities","text":"<ol> <li>What regulations are your existing customers subject to? (e.g., GDPR, public sector regulations, some ISO 27001 profile)</li> <li>Can you show us your ISO-27001 certification?<ol> <li>Which profile?</li> <li>Which organization made the audit?</li> <li>Can we get a copy of the Statement of Applicability (SoA)?</li> </ol> </li> <li>Who is overall responsible with compliance in your organization?</li> <li>How do you implement regulatory and contractual requirements?</li> <li>How is a new requirement discovered?<ol> <li>What is the journey that a requirement takes from discovery, to updating policies, to training employees, to implementation, to evidence of implementation?</li> </ol> </li> <li>Do your data-centers fulfill physical security \"skyddsklass 3\" according to SSF 130 and SSF 200?<ol> <li>If not, how do you comply with Directive (EU) 2022/2557 Resilience of critical entities Art. 13 p. 1(b)?</li> <li>If not, how is physical security handled?</li> </ol> </li> <li>How do you handle incidents and deviations?<ol> <li>What response times / time to resolution do you offer?</li> <li>What are your actual response times / time to resolution?</li> </ol> </li> <li>What is your change management process?</li> <li>How do you handle technical vulnerabilities?</li> <li>How do you handle capacity management?</li> <li>In case of a breach, how long until you notify your customers?</li> <li>What SLA do you offer?<ol> <li>What uptime do you offer?</li> <li>What is your measured uptime?</li> <li>Do you have a public status page?</li> </ol> </li> <li>How do you handle access control?</li> <li>Does your operation team have individual accounts? How do you handle team member on-boarding / off-boarding?</li> <li>How do you communicate credentials to your customers?</li> <li>Do you have audit logs?<ol> <li>How long do you store audit logs? Who has access to them? How are they protected against disclosure and tampering?</li> </ol> </li> <li>How do you handle business continuity?<ol> <li>How often do you test fail-over? How did the last test go?</li> </ol> </li> <li>How do you handle disaster recovery?<ol> <li>How often do you test disaster recovery? How did the last test go?</li> </ol> </li> <li>What is your use of cryptography policy?</li> <li>How do you deal with DDoS attacks?</li> <li>Who are your colocation providers? Are they subprocessors? See Guidance from the Danish Data Protection Authority.<ol> <li>Does your colocation provider have access to personal data, e.g. access to the server cabinet and can access the information that is processed on the servers or transferred via switches?</li> <li>Can your colocation provider replace hard drives, memory, etc.?</li> <li>Can your colocation provider move, restart or otherwise handle the servers?</li> <li>Does your colocation provider provide additional services beyond physical facilities as well as electricity and Internet?</li> </ol> </li> <li>When did you perform the last penetration test? 1. Can you share anything about the major findings and how you resolved them?</li> </ol> <p>For Welkin Enterprise Customers</p> <p>Feel free to skip the questions below. They are designed for our Managed Service and might not be relevant for you. We share them here for the sake of full transparency.</p>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#legal-issues","title":"Legal issues","text":"<ol> <li>Do you fully operate under EU jurisdiction?</li> <li>Is your ownership fully under EU jurisdiction?</li> <li>Are your suppliers fully under EU jurisdiction?<ol> <li>Even the web fonts and analytics code on your front-page?</li> </ol> </li> <li>Do you have a DPO?<ol> <li>Is this an internal employee or outsourced?</li> </ol> </li> <li>Can you show us your Data Processing Agreement (DPA)?</li> <li>[HIPAA only] Are you familiar with Business Associate Agreements?<ol> <li>Are you ready to sign one with us?</li> </ol> </li> </ol>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#collaboration","title":"Collaboration","text":"<ol> <li>How can we collaborate with your on-call team?<ol> <li>What collaboration channels do you offer? (e.g., Slack, Teams, phone, service desk)</li> <li>What response times can we expect?</li> <li>Is your on-call team available 24/7?</li> </ol> </li> <li>Are you open to having quarterly operations (engineering) retrospectives? Our engineering team wants to keep a close loop with vendors and regularly discuss what went well, what can be improved, and devise a concrete action plan.</li> <li>Are you open to having quarterly roadmap discussions?</li> </ol>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#environment-management","title":"Environment Management","text":"<ol> <li>What environmental policies and certifications do you have?</li> <li>What energy sources are your data-centers using?</li> <li>How do you work to become more energy efficient?</li> <li>How do you recycle used/old equipment?</li> <li>Do you do any form of environmental compensation activities?</li> </ol>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/provider-audit/#evidence","title":"Evidence","text":"<p>The audit should conclude with gathering the following documents in an \"evidence package\":</p> <ol> <li>Filled questionnaire</li> <li>All relevant certificates, e.g., ISO 14001, ISO 27001, \u201cgreen cloud\u201d</li> <li>Latest version of the Terms of Service and Data Protection Agreement</li> <li>All relevant certificates from data-centre providers</li> <li>Signed and transparent ownership structure</li> </ol>","tags":["HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 12 \u00a7","MSBFS 2020:7 4 kap. 21 \u00a7","HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","GDPR Art. 28 Processor","NIST SP 800-171 3.13.16","NIS2 Minimum Requirement (d) Security of direct suppliers","ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","ISO 27001 Annex A 7 Physical Controls"]},{"location":"operator-manual/runbooks/","title":"Runbooks","text":"<p>Note</p> <p>This page describes runbooks for platform administrators. As an application developer, you are most likely looking for Troubleshooting for Application Developers.</p> <p>For Welkin Enterprise Customers</p> <p>To get access to the Welkin runbooks, contact Elastisys.</p> <p>Runbooks document step-by-step processes, ensuring that tasks are performed consistently and correctly, reducing errors and reliance on tribal knowledge. Runbooks are essential for maintaining efficiency, consistency, and reliability in Welkin operations.</p> <p>Welkin runbooks are searchable and describe what to do:</p> <ul> <li>when receiving a change request from an application developer;</li> <li>how to handle an alert.</li> </ul> <p>The remainder of this page illustrates an example of a Welkin runbook for the ThanosCompactHalted alert.</p>"},{"location":"operator-manual/runbooks/#example-thanos-compact-halted-pvc","title":"Example: Thanos Compact Halted - PVC","text":""},{"location":"operator-manual/runbooks/#alert-thanoscompacthalted","title":"Alert: ThanosCompactHalted","text":""},{"location":"operator-manual/runbooks/#tags","title":"Tags","text":"<ul> <li>Thanos</li> <li>ThanosCompactHalted</li> <li>PVC</li> </ul>"},{"location":"operator-manual/runbooks/#reason","title":"Reason","text":"<p>The Thanos compactor is responsible for downsampling and pushing metrics from its PVC to object storage.</p> <p>One reason the compactor has halted is that the volume attached to Thanos compactor is full.</p>"},{"location":"operator-manual/runbooks/#impact","title":"Impact","text":"<ul> <li>Retention rules are not enforced</li> <li>Query performance might degrade</li> <li>Downsampling is not performed</li> </ul>"},{"location":"operator-manual/runbooks/#diagnosis","title":"Diagnosis","text":"<p>Start investigating if the PVC for the Thanos compactor is full. This can be easily checked in the Grafana dashboard (Kubernetes/Persistent Volumes).</p> <p>It is also possible to look at the compactor logs, which should show that there is no space left on the device.</p> <pre><code>kubectl logs -n thanos deployments/thanos-receiver-compactor | grep halt\n\nts=2024-07-18T05:53:16.578898999Z caller=compact.go:527 level=error msg=\"critical error detected; halting\" ... 2 errors: preallocate: no space left on device; ...\"\n</code></pre>"},{"location":"operator-manual/runbooks/#mitigation","title":"Mitigation","text":"<p>To resolve this issue, we need to increase the volume size for the compactor.</p> <p>Update the sc/mc-config.yaml to increase the PVC for Thanos compactor</p> <pre><code>     persistence:\n       size: 50Gi\n   compactor:\n      persistence:\n-      size: XGi\n+      size: YGi\n</code></pre> <pre><code>./bin/ck8s ops helmfile sc -l app=thanos diff\n./bin/ck8s ops helmfile sc -l app=thanos apply\n</code></pre> <p>Wait for a while, then check for halting again.</p> <pre><code>kubectl logs -n thanos deployments/thanos-receiver-compactor | grep halt\n</code></pre>"},{"location":"operator-manual/troubleshooting/","title":"Troubleshooting for Platform Administrators","text":"<p>For Welkin Enterprise Customers</p> <p>Please start by running these commands.</p> <p>If you are struggling, don't hesitate to file a ticket.</p> <p>You can run the following command from the compliantkubernetes-apps repository to collect diagnostic information that will help us support you. Ensure that you have put fingerprints received from Elastisys in a file named <code>${CK8S_CONFIG_PATH}/diagnostics_receiver.gpg</code>.</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt;\n</code></pre> <p>Show more examples on using the diagnostics command The command <code>ck8s diagnostics</code> can be provided with different flags to gather additional information from your environment, to see all available options run: <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --help\n</code></pre> <p>Some example use cases:</p> <ul> <li> <p>To include config files found in <code>CK8S_CONFIG_PATH</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; --include-config\n</code></pre> </li> <li> <p>To retrieve more information such as YAML manifests for resources in a specific namespace, in this example <code>ingress-nginx</code>:</p> <pre><code>./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> <li> <p>It is also possible to set which GPG keys should be used by setting <code>CK8S_PGP_FP</code>:</p> <pre><code>export CK8S_PGP_FP=&lt;gpg-fingerprint1&gt;,&lt;gpg-fingerprint2&gt;\n./bin/ck8s diagnostics &lt;wc|sc&gt; namespace ingress-nginx\n</code></pre> </li> </ul> <p>Please also provide us with your terminal in a text format. We need to look both at the commands you typed and their output.</p> <p>Help! Something is wrong with my Welkin Cluster. Fear no more, this guide will help you make sense.</p> <p>This guide assumes that:</p> <ul> <li>You have pre-requisites installed.</li> <li>Your environment variables, in particular <code>CK8S_CONFIG_PATH</code> is set, and <code>CLUSTER</code> set to either <code>sc</code> or <code>wc</code>.</li> <li>Your configuration folder is available.</li> <li><code>compliantkubernetes-apps</code> and <code>compliantkubernetes-kubespray</code> is available.</li> </ul> <p>Important</p> <p><code>./bin/ck8s</code> references the <code>compliantkubernetes-apps</code> CLI <code>./bin/ck8s-kubespray</code> references the <code>compliantkubernetes-kubespray</code> CLI</p> <p>Important</p> <p>For some of the Ansible commands below, you might require root privileges. To run commands as a privileged user with Ansible, use the <code>--become, -b</code> flag.</p> <p>Example: <code>ansible -i inventory.ini -b all -m ping</code></p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#i-have-no-clue-where-to-start","title":"I have no clue where to start","text":"<p>If you get lost, start checking from the \"physical layer\" and up.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#are-the-nodes-still-accessible-via-ssh","title":"Are the Nodes still accessible via SSH?","text":"<pre><code>ansible -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini all -m ping\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#are-the-nodes-doing-fine","title":"Are the Nodes \"doing fine\"?","text":"<p><code>dmesg</code> should not display unexpected messages. OOM will show up here.</p> <pre><code>ansible -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; dmesg | tail -n 10'\n</code></pre> <p>Uptime should show high uptime (e.g., days) and low load (e.g., less than 3):</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; uptime'\n</code></pre> <p>Any process that uses too much CPU?</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; ps -Ao user,uid,comm,pid,pcpu,tty --sort=-pcpu | head -n 6'\n</code></pre> <p>Is there enough disk space? All writeable file-systems should have at least 30% free.</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; df -h'\n</code></pre> <p>Is there enough available memory? There should be at least a few GB of available memory.</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; cat /proc/meminfo | grep Available'\n</code></pre> <p>Can Nodes access the Internet?</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; hostname; curl --silent  https://checkip.amazonaws.com'\n</code></pre> <p>Are the Nodes having the proper time? You should see <code>System clock synchronized: yes</code> and <code>NTP service: active</code>.</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'echo; timedatectl status'\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#is-the-base-os-doing-fine","title":"Is the base OS doing fine?","text":"<p>We generally run the latest Ubuntu LTS, at the time of this writing Ubuntu 20.04 LTS.</p> <p>You can confirm this by doing:</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'cat /etc/lsb-release'\n</code></pre> <p>Are systemd units running fine? You should see <code>running</code> and not <code>degraded</code>.</p> <pre><code>ansible -i $CK8S_CONFIG_PATH/${CLUSTER}-config/inventory.ini all -m shell -a 'systemctl is-system-running'\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#are-the-kubernetes-clusters-doing-fine","title":"Are the Kubernetes Clusters doing fine?","text":"<p>Are the Nodes reporting in on Kubernetes? All Kubernetes Nodes, both control-plane and workers, should be <code>Ready</code>:</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get nodes\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#is-rook-doing-fine","title":"Is Rook doing fine?","text":"<p>If Rook is installed, is Rook doing fine? You should see <code>HEALTH_OK</code>.</p> <pre><code>export CK8S_KUBESPRAY_PATH=/path/to/compliantkubernetes-kubespray\n\n./bin/ck8s ops kubectl $CLUSTER -n rook-ceph apply -f $CK8S_KUBESPRAY_PATH/rook/toolbox-deploy.yaml\n</code></pre> <p>Once the Pod is Ready run:</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER -n rook-ceph exec deploy/rook-ceph-tools -- ceph status\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#are-kubernetes-pods-doing-fine","title":"Are Kubernetes Pods doing fine?","text":"<p>Pods should be <code>Running</code> or <code>Completed</code>, and fully <code>Ready</code> (e.g., <code>1/1</code> or <code>6/6</code>)?</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get --all-namespaces pods\n</code></pre> <p>Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., <code>2/2 2 2</code>).</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get --all-namespaces deployments\n</code></pre> <p>Are all DaemonSets fine? DaemonSets should show as many Pods Desired, Current, Ready and Up-to-date, as Desired.</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get --all-namespaces ds\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#are-helm-releases-fine","title":"Are Helm Releases fine?","text":"<p>All Releases should be <code>deployed</code>.</p> <pre><code>./bin/ck8s ops helm $CLUSTER list --all --all-namespaces\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#is-cert-manager-doing-fine","title":"Is cert-manager doing fine?","text":"<p>Are (Cluster)Issuers fine? All Resources should be <code>READY=True</code> or <code>valid</code>.</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get clusterissuers,issuers,certificates,orders,challenges --all-namespaces\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#where-do-i-find-the-nodes-public-and-private-ip","title":"Where do I find the Nodes public and private IP?","text":"<pre><code>find . -name inventory.ini\n</code></pre> <p>or</p> <pre><code>ansible-inventory -i ${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini --list all\n</code></pre> <p><code>ansible_host</code> is usually the public IP, while <code>ip</code> is usually the private IP.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#node-cannot-be-accessed-via-ssh","title":"Node cannot be accessed via SSH","text":"<p>Important</p> <p>Make sure it is \"not you\". Are you well connected to the VPN? Is this the only Node which lost SSH access?</p> <p>Important</p> <p>If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node.</p> <p>Try connecting to the unhealthy Node via a different Node and internal IP:</p> <pre><code>UNHEALTHY_NODE=172.0.10.205  # You lost access to this one\nJUMP_NODE=89.145.xxx.yyy  # You have access to this one\n\nssh -J ubuntu@$JUMP_NODE ubuntu@$UNHEALTHY_NODE\n</code></pre> <p>Try rebooting the Node via Infrastructure Provider specific CLI:</p> <pre><code>UNHEALTHY_NODE=cksc-worker-2\n\n# Example for ExoScale\nexo vm reboot --force $UNHEALTHY_NODE\n</code></pre> <p>If using Rook make sure its health goes back to <code>HEALTH_OK</code>.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#a-node-has-incorrect-time","title":"A Node has incorrect time","text":"<p>Incorrect time on a Node can have sever consequences with replication and monitoring. In fact, if you follow ISO 27001, A.12.4.4 Clock Synchronisation requires you to ensure clocks are synchronized.</p> <p>These days, Linux distributions should come out-of-the-box with timesyncd for time synchronization via NTP.</p> <p>To figure out what is wrong, SSH into the target Node and try the following:</p> <pre><code>sudo systemctl status systemd-timesyncd\nsudo journalctl --unit systemd-timesyncd\nsudo timedatectl status\nsudo timedatectl timesync-status\nsudo timedatectl show-timesync\n</code></pre> <p>Possible causes include incorrect NTP server settings, or NTP being blocked by firewall. For reminder, NTP works over UDP port 123.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#node-seems-not-fine","title":"Node seems not fine","text":"<p>Important</p> <p>If you are using Rook, it is usually set up with replication 2, which means it can tolerate one restarting Node. Make sure that, either Rook is healthy or that you are really sure you are restarting the right Node.</p> <p>Try rebooting the Node:</p> <pre><code>UNHEALTHY_NODE=89.145.xxx.yyy\n\nssh ubuntu@$UNHEALTHY_NODE sudo reboot\n</code></pre> <p>If using Rook make sure its health goes back to <code>HEALTH_OK</code>.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#node-seems-really-not-fine-i-want-a-new-one","title":"Node seems really not fine. I want a new one","text":"<p>Is it 2AM? Do not replace Nodes, instead simply add a new one. You might run out of capacity, you might lose redundancy, you might replace the wrong Node. Prefer to add a Node and see if that solves the problem.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#okay-i-want-to-add-a-new-node","title":"Okay, I want to add a new Node","text":"<p>Prefer this option if you \"quickly\" need to add CPU, memory or storage (i.e., Rook) capacity.</p> <p>First, check for infrastructure drift, as shown here.</p> <p>Depending on your provider: If the infrastructure is not managed by Terraform you can skip to step 3:</p> <ol> <li>Add a new Node by editing the <code>*.tfvars</code>.</li> <li>Re-apply Terraform.</li> <li>Add the new Node to the <code>inventory.ini</code> (skip this step if the Cluster is using a dynamic inventory).</li> <li> <p>Re-apply Kubespray only for the new Node.</p> <pre><code>cd [welkin-kubespray-root-dir]\n\nCLUSTER=[sc | wc]\n\n./bin/ck8s-kubespray run-playbook $CLUSTER facts.yml\n./bin/ck8s-kubespray run-playbook $CLUSTER scale.yml -b --limit=[new_node_name]\n</code></pre> </li> <li> <p>Add SSH keys to the new Node if necessary</p> <pre><code>./bin/ck8s-kubespray apply-ssh $CLUSTER --limit=[new_node_name]\n</code></pre> </li> <li> <p>Update Network Policies</p> <pre><code>cd [welkin-apps-root-dir]\n\n./bin/ck8s update-ips sc update\n./bin/ck8s update-ips wc update\n\n./bin/ck8s ops helmfile sc -l app=common-np -i apply\n./bin/ck8s ops helmfile wc -l app=common-np -i apply\n\n./bin/ck8s ops helmfile sc -l app=service-cluster-np -i apply\n# or\n./bin/ck8s ops helmfile wc -l app=workload-cluster-np -i apply\n</code></pre> <p>Check that the new Node joined the Cluster, as shown here.</p> </li> </ol>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#a-systemd-unit-failed","title":"A systemd unit failed","text":"<p>SSH into the Node. Check which systemd unit is failing:</p> <pre><code>systemctl --failed\n</code></pre> <p>Gather more information:</p> <pre><code>FAILED_UNIT=fwupd-refresh.service\n\nsystemctl status $FAILED_UNIT\njournalctl --unit $FAILED_UNIT\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#rook-seems-not-fine","title":"Rook seems not fine","text":"<p>Please check the following upstream documents:</p> <ul> <li>Rook Common Issues</li> <li>Ceph Common Issues</li> </ul>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#pod-seems-not-fine","title":"Pod seems not fine","text":"<p>Make sure you are on the right Cluster:</p> <pre><code>echo $CK8S_CONFIG_PATH\necho $CLUSTER\n</code></pre> <p>Find the name of the Pod which is not fine:</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER get pod -A\n\n# Copy-paste the Pod and Pod namespace below\nUNHEALTHY_POD=prometheus-kube-prometheus-stack-prometheus-0\nUNHEALTHY_POD_NAMESPACE=monitoring\n</code></pre> <p>Gather some \"evidence\" for later diagnostics, when the heat is over:</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER describe pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\n./bin/ck8s ops kubectl $CLUSTER logs -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\n</code></pre> <p>Try to kill and check if the underlying Deployment, StatefulSet or DaemonSet will restart it:</p> <pre><code>./bin/ck8s ops kubectl $CLUSTER delete pod -n $UNHEALTHY_POD_NAMESPACE $UNHEALTHY_POD\n./bin/ck8s ops kubectl $CLUSTER get pod -A --watch\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#helm-release-is-failed","title":"Helm Release is <code>failed</code>","text":"<p>Make sure you are on the right Cluster:</p> <pre><code>echo $CK8S_CONFIG_PATH\necho $CLUSTER\n</code></pre> <p>Find the failed Release:</p> <pre><code>./bin/ck8s ops helm $CLUSTER ls --all-namespaces --all\n\nFAILED_RELEASE=user-rbac\nFAILED_RELEASE_NAMESPACE=kube-system\n</code></pre> <p>Just to make sure, do a drift check, as shown here.</p> <p>Remove the failed Release:</p> <pre><code>./bin/ck8s ops helm $CLUSTER uninstall -n $FAILED_RELEASE_NAMESPACE $FAILED_RELEASE\n</code></pre> <p>Re-apply <code>apps</code> according to documentation.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#cert-manager-is-not-fine","title":"cert-manager is not fine","text":"<p>Follow cert-manager's troubleshooting, specifically:</p> <ul> <li>Troubleshooting</li> <li>Troubleshooting Issuing ACME Certificates</li> </ul>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#failed-to-perform-self-check-no-such-host","title":"Failed to perform self check: no such host","text":"<p>If with <code>kubectl describe challenges -A</code> you get an error similar to below:</p> <pre><code>Waiting for HTTP-01 challenge propagation: failed to perform self check\n    GET request ''http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls'':\n    Get \"http://url/.well-known/acme-challenge/xVfDZoLlqs4tad2qOiCT4sjChNRausd5iNpbWuGm5ls\":\n    dial tcp: lookup opensearch.domain on 10.177.0.3:53: no such host'\n</code></pre> <p>Then you might have a DNS issue inside your Cluster. Make sure that <code>global.clusterDns</code> in <code>common-config.yaml</code> is set to the CoreDNS Service IP returned by <code>kubectl get svc -n kube-system coredns</code>.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#failed-to-perform-self-check-connection-timed-out","title":"Failed to perform self check: connection timed out","text":"<p>If with <code>kubectl describe challenges -A</code> you get an error similar to below:</p> <pre><code>Reason: Waiting for http-01 challenge propagation: failed to perform self check GET request 'http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA': Get \"http://abc.com/.well-known/acme-challenge/Oej8tloD2wuHNBWS6eVhSKmGkZNfjLRemPmpJoHOPkA\": dial tcp 18.192.17.98:80: connect: connection timed out\n</code></pre> <p>Then your Kubernetes data plane Nodes cannot connect to themselves with the IP address of the load-balancer that fronts them. The easiest is to configure the load-balancer's IP address on the loopback interface of each Nodes. (See example here.)</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-infrastructure-drifted-due-to-manual-intervention","title":"How do I check if infrastructure drifted due to manual intervention?","text":"<p>Go to the docs of the Infrastructure Provider and run Terraform <code>plan</code> instead of <code>apply</code>. For Exoscale, it looks as follows:</p> <pre><code>TF_SCRIPTS_DIR=$(readlink -f compliantkubernetes-kubespray/kubespray/contrib/terraform/exoscale)\npushd ${TF_SCRIPTS_DIR}\nexport TF_VAR_inventory_file=${CK8S_CONFIG_PATH}/${CLUSTER}-config/inventory.ini\nterraform init\nterraform plan \\\n    -var-file=${CK8S_CONFIG_PATH}/${CLUSTER}-config/cluster.tfvars \\\n    -state=${CK8S_CONFIG_PATH}/${CLUSTER}-config/terraform.tfstate\npopd\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-the-kubespray-setup-drifted-due-to-manual-intervention","title":"How do I check if the Kubespray setup drifted due to manual intervention?","text":"<p>At the time of this writing, this cannot be done, but efforts are underway.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#how-do-i-check-if-apps-drifted-due-to-manual-intervention","title":"How do I check if <code>apps</code> drifted due to manual intervention?","text":"<pre><code># For Management Cluster\n./bin/ck8s ops helmfile sc diff  # Respond \"n\" if you get WARN\n</code></pre> <pre><code># For the Workload Clusters\n./bin/ck8s ops helmfile wc diff  # Respond \"n\" if you get WARN\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#velero-backup-stuck-in-progress","title":"Velero backup stuck in progress","text":"<p>Velero is known to get stuck <code>InProgress</code> when doing backups</p> <pre><code>velero backup get\n\nNAME                                 STATUS             ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\nvelero-daily-backup-20211005143248   InProgress         0        0          2021-10-05 14:32:48 +0200 CEST   29d       default            !nobackup\n</code></pre> <p>First try to delete the backup</p> <pre><code>./velero backup delete velero-daily-backup-20211005143248\n</code></pre> <p>Then kill all the Pods under the velero namespace</p> <pre><code>./bin/ck8s ops kubectl wc delete pods -n velero --all\n</code></pre> <p>Check that the backup is gone</p> <pre><code>velero backup get\n\nNAME                                 STATUS             ERRORS   WARNINGS   CREATED                          EXPIRES   STORAGE LOCATION   SELECTOR\n</code></pre> <p>Recreate the backup from a schedule</p> <pre><code>velero backup create --from-schedule velero-daily-backup\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/troubleshooting/#how-do-i-use-kubectl-and-helm-directly","title":"How do I use <code>kubectl</code> and <code>helm</code> directly?","text":"<p>This guide makes heavy use of the <code>compliantkubernetes-apps</code> CLI to access and control Welkin Clusters. However, you can use <code>kubectl</code> and <code>helm</code> directly, by exporting a <code>KUBECONFIG</code> like so:</p> <pre><code>export KUBECONFIG=${CK8S_CONFIG_PATH}/.state/kube_config_${CLUSTER}.yaml\n\nkubectl get pods -A\n\nhelm list -A\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"]},{"location":"operator-manual/understand-the-basics/","title":"Understand the Basics","text":"<p>Welkin is a platform which significantly reduces the cognitive load on the application team. However, this means that you the platform administrator are taking over some of that complexity. Before you can operate Welkin, you need to understand the basics. This section provides some useful links to build up this understanding.</p> <ul> <li>Linux administration<ul> <li>We recommend Linux Foundation Certified System Administrator (LFCS)</li> <li>Ansible</li> </ul> </li> <li>Containers<ul> <li>We recommend Containers Fundamentals (LFS253)</li> </ul> </li> <li>Kubernetes administration<ul> <li>We recommend Certified Kubernetes Administrator (CKA)</li> </ul> </li> <li>Kubernetes networking<ul> <li>We recommend Introduction to Cilium (LFS146)</li> <li>NOTE: In Welkin, we use Calico as a CNI, however, the basic CNI concepts are the same.</li> <li>Ingress Controllers</li> <li>NGINX Ingress Controller</li> </ul> </li> <li>Helm<ul> <li>We recommend Managing Kubernetes Applications with Helm (LFS244)</li> </ul> </li> <li>Metrics observability<ul> <li>Prometheus</li> <li>Thanos</li> </ul> </li> <li>Logs observability<ul> <li>Fluentd<ul> <li>S3 plugin for Fluentd</li> <li>OpenSearch plugin for Fluentd</li> </ul> </li> <li>OpenSearch</li> </ul> </li> <li>Container registries and container vulnerability scanning<ul> <li>Harbor</li> <li>Trivy</li> </ul> </li> <li>Kubernetes security hardening<ul> <li>We recommend Certified Kubernetes Security Specialist (CKS)</li> <li>Pod Security Standards</li> <li>Falco</li> <li>Kured</li> <li>OpenPolicyAgent and Gatekeeper</li> <li>OpenID</li> <li>Dex</li> </ul> </li> <li>Kubernetes backups<ul> <li>Velero</li> <li>Rclone</li> </ul> </li> </ul> <p>Platform administration, in particular on-call, can be stressful. Therefore, familiarize yourself with:</p> <ul> <li>OODA loop, in particular, make sure you always orient yourself before deciding what to do, especially at 2am.</li> <li>The Site Reliability and Software Engineering Soft Skills That Matter Most</li> </ul>"},{"location":"operator-manual/understand-welkin/","title":"Understand Welkin","text":"<p>This page gives you a basic understanding of Welkin from the point-of-view of the Platform Administrator. For a basic understanding of Welkin from the point-of-view of the Application Developers, head to Application Developer Overview.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#welkin-architecture","title":"Welkin Architecture","text":"<p>To begin with, you should familiarize yourself with Welkin's architecture. It describes what components are part of Welkin and what component talks to which other components. It allows you to create a mental model and reason about complex failure modes, such as \"a buffer overflow in Fluentd may be caused by OpenSearch lacking sufficient capacity to ingest all logs\".</p> <p>In particular, notice that:</p> <ul> <li>Welkin is composed of at least two Kubernetes Clusters:<ul> <li>at least one Workload Cluster: this hosts the application(s) of the Application Developer; and</li> <li>one Service Cluster: this provides several Service Endpoints to the Application Developers, in particular around authentication and observability.</li> </ul> </li> <li>Welkin is composed of two layers:<ul> <li>The Kubernetes-lifecycle layer sets up rather vanilla Kubernetes Clusters with some security defaults. This layer is implemented either via Kubespray or Cluster API.</li> <li>The Welkin Apps layer augments the two Kubernetes Clusters with projects around security and observability. This layer has a single implementation.</li> </ul> </li> </ul>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#kubespray-vs-cluster-api","title":"Kubespray vs Cluster API","text":"<p>Although Welkin tries to be a platform which is as portable as possible, the two implementations of the Kubernetes-lifecycle layer differ quite a bit. As a Platform Administrator, you need to be aware of these differences.</p> <p>Welkin recommends using the Cluster API implementation when setting up a new Environment. Cluster API offers features such as Cluster autoscaling, Node self-healing and faster Kubernetes upgrades. On the downside, it requires a mature Cluster API provider. Not all infrastructure providers have a mature Cluster API provider. See ADR-0033 Run Cluster API Controllers on Management Cluster for more implementation details.</p> <p>Therefore, Welkin Environments can also be set up using Kubespray. Kubespray runs on pretty much any infrastructure you throw at it. However, it lacks support for Cluster autoscaling and Kubernetes upgrades are slower (albeit reliable).</p> <p>For both Cluster API and Kubespray, the infrastructure provider might come with its own limitations, such as lack of load-balancers and/or of block storage which integrate with Kubernetes. The provider audit is a systematic way to discover and understand these differences, so as to configure Welkin properly.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#configuration-repository","title":"Configuration Repository","text":"<p>Welkin abides to the configuration-as-code and infrastructure-as-code principles. This makes it easy to integrate Welkin with your existing CI/CD end embrace full GitOps. In essence:</p> <ul> <li>All Welkin configuration is stored in text files (most frequently YAML). Welkin expects you to stores these files in a git repository, which is why we refer to Welkin configuration as configuration repository.</li> <li>All Welkin operations are done via a command-line interface (CLI) which requires no graphical interface. The commands can run either on the platform administrator's laptop or in a CI/CD pipeline.</li> </ul> <p>A typical configuration repository looks as follows:</p> <pre><code>my-welkin-environment\n|-- adminlog\n|   `-- changes\n|   `-- incidents\n|   `-- maintenance\n|-- backups\n|-- capi\n|   |-- defaults\n|   |   `-- values.yaml\n|   |-- clusterctl-config.yaml\n|   |-- secrets.yaml\n|   `-- values.yaml\n|-- common-config.yaml\n|-- ck8s-cluster-api\n|-- compliantkubernetes-apps\n|-- compliantkubernetes-kubespray\n|-- defaults\n|   |-- common-config.yaml\n|   |-- sc-config.yaml\n|   `-- wc-config.yaml\n|-- README.md\n|-- sc-config\n|   |-- group_vars\n|   |   |-- all\n|   |   |   |-- ck8s-kubespray-general.yaml\n|   |   |   `-- ck8s-ssh-keys.yaml\n|   |   |-- etcd\n|   |   |   `-- ck8s-etcd.yaml\n|   |   `-- k8s_cluster\n|   |       |-- ck8s-k8s-cluster-default.yaml\n|   |       `-- ck8s-k8s-cluster.yaml\n|   `-- inventory.ini\n|-- sc-config.yaml\n|-- secrets.yaml\n|-- .sops.yaml\n|-- .state\n|   |-- kube_config_sc.yaml\n|   |-- kube_config_wc.yaml\n|   `-- s3cfg.ini\n|-- wc-config\n|   |-- group_vars\n|   |   |-- all\n|   |   |   |-- ck8s-kubespray-general.yaml\n|   |   |   `-- ck8s-ssh-keys.yaml\n|   |   |-- etcd\n|   |   |   `-- ck8s-etcd.yaml\n|   |   `-- k8s_cluster\n|   |       |-- ck8s-k8s-cluster-default.yaml\n|   |       `-- ck8s-k8s-cluster.yaml\n|   `-- inventory.ini\n`-- wc-config.yaml\n</code></pre> <p>Let us dive into the role of each of these files in the order you would commonly look at them:</p> <ul> <li><code>README.md</code> is a human description of the environment. This file is optional and ignored by Welkin. We recommend describing here the purpose of the Environment, what kind of applications it hosts, what infrastructure provider it runs on and any noteworthy deviations.</li> <li><code>adminlogs</code> is a folder where you can put text files (Markdown) describing operations you performed against this environment. This folder is optional and ignored by Welkin. We recommend having separate folders for changes, incidents and maintenance. See more on writing administrator logs in a later section.</li> <li><code>.sops.yaml</code> contains sops configuration, used by Welkin to encrypt secrets. Welkin makes sure to decrypt secrets when it needs them.</li> <li><code>ck8s-cluster-api</code>, <code>compliantkubernetes-apps</code> and <code>compliantkubernetes-kubespray</code> are the git submodules of the Cluster API, Apps and Kubespray layer, respectively. These contain the command-line tools to operate Welkin.</li> </ul> <p>The Welkin Kubespray layer initializes and reads the following configuration files and folders:</p> <ul> <li><code>sc-config</code> and <code>wc-config</code> store configuration for the Service Cluster and Workload Cluster, respectively. These folders are consumed by Ansible, which is part of this layer. If <code>group_vars</code>, <code>all</code> and <code>inventory.ini</code> look new to you, we recommend you learn more about Ansible.</li> </ul> <p>The Welkin Cluster API layer initializes and reads the following configuration files and folders:</p> <ul> <li><code>capi/defaults</code> is a folder which contains the default <code>capi/values.yaml</code> for the infrastructure provider and flavor you chose when you initialized the configuration repository using Welkin. Do not change these files, as they may be overridden by Welkin. Instead, override configuration values with the files described below.</li> <li><code>capi/secrets.yaml</code> contains secrets. This file is encrypted using the information in <code>.sops.yaml</code>. You should only edit this file using sops to make sure secrets never end up in plain text in the configuration repository.</li> <li><code>capi/values.yaml</code> contains override configuration.</li> </ul> <p>The Welkin Apps layer initializes and reads the following configuration files and folders:</p> <ul> <li><code>backups</code> is a folder in which Welkin stores copies of previous configurations. See this as a convenience, given that all configuration is already in git. Note that these Welkin configuration backups are unrelated to application or platform data backups.</li> <li><code>defaults</code> is a folder which contains the default <code>common-config.yaml</code>, <code>sc-config.yaml</code> and <code>wc-config.yaml</code> for the infrastructure provider and flavor you chose when you initialized the configuration repository using Welkin. Do not change these files, as they may be overridden by Welkin. Instead, override configuration values with the files described below.</li> <li><code>common-config.yaml</code> contains override configuration common to both the Service Cluster and the Workload Cluster.</li> <li><code>wc-config.yaml</code> and <code>sc-config.yaml</code> contains override configuration specific to the Service Cluster and the Workload Cluster, respectively.</li> <li><code>secrets.yaml</code> contains secrets, both for the Workload Cluster and Service Cluster. This file is encrypted using the information in <code>.sops.yaml</code>. You should only edit this file using sops to make sure secrets never end up in plain text in the configuration repository.</li> </ul> <p>Except for <code>init</code> sub-commands and configuration migration steps, the files above are only consumed by Welkin. They only take as input explicit choices that you made and describe how an Environment should be.</p> <p>The following files and folders are stored by Welkin in the configuration repository and contain information from an Environment:</p> <ul> <li><code>.state</code> is a folder which contains the \"state\", i.e., files produced by Welkin needed to access an Environment during daily operations, in particular kubeconfig and S3 object storage access. Typically, these files should be encrypted with sops for information security purposes. In Welkin, kubeconfigs generally don't contain credentials, only pointers to OpenID configuration, hence are usually not encrypted.</li> </ul> <p>To make sure your whole team knows exactly which version of Welkin an Environment runs, it is common practice to add the source code, in the example above <code>compliantkubernetes-apps</code> and <code>compliantkubernetes-kubespray</code>, as git submodules. An alternative is to pack all Welkin source code in a Docker image.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#default-configuration-flavors-and-providers","title":"Default Configuration: Flavors and Providers","text":"<p>To make it easy to get started, Welkin supports several default configurations depending on the flavor, provider and installer:</p> <ul> <li>Flavor (<code>CK8S_FLAVOR</code>) is either <code>prod</code>, <code>dev</code> or <code>air-gapped</code>.<ul> <li>The <code>prod</code> flavor is designed for most production environments.</li> <li>The <code>dev</code> flavor tries to have a smaller footprint. It is designed for contributors -- people developing Welkin -- and might not be stable enough for application development environments.</li> <li>The <code>air-gapped</code> flavor is designed for air-gapped networks.</li> </ul> </li> <li>Provider (<code>CK8S_CLOUD_PROVIDER</code>) is the name of the underlying infrastructure provider, such as <code>baremetal</code> or <code>openstack</code>.</li> <li>Installer (<code>CK8S_K8S_INSTALLER</code>) is the name Kubernetes-lifecycle layer, <code>capi</code> for Cluster API or <code>kubespray</code> for Kubespray.</li> </ul>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#what-does-it-mean-to-use-welkin-as-a-platform-administrator-on-a-daily-basis","title":"What does it mean to use Welkin as a Platform Administrator on a daily basis?","text":"<p>You must specify a configuration repository in the environment variable <code>CK8S_CONFIG_PATH</code>. Then you can interact with Welkin via commands such as <code>ck8s</code> and <code>ck8s-kubespray</code> provided in the source code. A typical usage of these commands is described later in this guide.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#writing-administrator-logs","title":"Writing administrator logs","text":"<p>We highly recommend writing admin logs to capture any changes made to a Welkin environment. These logs should summarize:</p> <ul> <li>What happened</li> <li>Why it was done</li> <li>The steps taken</li> </ul> <p>Admin logs serves as a reference for future you and your team, offering guidance when repeating similar changes or for troubleshooting.</p> <p>We recommend separating admin logs into three different categories:</p> <ul> <li>Maintenance - Document steps done during a scheduled maintenance e.g. Kubespray or Welkin Apps upgrade</li> <li>Change - Document steps done for changes done outside of scheduled maintenance, e.g. configuration changes</li> <li>Incident - Describe the incident and the steps taken to resolve it</li> </ul> <p>When operating on a Cluster, we recommend capturing terminal output to keep a detailed log of what steps were taken to solve e.g. an incident. This can be done with tools such as <code>script</code>.</p> <p>Important</p> <p>Since the terminal output captured may contain sensitive information, it is also strongly recommended to encrypt the output file if it is planned to be stored in a Git repository or similar.</p> <p>Templates for admin logs can be found here which can be used and adapted to your needs.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/understand-welkin/#next-steps","title":"Next Steps","text":"<p>Now that you have a good understanding for Welkin, proceed with a provider audit to understand the capabilities of the underlying infrastructure provider and determine how to best configure Welkin.</p>","tags":["ISO 27001 Annex A 8.9 Configuration Management","ISO 27001 Annex A 8.32 Change Management"]},{"location":"operator-manual/user-alerts/","title":"Enabling user alerts","text":"<p>This is administrator-facing documentation associated with this user guide. Please read that one first.</p> <p>Important</p> <p>Alertmanager should not be accessed via an Ingress, since this circumvents access control and audit logs. Please find the appropriate access method in the user guide linked above.</p> <p>Ensure the following configuration changes are set in <code>wc-config.yaml</code>:</p> <ol> <li>Ensure prometheus.devAlertmanager.enabled is true.</li> <li>Ensure prometheus.devAlertmanager.ingressEnabled is false.</li> </ol> <p>Then apply Welkin Apps.</p>"},{"location":"operator-manual/user-alerts/#example","title":"Example","text":"<p>Please find below an example taken from <code>wc-config.yaml</code>:</p> <pre><code>prometheus:\n ## Application developer controlled alertmanager configuration.\n  devAlertmanager:\n    enabled: true\n    ## Create basic-auth protected ingress to alertmanager\n    ingressEnabled: false\n</code></pre>"},{"location":"operator-manual/user-managed-crds/","title":"Self-managed Cluster resources (Preview)","text":"<p>Adding certain Cluster-wide resources, such as CustomResourceDefinitions, are Cluster-wide settings that are prohibited by Welkin for security purposes. However, many popular applications, in particular ones with Operators, require Cluster-wide resources to be installed. This preview feature enables application developers to self-manage a predefined set of such Cluster-wide resources. This trade-off means that application developers get the ability to install and manage such popular applications, but without compromising the security posture of the Cluster. By enabling this feature, a predefined set of Cluster-wide resources is allowed to be installed. In addition to CustomResourceDefinitions, other Cluster-wide resources, such as the required ClusterRoles and ServiceAccounts are installed. The list of currently supported applications for self-management is:</p> <ul> <li>SealedSecrets</li> <li>MongoDB</li> <li>Flux</li> <li>Kafka</li> <li>Jaeger</li> </ul>"},{"location":"operator-manual/user-managed-crds/#limitations","title":"Limitations","text":"<p>Welkin does not allow application developers to install ClusterRoles and ClusterRoleBindings. For pre-approved operators that require ClusterRoles and ClusterRoleBindings. They are installed during enabling of self-managed-CRDs feature. The installation of self-managed Cluster resources can be limited to a pre-defined Namespace. For example, Sealed Secrets is required to be installed in the namespace <code>sealed-secrets</code>. This to correctly bind a Cluster role to the service account.</p>"},{"location":"operator-manual/user-managed-crds/#enable-self-managed-cluster-wide-resources","title":"Enable self-managed Cluster-wide resources","text":"<p>By default, this feature is disabled. To enable it, add the following snippet to the environments <code>wc-config.yaml</code> file</p> <pre><code>user:\n  # Enable Sealed Secrets\n  sealedSecrets:\n    enabled: true\n  # Enable MongoDB\n  mongodb:\n    enabled: true\n  # Enable Flux\n  fluxv2:\n    enabled: true\n  # Enable Kafka\n  kafka:\n    enabled: true\n  # Enable Jaeger\n  jaeger:\n    enabled: true\n\ngatekeeper:\n  allowUserCRDs:\n    # Enable feature\n    enabled: true\n    enforcement: deny\n    # Add extra allowed CRDs\n    extraCRDs: []\n    # - names:\n    #     - sealedsecrets.bitnami.com\n    #   group: \"bitnami.com\"\n\n    # Add extra service accounts that needs access to CRDs\n    extraServiceAccounts: []\n    #  - namespace: \"gatekeeper-system\"\n    #    name: \"gatekeeper-admin-upgrade-crds\"\n</code></pre>"},{"location":"operator-manual/schema/config/","title":"Config","text":"<p>This table was generated from config.yaml.</p> <p>Cells marked with \"\u2014\" mean \"not specified in schema\".</p>"},{"location":"operator-manual/schema/config/#alerts","title":"<code>alerts</code>","text":"<p>Configure alerting.</p> Key Type Default Description alerts.alertTo\u00b6 string \u2014 \u2014 alerts.customReceivers[]\u00b6 array \u2014 See note alerts.customRoutes[]\u00b6 array \u2014 See note alerts.opsGenie\u00b6 object \u2014 Configure alerting to OpsGenie. alerts.opsGenie.apiUrl\u00b6 string <code>https://api.eu.opsgenie.com</code> \u2014 alerts.opsGenie.updateAlerts\u00b6 boolean \u2014 \u2014 alerts.opsGenieHeartbeat\u00b6 object \u2014 Configure heartbeats to OpsGenie. alerts.opsGenieHeartbeat.enabled\u00b6 boolean \u2014 \u2014 alerts.runbookUrls\u00b6 object \u2014 Configure runbooks for alertsRunbooks can be configured on an alert group level or per individual alert alerts.runbookUrls.alertmanager\u00b6 object \u2014 See note alerts.runbookUrls.alertmanager.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.backupStatus\u00b6 object \u2014 See note alerts.runbookUrls.backupStatus.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.blackbox\u00b6 object \u2014 See note alerts.runbookUrls.blackbox.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.certManager\u00b6 object \u2014 See note alerts.runbookUrls.certManager.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.clusterApi\u00b6 object \u2014 See note alerts.runbookUrls.clusterApi.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.clusterAutoscaler\u00b6 object \u2014 See note alerts.runbookUrls.clusterAutoscaler.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.clusterCapacityManagement\u00b6 object \u2014 See note alerts.runbookUrls.clusterCapacityManagement.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.configReloaders\u00b6 object \u2014 See note alerts.runbookUrls.configReloaders.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.coreDns\u00b6 object \u2014 See note alerts.runbookUrls.coreDns.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.dailyChecks\u00b6 object \u2014 See note alerts.runbookUrls.dailyChecks.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.diskPerf\u00b6 object \u2014 See note alerts.runbookUrls.diskPerf.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.falco\u00b6 object \u2014 See note alerts.runbookUrls.falco.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.fluentd\u00b6 object \u2014 See note alerts.runbookUrls.fluentd.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.general\u00b6 object \u2014 See note alerts.runbookUrls.general.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.harbor\u00b6 object \u2014 See note alerts.runbookUrls.harbor.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.hnc\u00b6 object \u2014 See note alerts.runbookUrls.hnc.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kubeStateMetrics\u00b6 object \u2014 See note alerts.runbookUrls.kubeStateMetrics.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kubernetesApps\u00b6 object \u2014 See note alerts.runbookUrls.kubernetesApps.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kubernetesResources\u00b6 object \u2014 See note alerts.runbookUrls.kubernetesResources.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kubernetesStorage\u00b6 object \u2014 See note alerts.runbookUrls.kubernetesStorage.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kubernetesSystem\u00b6 object \u2014 See note alerts.runbookUrls.kubernetesSystem.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.kured\u00b6 object \u2014 See note alerts.runbookUrls.kured.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.missingMetrics\u00b6 object \u2014 See note alerts.runbookUrls.missingMetrics.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.nodeExporter\u00b6 object \u2014 See note alerts.runbookUrls.nodeExporter.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.nodeNetwork\u00b6 object \u2014 See note alerts.runbookUrls.nodeNetwork.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.opensearch\u00b6 object \u2014 See note alerts.runbookUrls.opensearch.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.openstack\u00b6 object \u2014 See note alerts.runbookUrls.openstack.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.packetsDropped\u00b6 object \u2014 See note alerts.runbookUrls.packetsDropped.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.prometheus\u00b6 object \u2014 See note alerts.runbookUrls.prometheus.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.prometheusOperator\u00b6 object \u2014 See note alerts.runbookUrls.prometheusOperator.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.thanos\u00b6 object \u2014 See note alerts.runbookUrls.thanos.group\u00b6 string \u2014 \u2014 alerts.runbookUrls.webhook\u00b6 object \u2014 See note alerts.runbookUrls.webhook.group\u00b6 string \u2014 \u2014 alerts.slack\u00b6 object \u2014 Configure alerting to Slack. alerts.slack.customTemplate\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:alerts.customReceivers[]","title":"Notes for <code>alerts.customReceivers[]</code>","text":"<p>Additional receivers that will be added to the configuration of alertmanager</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.customRoutes[]","title":"Notes for <code>alerts.customRoutes[]</code>","text":"<p>Additional route receivers that will be added to the configuration of alertmanager</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.alertmanager","title":"Notes for <code>alerts.runbookUrls.alertmanager</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.backupStatus","title":"Notes for <code>alerts.runbookUrls.backupStatus</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.blackbox","title":"Notes for <code>alerts.runbookUrls.blackbox</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.certManager","title":"Notes for <code>alerts.runbookUrls.certManager</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.clusterApi","title":"Notes for <code>alerts.runbookUrls.clusterApi</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.clusterAutoscaler","title":"Notes for <code>alerts.runbookUrls.clusterAutoscaler</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.clusterCapacityManagement","title":"Notes for <code>alerts.runbookUrls.clusterCapacityManagement</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.configReloaders","title":"Notes for <code>alerts.runbookUrls.configReloaders</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.coreDns","title":"Notes for <code>alerts.runbookUrls.coreDns</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.dailyChecks","title":"Notes for <code>alerts.runbookUrls.dailyChecks</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.diskPerf","title":"Notes for <code>alerts.runbookUrls.diskPerf</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.falco","title":"Notes for <code>alerts.runbookUrls.falco</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.fluentd","title":"Notes for <code>alerts.runbookUrls.fluentd</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.general","title":"Notes for <code>alerts.runbookUrls.general</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.harbor","title":"Notes for <code>alerts.runbookUrls.harbor</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.hnc","title":"Notes for <code>alerts.runbookUrls.hnc</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kubeStateMetrics","title":"Notes for <code>alerts.runbookUrls.kubeStateMetrics</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kubernetesApps","title":"Notes for <code>alerts.runbookUrls.kubernetesApps</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kubernetesResources","title":"Notes for <code>alerts.runbookUrls.kubernetesResources</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kubernetesStorage","title":"Notes for <code>alerts.runbookUrls.kubernetesStorage</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kubernetesSystem","title":"Notes for <code>alerts.runbookUrls.kubernetesSystem</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.kured","title":"Notes for <code>alerts.runbookUrls.kured</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.missingMetrics","title":"Notes for <code>alerts.runbookUrls.missingMetrics</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.nodeExporter","title":"Notes for <code>alerts.runbookUrls.nodeExporter</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.nodeNetwork","title":"Notes for <code>alerts.runbookUrls.nodeNetwork</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.opensearch","title":"Notes for <code>alerts.runbookUrls.opensearch</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.openstack","title":"Notes for <code>alerts.runbookUrls.openstack</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.packetsDropped","title":"Notes for <code>alerts.runbookUrls.packetsDropped</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.prometheus","title":"Notes for <code>alerts.runbookUrls.prometheus</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.prometheusOperator","title":"Notes for <code>alerts.runbookUrls.prometheusOperator</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://runbooks.prometheus-operator.dev/runbooks/</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.thanos","title":"Notes for <code>alerts.runbookUrls.thanos</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses upstream runbooks by default</p> <p>https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:alerts.runbookUrls.webhook","title":"Notes for <code>alerts.runbookUrls.webhook</code>","text":"<p>Example:</p> <p>group: link-to-alert-group-runbook AlertName: link-to-specific-alert-runbook</p> <p>Uses no upstream runbook by default</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#certmanager","title":"<code>certmanager</code>","text":"<p>Configure cert-manager, used to provision certificates either self-signed or via Let's Encrypt.</p> Key Type Default Description certmanager.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. certmanager.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. certmanager.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). certmanager.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). certmanager.cainjector\u00b6 object \u2014 This is meant to describe the base class if you will, for Welkin resources. certmanager.cainjector.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. certmanager.cainjector.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. certmanager.cainjector.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). certmanager.cainjector.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). certmanager.cainjector.enabled\u00b6 boolean \u2014 \u2014 certmanager.cainjector.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container certmanager.cainjector.nodeSelector\u00b6 object \u2014 See note certmanager.cainjector.resources\u00b6 object \u2014 See note certmanager.cainjector.resources.limits\u00b6 object \u2014 \u2014 certmanager.cainjector.resources.requests\u00b6 object \u2014 \u2014 certmanager.cainjector.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration certmanager.cainjector.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. certmanager.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container certmanager.nodeSelector\u00b6 object \u2014 See note certmanager.resources\u00b6 object \u2014 See note certmanager.resources.limits\u00b6 object \u2014 \u2014 certmanager.resources.requests\u00b6 object \u2014 \u2014 certmanager.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration certmanager.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. certmanager.webhook\u00b6 object \u2014 This is meant to describe the base class if you will, for Welkin resources. certmanager.webhook.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. certmanager.webhook.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. certmanager.webhook.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). certmanager.webhook.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). certmanager.webhook.enabled\u00b6 boolean \u2014 \u2014 certmanager.webhook.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container certmanager.webhook.nodeSelector\u00b6 object \u2014 See note certmanager.webhook.resources\u00b6 object \u2014 See note certmanager.webhook.resources.limits\u00b6 object \u2014 \u2014 certmanager.webhook.resources.requests\u00b6 object \u2014 \u2014 certmanager.webhook.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration certmanager.webhook.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains."},{"location":"operator-manual/schema/config/#note:certmanager.cainjector.nodeSelector","title":"Notes for <code>certmanager.cainjector.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:certmanager.cainjector.resources","title":"Notes for <code>certmanager.cainjector.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:certmanager.nodeSelector","title":"Notes for <code>certmanager.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:certmanager.resources","title":"Notes for <code>certmanager.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:certmanager.webhook.nodeSelector","title":"Notes for <code>certmanager.webhook.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:certmanager.webhook.resources","title":"Notes for <code>certmanager.webhook.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#clusteradmin","title":"<code>clusterAdmin</code>","text":"<p>Configure the cluster admins.</p> Key Type Default Description clusterAdmin.groups[]\u00b6 array of string \u2014 Configure the cluster admin groups. clusterAdmin.users[]\u00b6 array of string \u2014 Configure the cluster admin users."},{"location":"operator-manual/schema/config/#clusterapi","title":"<code>clusterApi</code>","text":"<p>Set to true if kubernetes is installed with cluster-api.</p> Key Type Default Description clusterApi.clusters[]\u00b6 array of string \u2014 List of clusters to monitor.Used when monitoring clusters for autoscaling. clusterApi.enabled\u00b6 boolean \u2014 \u2014 clusterApi.monitoring\u00b6 object \u2014 Enable autoscaling monitoring of cluster API clusters. clusterApi.monitoring.enabled\u00b6 boolean \u2014 \u2014"},{"location":"operator-manual/schema/config/#dex","title":"<code>dex</code>","text":"<p>Configure Dex, the federated OIDC Identity Provider.</p> <p>Note</p> <p>Dex is installed in the service cluster, so this configuration mainly applies there.</p> Key Type Default Description dex.additionalKubeloginRedirects[]\u00b6 array of string \u2014 Configure Dex with additional Kubelogin redirects. dex.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. dex.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. dex.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). dex.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). dex.contentSecurityPolicy\u00b6 object \u2014 Configure Content-Security-Policy header rulesReference: https://content-security-policy.com/ dex.enableStaticLogin\u00b6 boolean <code>True</code> Configure Dex with a static password login <code>admin@example.com</code>. dex.expiry\u00b6 object \u2014 Configure expiry when authenticating with Dex. dex.expiry.deviceRequests\u00b6 string \u2014 See note dex.expiry.idToken\u00b6 string \u2014 See note dex.expiry.refreshTokens\u00b6 object \u2014 Configure expiry of refresh tokens when authenticating with Dex. dex.expiry.refreshTokens.absoluteLifetime\u00b6 string \u2014 See note dex.expiry.refreshTokens.reuseInterval\u00b6 string \u2014 See note dex.expiry.refreshTokens.validIfNotUsedFor\u00b6 string \u2014 See note dex.expiry.signingKeys\u00b6 string \u2014 See note dex.google\u00b6 object \u2014 Configure Dex with specific options when using the Google connector. dex.google.SASecretName\u00b6 string \u2014 \u2014 dex.google.groupSupport\u00b6 boolean \u2014 \u2014 dex.nodeSelector\u00b6 object \u2014 See note dex.replicaCount\u00b6 number <code>2</code> \u2014 dex.resources\u00b6 object \u2014 See note dex.resources.limits\u00b6 object \u2014 \u2014 dex.resources.requests\u00b6 object \u2014 \u2014 dex.serviceMonitor\u00b6 object \u2014 Configure the Service Monitor collecting metrics from Dex. dex.serviceMonitor.enabled\u00b6 boolean <code>True</code> \u2014 dex.subdomain\u00b6 string <code>dex</code> See note dex.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration dex.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains."},{"location":"operator-manual/schema/config/#note:dex.expiry.deviceRequests","title":"Notes for <code>dex.expiry.deviceRequests</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.expiry.idToken","title":"Notes for <code>dex.expiry.idToken</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.expiry.refreshTokens.absoluteLifetime","title":"Notes for <code>dex.expiry.refreshTokens.absoluteLifetime</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.expiry.refreshTokens.reuseInterval","title":"Notes for <code>dex.expiry.refreshTokens.reuseInterval</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.expiry.refreshTokens.validIfNotUsedFor","title":"Notes for <code>dex.expiry.refreshTokens.validIfNotUsedFor</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.expiry.signingKeys","title":"Notes for <code>dex.expiry.signingKeys</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.nodeSelector","title":"Notes for <code>dex.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.resources","title":"Notes for <code>dex.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:dex.subdomain","title":"Notes for <code>dex.subdomain</code>","text":"<p>Subdomain of <code>baseDomain</code> that the Ingress to Dex will be created with.</p> <p>Note</p> <p>Must be set for both service and workload clusters.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#externaldns","title":"<code>externalDns</code>","text":"<p>Configure External DNS.</p> <p>External DNS manages DNS records based on Kubernetes resources, and can automatically configure DNS records from:</p> <ul> <li>CRD resources</li> <li>Ingress resources</li> <li>Service resources</li> </ul> <p>Currently only AWS Route 53 is supported as the DNS provider.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> Key Type Default Description externalDns.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. externalDns.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. externalDns.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). externalDns.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). externalDns.domains[]\u00b6 array of string \u2014 Configure the domains External DNS should manage. externalDns.enabled\u00b6 boolean \u2014 \u2014 externalDns.endpoints[]\u00b6 array of object \u2014 See note externalDns.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container externalDns.logLevel\u00b6 string \u2014 See note externalDns.namespaced\u00b6 boolean \u2014 \u2014 externalDns.provider\u00b6 string \u2014 See note externalDns.resources\u00b6 object \u2014 See note externalDns.resources.limits\u00b6 object \u2014 \u2014 externalDns.resources.requests\u00b6 object \u2014 \u2014 externalDns.sources\u00b6 object \u2014 Configure the sources External DNS should manage DNS records for. externalDns.sources.crd\u00b6 boolean \u2014 \u2014 externalDns.sources.ingress\u00b6 boolean \u2014 \u2014 externalDns.sources.service\u00b6 boolean \u2014 \u2014 externalDns.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration externalDns.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. externalDns.txtPrefix\u00b6 string \u2014 Configure a prefix to TXT records.This is required with AWS Route 53 if CNAME records are preferred over A/AAAA records as it cannot handle both at the same time."},{"location":"operator-manual/schema/config/#note:externalDns.endpoints[]","title":"Notes for <code>externalDns.endpoints[]</code>","text":"<p>Configure the endpoints to create DNS records for.</p> <p>Requires <code>externalDns.sources.crd</code> to be enabled.</p> <p>Configure an endpoint to create a DNS record for.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:externalDns.logLevel","title":"Notes for <code>externalDns.logLevel</code>","text":"<p>Examples:</p> <pre><code>info\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:externalDns.provider","title":"Notes for <code>externalDns.provider</code>","text":"<p>Examples:</p> <pre><code>aws\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:externalDns.resources","title":"Notes for <code>externalDns.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#externaltrafficpolicy","title":"<code>externalTrafficPolicy</code>","text":"<p>Configure global ingress external traffic policy.</p> Key Type Default Description externalTrafficPolicy.local\u00b6 boolean <code>True</code> \u2014 externalTrafficPolicy.whitelistRange\u00b6 object \u2014 See note"},{"location":"operator-manual/schema/config/#note:externalTrafficPolicy.whitelistRange","title":"Notes for <code>externalTrafficPolicy.whitelistRange</code>","text":"<p>Configure allowlist CIDR ranges for ingresses.</p> <p>This is done via the ingress annotation <code>nginx.ingress.kubernetes.io/whitelist-source-range</code>.</p> <p>Set to <code>false</code> to explicitly opt-out of this annotation.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#falco","title":"<code>falco</code>","text":"<p>Configuration for Falco, runtime security tool and threat detection.</p> Key Type Default Description falco.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. falco.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. falco.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). falco.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). falco.alerts\u00b6 object \u2014 Configure Falco alerts sent from Falco sidekick. falco.alerts.enabled\u00b6 boolean \u2014 \u2014 falco.alerts.hostPort\u00b6 string <code>http://alertmanager-operated.monitoring:9093</code> Configure the notification channel for Falco alerts. falco.alerts.priority\u00b6 string <code>notice</code> Configure the notification priority for Falco alerts. falco.alerts.type\u00b6 string <code>alertmanager</code> See note falco.artifact\u00b6 object \u2014 Configure Falcoctl artefact management.See the upstream repository for reference. falco.artifact.install\u00b6 object \u2014 Configure Falcoctl artefact install. falco.artifact.install.enabled\u00b6 boolean \u2014 Configure Falcoctl to install additional artifacts before Falco starts.Set this to false in an air-gapped environment, unless artifacts are self-hosted and <code>customIndexes</code> are configured. falco.customIndexes[]\u00b6 array of object \u2014 Configure custom artefact indices for Falcoctl.Configure custom artefact index for Falcoctl. falco.customRules\u00b6 object \u2014 See note falco.driver\u00b6 object \u2014 Configuration for the Falco syscall driver used to collect events.See the upstream documentation for more information. falco.driver.kind\u00b6 string <code>kmod</code> See note falco.enabled\u00b6 boolean <code>True</code> \u2014 falco.falcoSidekick\u00b6 object \u2014 Basic configuration for Falco Sidekick, the deployment that forwards Falco alerts to Alertmanager. falco.falcoSidekick.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. falco.falcoSidekick.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. falco.falcoSidekick.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). falco.falcoSidekick.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). falco.falcoSidekick.nodeSelector\u00b6 object \u2014 See note falco.falcoSidekick.resources\u00b6 object \u2014 See note falco.falcoSidekick.resources.limits\u00b6 object \u2014 \u2014 falco.falcoSidekick.resources.requests\u00b6 object \u2014 \u2014 falco.falcoSidekick.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration falco.nodeSelector\u00b6 object \u2014 See note falco.resources\u00b6 object \u2014 See note falco.resources.limits\u00b6 object \u2014 \u2014 falco.resources.requests\u00b6 object \u2014 \u2014 falco.rulesFiles\u00b6 object \u2014 Configure standard rules to use in Falco.See the upstream documentation for reference. falco.rulesFiles.default\u00b6 object \u2014 Configure Falco default rules falco.rulesFiles.default.enabled\u00b6 boolean <code>True</code> \u2014 falco.rulesFiles.default.version\u00b6 string <code>3.0.1</code> \u2014 falco.rulesFiles.incubating\u00b6 object \u2014 Configure Falco incubating rules falco.rulesFiles.incubating.enabled\u00b6 boolean \u2014 \u2014 falco.rulesFiles.incubating.version\u00b6 string <code>3.0.1</code> \u2014 falco.rulesFiles.sandbox\u00b6 object \u2014 Configure Falco sandbox rules falco.rulesFiles.sandbox.enabled\u00b6 boolean \u2014 \u2014 falco.rulesFiles.sandbox.version\u00b6 string <code>3.0.1</code> \u2014 falco.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration falco.tty\u00b6 boolean <code>True</code> Attach the Falco process to a TTY inside the container.Needed to flush Falco logs as soon as they are emitted. falco.useContainerEngine\u00b6 boolean <code>True</code> Use the new container engine collector that replaces the old docker, containerd, crio and podman collectors."},{"location":"operator-manual/schema/config/#note:falco.alerts.type","title":"Notes for <code>falco.alerts.type</code>","text":"<p>Configure the notification channel for Falco alerts.</p> <p>Possible values:</p> <pre><code>alertmanager\n</code></pre> <pre><code>slack\n</code></pre> <pre><code>none\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.customRules","title":"Notes for <code>falco.customRules</code>","text":"<p>Configure custom rules to use in Falco.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>The keys will become the file name of the generated rule file, and all files are parsed in alphabetical order.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.driver.kind","title":"Notes for <code>falco.driver.kind</code>","text":"<p>Possible values:</p> <pre><code>kmod\n</code></pre> <pre><code>modern_ebpf\n</code></pre> <pre><code>ebpf\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.falcoSidekick.nodeSelector","title":"Notes for <code>falco.falcoSidekick.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.falcoSidekick.resources","title":"Notes for <code>falco.falcoSidekick.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.nodeSelector","title":"Notes for <code>falco.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:falco.resources","title":"Notes for <code>falco.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#fluentd","title":"<code>fluentd</code>","text":"<p>Configuration for Fluentd.</p> <p>Fluentd automatically collects logs from all containers running in the environment.</p> <p>In the service cluster audit, application, and platform logs can be shipped to object storage. In the workload cluster audit logs can be shipped to object storage and application and platform logs to OpenSearch running in the service cluster.</p> <p>Logs are collected using a daemon set, and in the workload cluster two sets are deployed, one for the system nodes and one for the worker nodes. Application developer can modify two ConfigMaps to add additional configuration and plugins to the set running on the worker nodes.</p> <p>When logs are shipped to object storage a stateful aggregator is deployed that buffers logs with persistence before they are shipped. When logs are shipped to OpenSearch it is done directly from the forwarder daemons.</p> <p>Shipping audit and service cluster logs requires that <code>objectStorage</code> is configured, and will use the bucket or container set in <code>objectStorage.buckets.audit</code> and <code>objectStorage.buckets.scLogs</code> respectively.</p> <p>Note</p> <p>Fluentd is installed in both service cluster and workload cluster, so this configuration applies there with some exceptions.</p> Key Type Default Description fluentd.aggregator\u00b6 object \u2014 Configure Fluentd aggregator, used to buffer logs with persistence before they are shipped to object storage. fluentd.aggregator.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. fluentd.aggregator.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. fluentd.aggregator.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). fluentd.aggregator.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). fluentd.aggregator.buffer\u00b6 object \u2014 See note fluentd.aggregator.buffer.chunkLimitSize\u00b6 string \u2014 See note fluentd.aggregator.buffer.flushInterval\u00b6 string \u2014 See note fluentd.aggregator.buffer.flushMode\u00b6 string \u2014 See note fluentd.aggregator.buffer.flushThreadBurstInterval\u00b6 number \u2014 See note fluentd.aggregator.buffer.flushThreadCount\u00b6 integer \u2014 The number of threads to flush/write chunks in parallel.Flushing parameters fluentd.aggregator.buffer.retryForever\u00b6 boolean \u2014 If true, plugin will ignore <code>retryTimeout</code> and <code>retryMaxTimes</code> options and retry flushing forever.Retries parameters fluentd.aggregator.buffer.retryMaxInterval\u00b6 integer \u2014 The maximum interval (seconds) for exponential backoff between retries while failing.Retries parameters fluentd.aggregator.buffer.retryType\u00b6 string \u2014 See note fluentd.aggregator.buffer.timekey\u00b6 string \u2014 See note fluentd.aggregator.buffer.timekeyUseUtc\u00b6 boolean \u2014 Output plugin decides to use UTC or not to format placeholders using timekey.Common/Time  parameters fluentd.aggregator.buffer.timekeyWait\u00b6 string \u2014 See note fluentd.aggregator.buffer.totalLimitSize\u00b6 string \u2014 See note fluentd.aggregator.nodeSelector\u00b6 object \u2014 See note fluentd.aggregator.persistence\u00b6 object \u2014 Configure Fluentd aggregator persistence. fluentd.aggregator.persistence.storage\u00b6 string <code>10Gi</code> \u2014 fluentd.aggregator.resources\u00b6 object \u2014 See note fluentd.aggregator.resources.limits\u00b6 object \u2014 \u2014 fluentd.aggregator.resources.requests\u00b6 object \u2014 \u2014 fluentd.aggregator.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration fluentd.audit\u00b6 object \u2014 Configure Fluentd audit log collection. fluentd.audit.compaction\u00b6 object \u2014 Configure the compaction of logs stored in object storage. fluentd.audit.compaction.days\u00b6 number \u2014 Configure the days to consider for compaction or the days to retain. fluentd.audit.compaction.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.audit.compaction.ephemeralVolumes\u00b6 object \u2014 Configure the job to run with an ephemeral volume if the nodes risk running out of storage. fluentd.audit.compaction.ephemeralVolumes.enabled\u00b6 boolean \u2014 \u2014 fluentd.audit.compaction.schedule\u00b6 string \u2014 \u2014 fluentd.audit.enabled\u00b6 boolean \u2014 \u2014 fluentd.audit.filters\u00b6 string \u2014 Configure Fluentd audit log filter stages.To capture audit logs label the logs with the <code>@AUDIT</code> label. fluentd.audit.retention\u00b6 object \u2014 Configure the retention of logs stored in object storage. fluentd.audit.retention.days\u00b6 number \u2014 Configure the days to consider for compaction or the days to retain. fluentd.audit.retention.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.audit.retention.schedule\u00b6 string \u2014 \u2014 fluentd.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.extraConfigMaps\u00b6 object \u2014 See note fluentd.forwarder\u00b6 object \u2014 Configure Fluentd forwarder, used to collect and forward logs on system nodes. fluentd.forwarder.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. fluentd.forwarder.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. fluentd.forwarder.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). fluentd.forwarder.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). fluentd.forwarder.buffer\u00b6 object \u2014 See note fluentd.forwarder.buffer.chunkLimitSize\u00b6 string \u2014 See note fluentd.forwarder.buffer.flushInterval\u00b6 string \u2014 See note fluentd.forwarder.buffer.flushMode\u00b6 string \u2014 See note fluentd.forwarder.buffer.flushThreadBurstInterval\u00b6 number \u2014 See note fluentd.forwarder.buffer.flushThreadCount\u00b6 integer \u2014 The number of threads to flush/write chunks in parallel.Flushing parameters fluentd.forwarder.buffer.retryForever\u00b6 boolean \u2014 If true, plugin will ignore <code>retryTimeout</code> and <code>retryMaxTimes</code> options and retry flushing forever.Retries parameters fluentd.forwarder.buffer.retryMaxInterval\u00b6 integer \u2014 The maximum interval (seconds) for exponential backoff between retries while failing.Retries parameters fluentd.forwarder.buffer.retryType\u00b6 string \u2014 See note fluentd.forwarder.buffer.timekey\u00b6 string \u2014 See note fluentd.forwarder.buffer.timekeyUseUtc\u00b6 boolean \u2014 Output plugin decides to use UTC or not to format placeholders using timekey.Common/Time  parameters fluentd.forwarder.buffer.timekeyWait\u00b6 string \u2014 See note fluentd.forwarder.buffer.totalLimitSize\u00b6 string \u2014 See note fluentd.forwarder.image\u00b6 object \u2014 Configure Fluentd forwarder image repository and tag fluentd.forwarder.image.repository\u00b6 string <code>ghcr.io/elastisys/fluentd-forwarder</code> \u2014 fluentd.forwarder.image.tag\u00b6 string <code>v4.7.5-ck8s1</code> \u2014 fluentd.forwarder.livenessThresholdSeconds\u00b6 number <code>900</code> \u2014 fluentd.forwarder.nodeSelector\u00b6 object \u2014 See note fluentd.forwarder.requestTimeout\u00b6 string <code>60s</code> \u2014 fluentd.forwarder.resources\u00b6 object \u2014 See note fluentd.forwarder.resources.limits\u00b6 object \u2014 \u2014 fluentd.forwarder.resources.requests\u00b6 object \u2014 \u2014 fluentd.forwarder.stuckThresholdSeconds\u00b6 number <code>1200</code> \u2014 fluentd.forwarder.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration fluentd.logManager\u00b6 object \u2014 Configure log-manager, used to manage compaction and retention of logs store in object storage. fluentd.logManager.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. fluentd.logManager.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. fluentd.logManager.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). fluentd.logManager.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). fluentd.logManager.compaction\u00b6 object \u2014 Configure log-manager compaction. fluentd.logManager.compaction.azureCopyBufferGB\u00b6 number \u2014 Configure the memory buffer size in GB (accepts decimals) for Azure copy operations. fluentd.logManager.compaction.azureCopyConcurrency\u00b6 number \u2014 Configure the maximum number of concurrent download requests for Azure copy operations. fluentd.logManager.compaction.resources\u00b6 object \u2014 See note fluentd.logManager.compaction.resources.limits\u00b6 object \u2014 \u2014 fluentd.logManager.compaction.resources.requests\u00b6 object \u2014 \u2014 fluentd.logManager.compaction.volume\u00b6 object \u2014 Configure log-manager compaction volume. fluentd.logManager.compaction.volume.storage\u00b6 string <code>5Gi</code> Configure log-manager compaction volume size. fluentd.logManager.nodeSelector\u00b6 object \u2014 See note fluentd.logManager.retention\u00b6 object \u2014 Configure log-manager retention. fluentd.logManager.retention.resources\u00b6 object \u2014 See note fluentd.logManager.retention.resources.limits\u00b6 object \u2014 \u2014 fluentd.logManager.retention.resources.requests\u00b6 object \u2014 \u2014 fluentd.logManager.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration fluentd.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to Fluentd. fluentd.objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. fluentd.objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. fluentd.objectStorage.s3.region\u00b6 string \u2014 Region to store data. fluentd.objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). fluentd.objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. fluentd.scLogs\u00b6 object \u2014 Configure Fluentd service cluster log collection. fluentd.scLogs.compaction\u00b6 object \u2014 Configure the compaction of logs stored in object storage. fluentd.scLogs.compaction.days\u00b6 number \u2014 Configure the days to consider for compaction or the days to retain. fluentd.scLogs.compaction.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.scLogs.compaction.ephemeralVolumes\u00b6 object \u2014 Configure the job to run with an ephemeral volume if the nodes risk running out of storage. fluentd.scLogs.compaction.ephemeralVolumes.enabled\u00b6 boolean \u2014 \u2014 fluentd.scLogs.compaction.schedule\u00b6 string \u2014 \u2014 fluentd.scLogs.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.scLogs.retention\u00b6 object \u2014 Configure the retention of logs stored in object storage. fluentd.scLogs.retention.days\u00b6 number \u2014 Configure the days to consider for compaction or the days to retain. fluentd.scLogs.retention.enabled\u00b6 boolean <code>True</code> \u2014 fluentd.scLogs.retention.schedule\u00b6 string \u2014 \u2014 fluentd.user\u00b6 object \u2014 Configure Fluentd forwarder, used to collect and forward logs on worker nodes that applications developers run their workload on. fluentd.user.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. fluentd.user.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. fluentd.user.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). fluentd.user.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). fluentd.user.nodeSelector\u00b6 object \u2014 See note fluentd.user.resources\u00b6 object \u2014 See note fluentd.user.resources.limits\u00b6 object \u2014 \u2014 fluentd.user.resources.requests\u00b6 object \u2014 \u2014 fluentd.user.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer","title":"Notes for <code>fluentd.aggregator.buffer</code>","text":"<p>Fluentd buffer configuration parameters.</p> <p>Note</p> <p>See upstream documentation for reference, set keys will be converted from <code>camelCase</code> to <code>snake_case</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.chunkLimitSize","title":"Notes for <code>fluentd.aggregator.buffer.chunkLimitSize</code>","text":"<p>Events will be written into chunks until the size of chunks become <code>chunkLimitSize</code>.</p> <p>Buffering parameters</p> <p>Examples:</p> <pre><code>50MB\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.flushInterval","title":"Notes for <code>fluentd.aggregator.buffer.flushInterval</code>","text":"<p>Flushes the buffer each <code>flushInterval</code>, if <code>flushMode</code> is equal to <code>interval</code>.</p> <p>Flushing parameters</p> <p>Examples:</p> <pre><code>15m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.flushMode","title":"Notes for <code>fluentd.aggregator.buffer.flushMode</code>","text":"<p>The flush mode to use.</p> <p>Flushing parameters</p> <p>Possible values:</p> <pre><code>lazy\n</code></pre> <pre><code>interval\n</code></pre> <pre><code>immediate\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.flushThreadBurstInterval","title":"Notes for <code>fluentd.aggregator.buffer.flushThreadBurstInterval</code>","text":"<p>The sleep interval (seconds) for threads between flushes when the output plugin flushes the waiting chunks to the next ones.</p> <p>Flushing parameters</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.retryType","title":"Notes for <code>fluentd.aggregator.buffer.retryType</code>","text":"<p>The retry algorithm type to use.</p> <p>Retries parameters</p> <p>Possible values:</p> <pre><code>exponential_backoff\n</code></pre> <pre><code>periodic\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.timekey","title":"Notes for <code>fluentd.aggregator.buffer.timekey</code>","text":"<p>Output plugin will flush chunks per specified time (enabled when time is specified in chunk keys).</p> <p>Common/Time  parameters</p> <p>Examples:</p> <pre><code>10m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.timekeyWait","title":"Notes for <code>fluentd.aggregator.buffer.timekeyWait</code>","text":"<p>Output plugin will write chunks after timekey_wait seconds later after timekey expiration.</p> <p>If a user configures timekey 60m, output plugin will wait delayed events for flushed timekey and write the chunk at 10 minutes of each hour.</p> <p>Common/Time  parameters</p> <p>Examples:</p> <pre><code>1m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.buffer.totalLimitSize","title":"Notes for <code>fluentd.aggregator.buffer.totalLimitSize</code>","text":"<p>The size limitation of this buffer plugin instance.</p> <p>Once the total size of stored buffer reached this threshold, all append operations will fail with error (and data will be lost).</p> <p>Buffering parameters</p> <p>Examples:</p> <pre><code>9GB\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.nodeSelector","title":"Notes for <code>fluentd.aggregator.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.aggregator.resources","title":"Notes for <code>fluentd.aggregator.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.extraConfigMaps","title":"Notes for <code>fluentd.extraConfigMaps</code>","text":"<p>Configure extra ConfigMaps for Fluentd.</p> <p>Note</p> <p>This is only applicable for Fluentd forwarder running on system nodes in the workload cluster.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer","title":"Notes for <code>fluentd.forwarder.buffer</code>","text":"<p>Fluentd buffer configuration parameters.</p> <p>Note</p> <p>See upstream documentation for reference, set keys will be converted from <code>camelCase</code> to <code>snake_case</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.chunkLimitSize","title":"Notes for <code>fluentd.forwarder.buffer.chunkLimitSize</code>","text":"<p>Events will be written into chunks until the size of chunks become <code>chunkLimitSize</code>.</p> <p>Buffering parameters</p> <p>Examples:</p> <pre><code>50MB\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.flushInterval","title":"Notes for <code>fluentd.forwarder.buffer.flushInterval</code>","text":"<p>Flushes the buffer each <code>flushInterval</code>, if <code>flushMode</code> is equal to <code>interval</code>.</p> <p>Flushing parameters</p> <p>Examples:</p> <pre><code>15m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.flushMode","title":"Notes for <code>fluentd.forwarder.buffer.flushMode</code>","text":"<p>The flush mode to use.</p> <p>Flushing parameters</p> <p>Possible values:</p> <pre><code>lazy\n</code></pre> <pre><code>interval\n</code></pre> <pre><code>immediate\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.flushThreadBurstInterval","title":"Notes for <code>fluentd.forwarder.buffer.flushThreadBurstInterval</code>","text":"<p>The sleep interval (seconds) for threads between flushes when the output plugin flushes the waiting chunks to the next ones.</p> <p>Flushing parameters</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.retryType","title":"Notes for <code>fluentd.forwarder.buffer.retryType</code>","text":"<p>The retry algorithm type to use.</p> <p>Retries parameters</p> <p>Possible values:</p> <pre><code>exponential_backoff\n</code></pre> <pre><code>periodic\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.timekey","title":"Notes for <code>fluentd.forwarder.buffer.timekey</code>","text":"<p>Output plugin will flush chunks per specified time (enabled when time is specified in chunk keys).</p> <p>Common/Time  parameters</p> <p>Examples:</p> <pre><code>10m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.timekeyWait","title":"Notes for <code>fluentd.forwarder.buffer.timekeyWait</code>","text":"<p>Output plugin will write chunks after timekey_wait seconds later after timekey expiration.</p> <p>If a user configures timekey 60m, output plugin will wait delayed events for flushed timekey and write the chunk at 10 minutes of each hour.</p> <p>Common/Time  parameters</p> <p>Examples:</p> <pre><code>1m\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.buffer.totalLimitSize","title":"Notes for <code>fluentd.forwarder.buffer.totalLimitSize</code>","text":"<p>The size limitation of this buffer plugin instance.</p> <p>Once the total size of stored buffer reached this threshold, all append operations will fail with error (and data will be lost).</p> <p>Buffering parameters</p> <p>Examples:</p> <pre><code>9GB\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.nodeSelector","title":"Notes for <code>fluentd.forwarder.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.forwarder.resources","title":"Notes for <code>fluentd.forwarder.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.logManager.compaction.resources","title":"Notes for <code>fluentd.logManager.compaction.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.logManager.nodeSelector","title":"Notes for <code>fluentd.logManager.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.logManager.retention.resources","title":"Notes for <code>fluentd.logManager.retention.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.user.nodeSelector","title":"Notes for <code>fluentd.user.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:fluentd.user.resources","title":"Notes for <code>fluentd.user.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#gatekeeper","title":"<code>gatekeeper</code>","text":"<p>Configure OPA Gatekeeper to give application developer access to Custom Resource Definitions.</p> <p>Some preconfigured services can be found under the key <code>user</code>.</p> <p>Note</p> <p>See the admin docs for context.</p> Key Type Default Description gatekeeper.allowUserCRDs\u00b6 object \u2014 Configure access to Custom Resource Definitions for application developers. gatekeeper.allowUserCRDs.adminConfUser\u00b6 string <code>kubernetes-admin</code> Configure the admin config user of the <code>/etc/kubernetes/admin.conf</code> found on the control plane nodes.This is necessary if Kubespray is used for managing the cluster. gatekeeper.allowUserCRDs.enabled\u00b6 boolean \u2014 \u2014 gatekeeper.allowUserCRDs.enforcement\u00b6 string <code>deny</code> See note gatekeeper.allowUserCRDs.extraCRDs[]\u00b6 array of object \u2014 Configure extra CRDs to allow for application developers.Configure extra CRDs to allow for application developers. gatekeeper.allowUserCRDs.extraServiceAccounts[]\u00b6 array of object \u2014 See note gatekeeper.enabled\u00b6 boolean <code>True</code> \u2014"},{"location":"operator-manual/schema/config/#note:gatekeeper.allowUserCRDs.enforcement","title":"Notes for <code>gatekeeper.allowUserCRDs.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:gatekeeper.allowUserCRDs.extraServiceAccounts[]","title":"Notes for <code>gatekeeper.allowUserCRDs.extraServiceAccounts[]</code>","text":"<p>Configure extra service accounts to allow access to configured CRDs.</p> <p>Configure an extra service account to allow access to configured CRDs.</p> <p>Examples:</p> <pre><code>[{'namespace': 'example-namespace', 'name': 'example-controller'}]\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#global","title":"<code>global</code>","text":"<p>Some common options used in various helm charts.</p> Key Type Default Description global.baseDomain\u00b6 string \u2014 See note global.ck8sCloudProvider\u00b6 string \u2014 See note global.ck8sConfigSerial\u00b6 string \u2014 See note global.ck8sEnvironmentName\u00b6 string \u2014 See note global.ck8sFlavor\u00b6 string \u2014 See note global.ck8sK8sInstaller\u00b6 string \u2014 See note global.ck8sVersion\u00b6 string \u2014 See note global.clusterDns\u00b6 string <code>10.233.0.3</code> IP of the cluster DNS in kubernetes global.clusterName\u00b6 string \u2014 \u2014 global.clustersMonitoring[]\u00b6 array of string \u2014 Configure the names of the workload clusters that sends metrics to the service cluster.Mainly used to filter metrics. global.containerRuntime\u00b6 string <code>containerd</code> See note global.enforceIPFamilies\u00b6 boolean \u2014 Enforce ipFamilyPolicy to all services that doesn't explicitly set it.This is done using a mutating webhook to all services that doesn't set this.The value it sets is taken from <code>.global.ipFamilies</code> global.enforceIPFamilyPolicy\u00b6 boolean \u2014 See note global.ipFamilies[]\u00b6 array of string <code>['IPv4']</code> Used to set the ipFamilyPolicy for all configurable services. global.ipFamilyPolicy\u00b6 string <code>SingleStack</code> See note global.issuer\u00b6 string <code>letsencrypt-staging</code> See note global.opsDomain\u00b6 string \u2014 See note global.scDomain\u00b6 string \u2014 If baseDomain for wc and sc are not the same, set the domain of the sc cluster. global.scOpsDomain\u00b6 string \u2014 If opsDomain for wc and sc are not the same, set the ops domain of the sc cluster. global.verifyTls\u00b6 boolean <code>True</code> Verify ingress certificates"},{"location":"operator-manual/schema/config/#note:global.baseDomain","title":"Notes for <code>global.baseDomain</code>","text":"<p>Domain intended for ingress usage in the workload cluster and to reach application developer facing services such as Grafana, Harbor and OpenSearch Dashboards. E.g. with 'prod.domain.com', OpenSearch Dashboards is reached via 'opensearch.prod.domain.com'.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sCloudProvider","title":"Notes for <code>global.ck8sCloudProvider</code>","text":"<p>Possible values:</p> <pre><code>aws\n</code></pre> <pre><code>azure\n</code></pre> <pre><code>baremetal\n</code></pre> <pre><code>citycloud\n</code></pre> <pre><code>elastx\n</code></pre> <pre><code>exoscale\n</code></pre> <pre><code>none\n</code></pre> <pre><code>safespring\n</code></pre> <pre><code>upcloud\n</code></pre> <pre><code>openstack\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sConfigSerial","title":"Notes for <code>global.ck8sConfigSerial</code>","text":"<p>This property is used during migrations to track state and ensure that the same version is used during <code>ck8s upgrade prepare</code> as during <code>ck8s upgrade apply</code>.</p> <p>Examples:</p> <pre><code>2025-04-29T08:34:21+00:00\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sEnvironmentName","title":"Notes for <code>global.ck8sEnvironmentName</code>","text":"<p>Examples:</p> <pre><code>my-welkin-cluster\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sFlavor","title":"Notes for <code>global.ck8sFlavor</code>","text":"<p>Possible values:</p> <pre><code>prod\n</code></pre> <pre><code>dev\n</code></pre> <pre><code>air-gapped\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sK8sInstaller","title":"Notes for <code>global.ck8sK8sInstaller</code>","text":"<p>Possible values:</p> <pre><code>capi\n</code></pre> <pre><code>kubespray\n</code></pre> <pre><code>none\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ck8sVersion","title":"Notes for <code>global.ck8sVersion</code>","text":"<p>Use version number if you are exactly at a release tag. Otherwise use full commit hash of current commit. <code>any</code>, can be used to disable this validation.</p> <p>Examples:</p> <pre><code>v0.42.1\n</code></pre> <pre><code>any\n</code></pre> <pre><code>424442541a567646c232d949bad1af2b5b7cb885\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.containerRuntime","title":"Notes for <code>global.containerRuntime</code>","text":"<p>Possible values:</p> <pre><code>containerd\n</code></pre> <pre><code>docker\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.enforceIPFamilyPolicy","title":"Notes for <code>global.enforceIPFamilyPolicy</code>","text":"<p>Enforce ipFamilyPolicy to all services that doesn't explicitly set it. This is done using a mutating webhook to all services that doesn't set this. The value it sets is taken from <code>.global.ipFamilyPolicy</code></p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.ipFamilyPolicy","title":"Notes for <code>global.ipFamilyPolicy</code>","text":"<p>Used to set the ipFamilyPolicy for all configurable services.</p> <p>Examples:</p> <pre><code>SingleStack\n</code></pre> <pre><code>PreferDualStack\n</code></pre> <pre><code>RequireDualStack\n</code></pre> <p>Possible values:</p> <pre><code>SingleStack\n</code></pre> <pre><code>PreferDualStack\n</code></pre> <pre><code>RequireDualStack\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.issuer","title":"Notes for <code>global.issuer</code>","text":"<p>Default cert-manager issuer to use for issuing certificates for ingresses. Normally one of <code>letsencrypt-staging</code> or <code>letsencrypt-prod</code>.</p> <p>Examples:</p> <pre><code>letsencrypt-staging\n</code></pre> <pre><code>letsencrypt-prod\n</code></pre> <pre><code>selfsigned\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:global.opsDomain","title":"Notes for <code>global.opsDomain</code>","text":"<p>Domain intended for ingress usage in the service cluster and to reach non-user facing services such as Thanos and OpenSearch. E.g. with 'ops.prod.domain.com', OpenSearch is reached via 'opensearch.ops.prod.domain.com'.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#gpu","title":"<code>gpu</code>","text":"<p>Configure the GPU Operator and its dependencies</p> Key Type Default Description gpu.daemonsets\u00b6 object \u2014 Configure GPU Daemonsets gpu.daemonsets.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration gpu.devicePlugin\u00b6 object \u2014 Configuration for the device plugin, e.g. timeslicing gpu.enabled\u00b6 boolean \u2014 \u2014 gpu.extraMetrics\u00b6 boolean \u2014 Adds some profiling metrics in DCGM if it's available in your GPU setup gpu.mig\u00b6 object \u2014 Configure MIG options like strategy gpu.mig.strategy\u00b6 string \u2014 See note gpu.nodeFeatureDiscovery\u00b6 object \u2014 Configure Node Feature Discovery gpu.nodeFeatureDiscovery.controlPlane\u00b6 object \u2014 Configure Node Feature Discovery Control Plane gpu.nodeFeatureDiscovery.controlPlane.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. gpu.nodeFeatureDiscovery.controlPlane.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. gpu.nodeFeatureDiscovery.controlPlane.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). gpu.nodeFeatureDiscovery.controlPlane.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). gpu.nodeFeatureDiscovery.controlPlane.resources\u00b6 object \u2014 See note gpu.nodeFeatureDiscovery.controlPlane.resources.limits\u00b6 object \u2014 \u2014 gpu.nodeFeatureDiscovery.controlPlane.resources.requests\u00b6 object \u2014 \u2014 gpu.nodeFeatureDiscovery.controlPlane.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration gpu.nodeFeatureDiscovery.worker\u00b6 object \u2014 Configure Node Feature Discovery workers gpu.nodeFeatureDiscovery.worker.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. gpu.nodeFeatureDiscovery.worker.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. gpu.nodeFeatureDiscovery.worker.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). gpu.nodeFeatureDiscovery.worker.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). gpu.nodeFeatureDiscovery.worker.resources\u00b6 object \u2014 See note gpu.nodeFeatureDiscovery.worker.resources.limits\u00b6 object \u2014 \u2014 gpu.nodeFeatureDiscovery.worker.resources.requests\u00b6 object \u2014 \u2014 gpu.nodeFeatureDiscovery.worker.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration gpu.operator\u00b6 object \u2014 Configure GPU Operator gpu.operator.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. gpu.operator.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. gpu.operator.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). gpu.operator.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). gpu.operator.resources\u00b6 object \u2014 See note gpu.operator.resources.limits\u00b6 object \u2014 \u2014 gpu.operator.resources.requests\u00b6 object \u2014 \u2014 gpu.operator.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:gpu.mig.strategy","title":"Notes for <code>gpu.mig.strategy</code>","text":"<p>None ignores MIG entirely, single makes MIG devices a standard GPU resource, and shared creates one resource type for each MIG configuration</p> <p>Possible values:</p> <pre><code>mixed\n</code></pre> <pre><code>single\n</code></pre> <pre><code>none\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:gpu.nodeFeatureDiscovery.controlPlane.resources","title":"Notes for <code>gpu.nodeFeatureDiscovery.controlPlane.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:gpu.nodeFeatureDiscovery.worker.resources","title":"Notes for <code>gpu.nodeFeatureDiscovery.worker.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:gpu.operator.resources","title":"Notes for <code>gpu.operator.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#grafana","title":"<code>grafana</code>","text":"<p>Configure Grafana, the metrics visualisation dashboard.</p> <p>Welkin hosts two instances of Grafana one for the Platform Administrator and one for the Application Developer.</p> <p>Note</p> <p>Grafana is installed in the service cluster, so this configuration mainly applies there.</p> Key Type Default Description grafana.ops\u00b6 object \u2014 Configure Grafana. grafana.ops.additionalConfigValues\u00b6 string \u2014 \u2014 grafana.ops.additionalDatasources\u00b6 object \u2014 \u2014 grafana.ops.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. grafana.ops.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. grafana.ops.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). grafana.ops.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). grafana.ops.contentSecurityPolicy\u00b6 object \u2014 Configure Content-Security-Policy header rulesReference: https://content-security-policy.com/ grafana.ops.dataproxy\u00b6 object \u2014 Configure Grafana dataproxy values grafana.ops.dataproxy.timeout\u00b6 number <code>600</code> \u2014 grafana.ops.enabled\u00b6 boolean <code>True</code> \u2014 grafana.ops.nodeSelector\u00b6 object \u2014 See note grafana.ops.oidc\u00b6 object \u2014 Configure authentication to Grafana via Dex. grafana.ops.oidc.allowedDomains[]\u00b6 array of string \u2014 Configure the domains of the users allowed to authenticate to Grafana. grafana.ops.oidc.enabled\u00b6 boolean <code>True</code> \u2014 grafana.ops.oidc.jwtEnabled\u00b6 boolean \u2014 This setting can weaken the security stance for authentication and should only be used in testing. grafana.ops.oidc.scopes\u00b6 string <code>openid profile email groups</code> \u2014 grafana.ops.oidc.skipRoleSync\u00b6 boolean \u2014 When enabled the roles for user can be managed within Grafana. grafana.ops.oidc.userGroups\u00b6 object \u2014 Configure the roles for groups. grafana.ops.oidc.userGroups.grafanaAdmin\u00b6 string <code>grafana_admin</code> \u2014 grafana.ops.oidc.userGroups.grafanaEditor\u00b6 string <code>grafana_editor</code> \u2014 grafana.ops.oidc.userGroups.grafanaViewer\u00b6 string <code>grafana_viewer</code> \u2014 grafana.ops.plugins[]\u00b6 array \u2014 \u2014 grafana.ops.resources\u00b6 object \u2014 See note grafana.ops.resources.limits\u00b6 object \u2014 \u2014 grafana.ops.resources.requests\u00b6 object \u2014 \u2014 grafana.ops.sidecar\u00b6 object \u2014 Configure the sidecar provisioning dashboards from ConfigMaps in Grafana. grafana.ops.sidecar.resources\u00b6 object \u2014 See note grafana.ops.sidecar.resources.limits\u00b6 object \u2014 \u2014 grafana.ops.sidecar.resources.requests\u00b6 object \u2014 \u2014 grafana.ops.subdomain\u00b6 string <code>grafana</code> See note grafana.ops.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration grafana.ops.trailingDots\u00b6 boolean <code>True</code> See note grafana.ops.viewersCanEdit\u00b6 boolean <code>True</code> \u2014 grafana.user\u00b6 object \u2014 Configure Grafana. grafana.user.additionalConfigValues\u00b6 string \u2014 \u2014 grafana.user.additionalDatasources\u00b6 object \u2014 \u2014 grafana.user.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. grafana.user.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. grafana.user.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). grafana.user.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). grafana.user.contentSecurityPolicy\u00b6 object \u2014 Configure Content-Security-Policy header rulesReference: https://content-security-policy.com/ grafana.user.dataproxy\u00b6 object \u2014 Configure Grafana dataproxy values grafana.user.dataproxy.timeout\u00b6 number <code>600</code> \u2014 grafana.user.enabled\u00b6 boolean <code>True</code> \u2014 grafana.user.nodeSelector\u00b6 object \u2014 See note grafana.user.oidc\u00b6 object \u2014 Configure authentication to Grafana via Dex. grafana.user.oidc.allowedDomains[]\u00b6 array of string \u2014 Configure the domains of the users allowed to authenticate to Grafana. grafana.user.oidc.enabled\u00b6 boolean <code>True</code> \u2014 grafana.user.oidc.jwtEnabled\u00b6 boolean \u2014 This setting can weaken the security stance for authentication and should only be used in testing. grafana.user.oidc.scopes\u00b6 string <code>openid profile email groups</code> \u2014 grafana.user.oidc.skipRoleSync\u00b6 boolean \u2014 When enabled the roles for user can be managed within Grafana. grafana.user.oidc.userGroups\u00b6 object \u2014 Configure the roles for groups. grafana.user.oidc.userGroups.grafanaAdmin\u00b6 string <code>grafana_admin</code> \u2014 grafana.user.oidc.userGroups.grafanaEditor\u00b6 string <code>grafana_editor</code> \u2014 grafana.user.oidc.userGroups.grafanaViewer\u00b6 string <code>grafana_viewer</code> \u2014 grafana.user.plugins[]\u00b6 array \u2014 \u2014 grafana.user.resources\u00b6 object \u2014 See note grafana.user.resources.limits\u00b6 object \u2014 \u2014 grafana.user.resources.requests\u00b6 object \u2014 \u2014 grafana.user.sidecar\u00b6 object \u2014 Configure the sidecar provisioning dashboards from ConfigMaps in Grafana. grafana.user.sidecar.resources\u00b6 object \u2014 See note grafana.user.sidecar.resources.limits\u00b6 object \u2014 \u2014 grafana.user.sidecar.resources.requests\u00b6 object \u2014 \u2014 grafana.user.subdomain\u00b6 string <code>grafana</code> See note grafana.user.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration grafana.user.trailingDots\u00b6 boolean <code>True</code> See note grafana.user.viewersCanEdit\u00b6 boolean <code>True</code> \u2014"},{"location":"operator-manual/schema/config/#note:grafana.ops.nodeSelector","title":"Notes for <code>grafana.ops.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.ops.resources","title":"Notes for <code>grafana.ops.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.ops.sidecar.resources","title":"Notes for <code>grafana.ops.sidecar.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.ops.subdomain","title":"Notes for <code>grafana.ops.subdomain</code>","text":"<p>For Admin Grafana the subdomain of <code>opsDomain</code> that the Ingress to Admin Grafana will be created with.</p> <p>For Dev Grafana the subdomain of <code>baseDomain</code> that the Ingress to Dev Grafana will be created with.</p> <p>Note</p> <p>Must be set for both service and workload clusters.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.ops.trailingDots","title":"Notes for <code>grafana.ops.trailingDots</code>","text":"<p>Configure Grafana to use absolute domain names.</p> <p>Warning</p> <p>Some operating systems and web browsers may have problems accessing Grafana when with this enabled.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.user.nodeSelector","title":"Notes for <code>grafana.user.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.user.resources","title":"Notes for <code>grafana.user.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.user.sidecar.resources","title":"Notes for <code>grafana.user.sidecar.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.user.subdomain","title":"Notes for <code>grafana.user.subdomain</code>","text":"<p>For Admin Grafana the subdomain of <code>opsDomain</code> that the Ingress to Admin Grafana will be created with.</p> <p>For Dev Grafana the subdomain of <code>baseDomain</code> that the Ingress to Dev Grafana will be created with.</p> <p>Note</p> <p>Must be set for both service and workload clusters.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:grafana.user.trailingDots","title":"Notes for <code>grafana.user.trailingDots</code>","text":"<p>Configure Grafana to use absolute domain names.</p> <p>Warning</p> <p>Some operating systems and web browsers may have problems accessing Grafana when with this enabled.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#grafanalabelenforcer","title":"<code>grafanaLabelEnforcer</code>","text":"<p>Configure Grafana Label Enforcer, responsible to filter metrics from different clusters for Grafana datasources.</p> Key Type Default Description grafanaLabelEnforcer.resources\u00b6 object \u2014 See note grafanaLabelEnforcer.resources.limits\u00b6 object \u2014 \u2014 grafanaLabelEnforcer.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:grafanaLabelEnforcer.resources","title":"Notes for <code>grafanaLabelEnforcer.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#harbor","title":"<code>harbor</code>","text":"<p>Configuration options for Harbor.</p> <p>Harbor is a container registry that deployed for the application developers to use when deploying their applications.</p> <p>Note</p> <p>See upstream documentation for reference. All config variables that exists in harbor are not exposed via our config.</p> Key Type Default Description harbor.alerts\u00b6 object \u2014 Configuration options for Harbor Alerts. harbor.alerts.maxTotalArtifacts\u00b6 number <code>3000</code> Alert when the total number of artifacts is above the set number. harbor.alerts.maxTotalStorageUsedGB\u00b6 number <code>1500</code> Alert when the total storage usage is above the set number. harbor.backup\u00b6 object \u2014 Configuration options for Backup Job. harbor.backup.enabled\u00b6 boolean <code>True</code> \u2014 harbor.backup.ephemeralBackupStore\u00b6 object \u2014 EphemeralBackupStore configuration for Harbor<code>StorageSize</code> defines how large the ephemeral volumes will be. harbor.backup.ephemeralBackupStore.enabled\u00b6 boolean \u2014 \u2014 harbor.backup.ephemeralBackupStore.storageSize\u00b6 string <code>10Gi</code> \u2014 harbor.backup.retentionDays\u00b6 number <code>7</code> <code>RetentionDays</code> defines how old a backup should be before deleting it. harbor.backup.schedule\u00b6 string \u2014 \u2014 harbor.core\u00b6 object \u2014 Configuration options for Core. harbor.core.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.core.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.core.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.core.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.core.replicas\u00b6 number <code>1</code> Number of Core pods harbor.core.resources\u00b6 object \u2014 See note harbor.core.resources.limits\u00b6 object \u2014 \u2014 harbor.core.resources.requests\u00b6 object \u2014 \u2014 harbor.database\u00b6 object \u2014 See note harbor.database.external\u00b6 object \u2014 Configuration options for External Database. harbor.database.external.coreDatabase\u00b6 string <code>registry</code> Name of the database for Core harbor.database.external.notaryServerDatabase\u00b6 string <code>notaryserver</code> Name of the database for Notary Server harbor.database.external.notarySignerDatabase\u00b6 string <code>notarysigner</code> Name of the database for Notary Signer harbor.database.external.port\u00b6 string <code>5432</code> Database listening port harbor.database.external.sslmode\u00b6 string <code>disable</code> See note harbor.database.internal\u00b6 object \u2014 Configuration options for Internal Database. harbor.database.internal.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.database.internal.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.database.internal.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.database.internal.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.database.internal.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.database.internal.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.database.internal.resources\u00b6 object \u2014 See note harbor.database.internal.resources.limits\u00b6 object \u2014 \u2014 harbor.database.internal.resources.requests\u00b6 object \u2014 \u2014 harbor.database.type\u00b6 string <code>internal</code> \u2014 harbor.enabled\u00b6 boolean <code>True</code> \u2014 harbor.exporter\u00b6 object \u2014 Configuration options for Exporter. harbor.exporter.external\u00b6 object \u2014 External configuration harbor.exporter.external.coreDatabase\u00b6 string \u2014 See note harbor.exporter.external.port\u00b6 string \u2014 See note harbor.exporter.resources\u00b6 object \u2014 See note harbor.exporter.resources.limits\u00b6 object \u2014 \u2014 harbor.exporter.resources.requests\u00b6 object \u2014 \u2014 harbor.gc\u00b6 object \u2014 Configuration options for GC (Garbage Collection). harbor.gc.enabled\u00b6 boolean <code>True</code> \u2014 harbor.gc.forceConfigure\u00b6 boolean \u2014 \u2014 harbor.gc.schedule\u00b6 string <code>0 0 0 * * SUN</code> See note harbor.ingress\u00b6 object \u2014 Configuration options for Ingress. harbor.ingress.additionalAnnotations\u00b6 object \u2014 \u2014 harbor.ingress.defaultAnnotations\u00b6 object \u2014 Default annotations for ingress harbor.ingress.defaultAnnotations.nginx.ingress.kubernetes.io/proxy-buffering\u00b6 string \u2014 \u2014 harbor.ingress.defaultAnnotations.nginx.ingress.kubernetes.io/proxy-request-buffering\u00b6 string \u2014 \u2014 harbor.jobservice\u00b6 object \u2014 Configuration options for Jobservice. harbor.jobservice.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.jobservice.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.jobservice.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.jobservice.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.jobservice.jobLog\u00b6 object \u2014 Job log configuration harbor.jobservice.jobLog.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.jobservice.jobLog.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.jobservice.jobLoggers[]\u00b6 array of string \u2014 Configuration options for JobLoggers harbor.jobservice.replicas\u00b6 number <code>1</code> Number of Jobservice pods harbor.jobservice.resources\u00b6 object \u2014 See note harbor.jobservice.resources.limits\u00b6 object \u2014 \u2014 harbor.jobservice.resources.requests\u00b6 object \u2014 \u2014 harbor.jobservice.scanDataExports\u00b6 object \u2014 Scan data exports configuration harbor.jobservice.scanDataExports.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.jobservice.scanDataExports.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.mpuCleaner\u00b6 object \u2014 Configuration options for MultipartUpload cleaner job harbor.mpuCleaner.enabled\u00b6 boolean <code>True</code> \u2014 harbor.mpuCleaner.maxAgeDays\u00b6 number <code>7</code> <code>maxAgeDays</code> defines how old an unfinished multipartupload is allowed to be before deleting it. harbor.mpuCleaner.schedule\u00b6 string \u2014 \u2014 harbor.nodeSelector\u00b6 object \u2014 See note harbor.notary\u00b6 object \u2014 Configuration options for Notary. harbor.notary.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.notary.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.notary.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.notary.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.notary.replicas\u00b6 number <code>1</code> \u2014 harbor.notary.resources\u00b6 object \u2014 See note harbor.notary.resources.limits\u00b6 object \u2014 \u2014 harbor.notary.resources.requests\u00b6 object \u2014 \u2014 harbor.notary.subdomain\u00b6 string <code>notary.harbor</code> \u2014 harbor.notarySigner\u00b6 object \u2014 Configuration options for Notary signer. harbor.notarySigner.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.notarySigner.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.notarySigner.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.notarySigner.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.notarySigner.resources\u00b6 object \u2014 See note harbor.notarySigner.resources.limits\u00b6 object \u2014 \u2014 harbor.notarySigner.resources.requests\u00b6 object \u2014 \u2014 harbor.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to harbor. harbor.objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. harbor.objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. harbor.objectStorage.s3.region\u00b6 string \u2014 Region to store data. harbor.objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). harbor.objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. harbor.oidc\u00b6 object \u2014 Configuration options for OIDC. harbor.oidc.adminGroupName\u00b6 string \u2014 \u2014 harbor.oidc.groupClaimName\u00b6 string <code>groups</code> \u2014 harbor.oidc.scope\u00b6 string <code>openid,email,profile,offline_access,groups</code> \u2014 harbor.persistence\u00b6 object \u2014 Configuration options for Persistence. harbor.persistence.disableRedirect\u00b6 boolean \u2014 See note harbor.persistence.type\u00b6 string \u2014 See note harbor.portal\u00b6 object \u2014 Configuration options for Portal. harbor.portal.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.portal.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.portal.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.portal.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.portal.replicas\u00b6 number <code>1</code> \u2014 harbor.portal.resources\u00b6 object \u2014 See note harbor.portal.resources.limits\u00b6 object \u2014 \u2014 harbor.portal.resources.requests\u00b6 object \u2014 \u2014 harbor.redis\u00b6 object \u2014 See note harbor.redis.external\u00b6 object \u2014 Configuration options when external Redis is set harbor.redis.external.addr\u00b6 string \u2014 See note harbor.redis.external.sentinelMasterSet\u00b6 string \u2014 \u2014 harbor.redis.internal\u00b6 object \u2014 Configuration options when internal Redis is set harbor.redis.internal.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.redis.internal.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.redis.internal.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.redis.internal.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.redis.internal.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.redis.internal.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.redis.internal.resources\u00b6 object \u2014 See note harbor.redis.internal.resources.limits\u00b6 object \u2014 \u2014 harbor.redis.internal.resources.requests\u00b6 object \u2014 \u2014 harbor.redis.type\u00b6 string <code>internal</code> \u2014 harbor.registry\u00b6 object \u2014 Registry configuration harbor.registry.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.registry.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.registry.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.registry.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.registry.controller\u00b6 object \u2014 Controller configuration harbor.registry.controller.resources\u00b6 object \u2014 See note harbor.registry.controller.resources.limits\u00b6 object \u2014 \u2014 harbor.registry.controller.resources.requests\u00b6 object \u2014 \u2014 harbor.registry.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.registry.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.registry.replicas\u00b6 number <code>1</code> \u2014 harbor.registry.resources\u00b6 object \u2014 See note harbor.registry.resources.limits\u00b6 object \u2014 \u2014 harbor.registry.resources.requests\u00b6 object \u2014 \u2014 harbor.s3\u00b6 object \u2014 Configuration options for S3.Storage Driver S3 harbor.s3.multipartcopychunksize\u00b6 -integer- -string- \u2014 Default chunk size for all but the last S3 Multipart Upload part when copying stored objects. harbor.s3.multipartcopymaxconcurrency\u00b6 -integer- -string- \u2014 Max number of concurrent S3 Multipart Upload operations when copying stored objects. harbor.s3.multipartcopythresholdsize\u00b6 string <code>536870912</code> Default object size above which S3 Multipart Upload will be used when copying stored objects. harbor.subdomain\u00b6 string <code>harbor</code> \u2014 harbor.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration harbor.trivy\u00b6 object \u2014 Configuration options for Trivy. harbor.trivy.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. harbor.trivy.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. harbor.trivy.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). harbor.trivy.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). harbor.trivy.extraEnvVars[]\u00b6 array of object \u2014 Array of additional environment variables to pass to Trivyname/value combination harbor.trivy.persistentVolumeClaim\u00b6 object \u2014 PersistentVolumeClaim harbor.trivy.persistentVolumeClaim.size\u00b6 string <code>1Gi</code> \u2014 harbor.trivy.replicas\u00b6 number <code>1</code> \u2014 harbor.trivy.resources\u00b6 object \u2014 See note harbor.trivy.resources.limits\u00b6 object \u2014 \u2014 harbor.trivy.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:harbor.core.resources","title":"Notes for <code>harbor.core.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.database","title":"Notes for <code>harbor.database</code>","text":"<p>Configuration options for Database used by Harbor</p> <p>Set <code>type</code> to define which type of redis Harbor should use.</p> <p>Only <code>external</code> or <code>internal</code> database can be enabled at the same time.</p> <p><code>External</code>: Defines an external postgres that harbor will use. For more details how to configure harbor to use an external database check the README</p> <p><code>Internal</code>: Use the internal database that is packaged with harbor.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.database.external.sslmode","title":"Notes for <code>harbor.database.external.sslmode</code>","text":"<p>Possible values:</p> <pre><code>disable\n</code></pre> <pre><code>require\n</code></pre> <pre><code>verify-ca\n</code></pre> <pre><code>verify-full\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.database.internal.resources","title":"Notes for <code>harbor.database.internal.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.exporter.external.coreDatabase","title":"Notes for <code>harbor.exporter.external.coreDatabase</code>","text":"<p>Examples:</p> <pre><code>registry\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.exporter.external.port","title":"Notes for <code>harbor.exporter.external.port</code>","text":"<p>Examples:</p> <pre><code>5432\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.exporter.resources","title":"Notes for <code>harbor.exporter.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.gc.schedule","title":"Notes for <code>harbor.gc.schedule</code>","text":"<p>Defines a CRON schedule when the garbage collection job should run. Uses a special Cron format that adds \"seconds\" as the first entry. Order: \"seconds, minutes, hours, day of month, month, day of week\".</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.jobservice.resources","title":"Notes for <code>harbor.jobservice.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.nodeSelector","title":"Notes for <code>harbor.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.notary.resources","title":"Notes for <code>harbor.notary.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.notarySigner.resources","title":"Notes for <code>harbor.notarySigner.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.persistence.disableRedirect","title":"Notes for <code>harbor.persistence.disableRedirect</code>","text":"<p>Controls whether or not Harbor registry redirects users to the object storage endpoint. Set this to true if the object storage is not reachable by users when pushing images to Harbor, e.g. if you run into this timeout error:</p> <pre><code>dial tcp &lt;IP&gt;:&lt;PORT&gt;: i/o timeout\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.persistence.type","title":"Notes for <code>harbor.persistence.type</code>","text":"<p>This should match what is set in global config</p> <p>Possible values:</p> <pre><code>filesystem\n</code></pre> <pre><code>swift\n</code></pre> <pre><code>objectStorage\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.portal.resources","title":"Notes for <code>harbor.portal.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.redis","title":"Notes for <code>harbor.redis</code>","text":"<p>Configuration options for Redis used by Harbor</p> <p>Set <code>type</code> to define which type of redis Harbor should use.</p> <p>Only <code>external</code> or <code>internal</code> redis can be enabled at the same time.</p> <p><code>External</code>: Defines an external redis that harbor will use. For more details how to configure harbor to use an external redis check the README</p> <p><code>Internal</code>: Use the internal redis that is packaged with harbor.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.redis.external.addr","title":"Notes for <code>harbor.redis.external.addr</code>","text":"<p>Examples:</p> <pre><code>rfs-redis-harbor.redis-system.svc.cluster.local:26379\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.redis.internal.resources","title":"Notes for <code>harbor.redis.internal.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.registry.controller.resources","title":"Notes for <code>harbor.registry.controller.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.registry.resources","title":"Notes for <code>harbor.registry.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:harbor.trivy.resources","title":"Notes for <code>harbor.trivy.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#hnc","title":"<code>hnc</code>","text":"<p>Configuration for Hierarchical Namespace Controller.</p> <p>Note</p> <p>See upstream documentation for reference.</p> Key Type Default Description hnc.additionalAllowPropagateResources[]\u00b6 array of object \u2014 See note hnc.enabled\u00b6 boolean <code>True</code> Enable HNC hnc.excludedNamespaces[]\u00b6 array of string \u2014 See note hnc.ha\u00b6 boolean <code>True</code> Enable HA mode for hnc webhooks. hnc.includedNamespacesRegex\u00b6 string \u2014 See note hnc.managedNamespaceAnnotations[]\u00b6 array of string \u2014 Annotations that will be propagated to subnamespaces (allows regex). hnc.managedNamespaceLabels[]\u00b6 array of string \u2014 Labels that will be propagated to subnamespaces (allows regex).Labels in particular must also be configured in the HierarchyConfiguration object to be propagated. hnc.manager\u00b6 object \u2014 This is meant to describe the base class if you will, for Welkin resources. hnc.manager.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. hnc.manager.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. hnc.manager.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). hnc.manager.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). hnc.manager.enabled\u00b6 boolean \u2014 \u2014 hnc.manager.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container hnc.manager.nodeSelector\u00b6 object \u2014 See note hnc.manager.resources\u00b6 object \u2014 See note hnc.manager.resources.limits\u00b6 object \u2014 \u2014 hnc.manager.resources.requests\u00b6 object \u2014 \u2014 hnc.manager.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration hnc.manager.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. hnc.serviceMonitor\u00b6 object \u2014 Service monitor for Hierarchical Namespace Controller. hnc.serviceMonitor.relabelings[]\u00b6 array \u2014 Relabeling hnc.unpropagatedAnnotations[]\u00b6 array \u2014 Annotations that will be stripped from propagated objects hnc.webhook\u00b6 object \u2014 Webhook for Hierarchical Namespace Controller. hnc.webhook.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. hnc.webhook.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. hnc.webhook.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). hnc.webhook.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). hnc.webhook.nodeSelector\u00b6 object \u2014 See note hnc.webhook.replicaCount\u00b6 integer \u2014 \u2014 hnc.webhook.resources\u00b6 object \u2014 See note hnc.webhook.resources.limits\u00b6 object \u2014 \u2014 hnc.webhook.resources.requests\u00b6 object \u2014 \u2014 hnc.webhook.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration hnc.webhook.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. hnc.webhookMatchConditions\u00b6 boolean \u2014 Fine grained mach conditions for webhook.This feature is only available in Kubernetes v1.28+."},{"location":"operator-manual/schema/config/#note:hnc.additionalAllowPropagateResources[]","title":"Notes for <code>hnc.additionalAllowPropagateResources[]</code>","text":"<p>Additional resources to enable opt-in propagation for. Objects that should be propagated must have one of the annotations listed here https://github.com/kubernetes-sigs/hierarchical-namespaces/blob/master/docs/user-guide/how-to.md#limit-the-propagation-of-an-object-to-descendant-namespaces</p> <p>Additional allow propagate resources for hnc.</p> <p>Examples:</p> <pre><code>{'resource': 'secrets'}\n</code></pre> <pre><code>{'resource': 'networkpolicies', 'group': 'networking.k8s.io'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.excludedNamespaces[]","title":"Notes for <code>hnc.excludedNamespaces[]</code>","text":"<p>Namespaces excluded by HNC, here you can configure a list of namespaces to exclude from HNC in addition to the default excluded namespaces.</p> <p>Including and excluding namespaces</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.includedNamespacesRegex","title":"Notes for <code>hnc.includedNamespacesRegex</code>","text":"<p>Included namespaces, empty string will include all.</p> <p>Including and excluding namespaces</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.manager.nodeSelector","title":"Notes for <code>hnc.manager.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.manager.resources","title":"Notes for <code>hnc.manager.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.webhook.nodeSelector","title":"Notes for <code>hnc.webhook.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:hnc.webhook.resources","title":"Notes for <code>hnc.webhook.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#images","title":"<code>images</code>","text":"<p>Configure individual container URI for images of all Welkin components, and (optionally) enable support for global registry and/or repository.</p> Key Type Default Description images.calico\u00b6 object \u2014 calico stack image configuration images.calico.accountant\u00b6 string \u2014 See note images.certManager\u00b6 object \u2014 cert-manager stack image configuration images.certManager.cainjector\u00b6 string \u2014 See note images.certManager.controller\u00b6 string \u2014 See note images.certManager.startupapicheck\u00b6 string \u2014 See note images.certManager.webhook\u00b6 string \u2014 See note images.dex\u00b6 object \u2014 dex stack image configuration images.dex.image\u00b6 string \u2014 See note images.externalDns\u00b6 object \u2014 external-dns stack image configuration images.externalDns.image\u00b6 string \u2014 See note images.falco\u00b6 object \u2014 falco stack image configuration images.falco.driverLoaderInit\u00b6 string \u2014 See note images.falco.falcoctl\u00b6 string \u2014 See note images.falco.image\u00b6 string \u2014 See note images.falco.sidekick\u00b6 string \u2014 See note images.fluentd\u00b6 object \u2014 fluentd stack image configuration images.fluentd.aggregator\u00b6 string \u2014 See note images.fluentd.forwarder\u00b6 string \u2014 See note images.fluentd.logManager\u00b6 string \u2014 See note images.gatekeeper\u00b6 object \u2014 gatekeeper stack image configuration images.gatekeeper.image\u00b6 string \u2014 See note images.gatekeeper.kubectl\u00b6 string \u2014 See note images.gatekeeper.postInstallLabelNamespace\u00b6 string \u2014 See note images.gatekeeper.preInstallCRDs\u00b6 string \u2014 See note images.global\u00b6 object \u2014 See note images.global.registry\u00b6 object \u2014 If enabled it will be used as the registry of images that don't supply their own. images.global.registry.enabled\u00b6 boolean \u2014 \u2014 images.global.registry.uri\u00b6 string \u2014 See note images.global.repository\u00b6 object \u2014 If enabled it will be used as the repository of images that don't supply their own. images.global.repository.enabled\u00b6 boolean \u2014 \u2014 images.global.repository.uri\u00b6 string \u2014 See note images.gpuOperator\u00b6 object \u2014 gpu-operator stack image configuration images.gpuOperator.nodeFeatureDiscovery\u00b6 string \u2014 See note images.gpuOperator.operator\u00b6 string \u2014 See note images.harbor\u00b6 object \u2014 harbor stack image configuration images.harbor.backupJob\u00b6 string \u2014 See note images.harbor.core\u00b6 string \u2014 See note images.harbor.database\u00b6 string \u2014 See note images.harbor.exporter\u00b6 string \u2014 See note images.harbor.initJob\u00b6 string \u2014 See note images.harbor.jobservice\u00b6 string \u2014 See note images.harbor.mpuCleaner\u00b6 string \u2014 See note images.harbor.portal\u00b6 string \u2014 See note images.harbor.redis\u00b6 string \u2014 See note images.harbor.registry\u00b6 string \u2014 See note images.harbor.registryController\u00b6 string \u2014 See note images.harbor.trivyAdapter\u00b6 string \u2014 See note images.hnc\u00b6 object \u2014 hnc stack image configuration images.hnc.image\u00b6 string \u2014 See note images.ingressNginx\u00b6 object \u2014 ingress-nginx stack image configuration images.ingressNginx.admissionWebhooksPatch\u00b6 string \u2014 See note images.ingressNginx.controller\u00b6 string \u2014 See note images.ingressNginx.controllerChroot\u00b6 string \u2014 See note images.ingressNginx.defaultBackend\u00b6 string \u2014 See note images.ingressNginx.fileCopier\u00b6 string \u2014 See note images.kured\u00b6 object \u2014 kured stack image configuration images.kured.image\u00b6 string \u2014 See note images.kyverno\u00b6 object \u2014 kyverno stack image configuration images.kyverno.crdsMigration\u00b6 string \u2014 See note images.kyverno.init\u00b6 string \u2014 See note images.kyverno.main\u00b6 string \u2014 See note images.kyverno.webhooksCleanup\u00b6 string \u2014 See note images.monitoring\u00b6 object \u2014 monitoring stack image configuration images.monitoring.admissionWebhooksPatch\u00b6 string \u2014 See note images.monitoring.alertmanager\u00b6 string \u2014 See note images.monitoring.blackboxExporter\u00b6 string \u2014 See note images.monitoring.configReloader\u00b6 string \u2014 See note images.monitoring.grafana\u00b6 string \u2014 See note images.monitoring.grafanaLabelEnforcer\u00b6 string \u2014 See note images.monitoring.grafanaSidecar\u00b6 string \u2014 See note images.monitoring.kubeStateMetrics\u00b6 string \u2014 See note images.monitoring.metricsServer\u00b6 string \u2014 See note images.monitoring.nodeExporter\u00b6 string \u2014 See note images.monitoring.prometheus\u00b6 string \u2014 See note images.monitoring.prometheusOperator\u00b6 string \u2014 See note images.monitoring.s3Exporter\u00b6 string \u2014 See note images.monitoring.trivyOperator\u00b6 string \u2014 See note images.nodeLocalDns\u00b6 object \u2014 node-local-dns stack image configuration images.nodeLocalDns.image\u00b6 string \u2014 See note images.opensearch\u00b6 object \u2014 opensearch stack image configuration images.opensearch.configurerJob\u00b6 string \u2014 See note images.opensearch.curatorCronjob\u00b6 string \u2014 See note images.opensearch.dashboards\u00b6 string \u2014 See note images.opensearch.exporter\u00b6 string \u2014 See note images.opensearch.image\u00b6 string \u2014 See note images.opensearch.initSysctl\u00b6 string \u2014 See note images.rclone\u00b6 object \u2014 rclone stack image configuration images.rclone.image\u00b6 string \u2014 See note images.tekton\u00b6 object \u2014 tekton stack image configuration images.tekton.controller\u00b6 string \u2014 See note images.tekton.remoteResolvers\u00b6 string \u2014 See note images.tekton.webhook\u00b6 string \u2014 See note images.thanos\u00b6 object \u2014 thanos stack image configuration images.thanos.image\u00b6 string \u2014 See note images.velero\u00b6 object \u2014 velero stack image configuration images.velero.image\u00b6 string \u2014 See note images.velero.kubectl\u00b6 string \u2014 See note images.velero.pluginAws\u00b6 string \u2014 See note images.velero.pluginAzure\u00b6 string \u2014 See note images.velero.pluginCsi\u00b6 string \u2014 See note images.velero.pluginGcp\u00b6 string \u2014 See note"},{"location":"operator-manual/schema/config/#note:images.calico.accountant","title":"Notes for <code>images.calico.accountant</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.certManager.cainjector","title":"Notes for <code>images.certManager.cainjector</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.certManager.controller","title":"Notes for <code>images.certManager.controller</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.certManager.startupapicheck","title":"Notes for <code>images.certManager.startupapicheck</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.certManager.webhook","title":"Notes for <code>images.certManager.webhook</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.dex.image","title":"Notes for <code>images.dex.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.externalDns.image","title":"Notes for <code>images.externalDns.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.falco.driverLoaderInit","title":"Notes for <code>images.falco.driverLoaderInit</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.falco.falcoctl","title":"Notes for <code>images.falco.falcoctl</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.falco.image","title":"Notes for <code>images.falco.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.falco.sidekick","title":"Notes for <code>images.falco.sidekick</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.fluentd.aggregator","title":"Notes for <code>images.fluentd.aggregator</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.fluentd.forwarder","title":"Notes for <code>images.fluentd.forwarder</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.fluentd.logManager","title":"Notes for <code>images.fluentd.logManager</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gatekeeper.image","title":"Notes for <code>images.gatekeeper.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gatekeeper.kubectl","title":"Notes for <code>images.gatekeeper.kubectl</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gatekeeper.postInstallLabelNamespace","title":"Notes for <code>images.gatekeeper.postInstallLabelNamespace</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gatekeeper.preInstallCRDs","title":"Notes for <code>images.gatekeeper.preInstallCRDs</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.global","title":"Notes for <code>images.global</code>","text":"<p>Global image registry and repository settings.</p> <p>If a global registry is supplied and enabled, and an image is specified that doesn't have a registry, the global registry will be used instead.</p> <p>If a global repository is supplied and enabled, and an image is specified that doesn't have a repository, the global repository will be used instead.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.global.registry.uri","title":"Notes for <code>images.global.registry.uri</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.global.repository.uri","title":"Notes for <code>images.global.repository.uri</code>","text":"<p>Examples:</p> <pre><code>ingress-nginx\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gpuOperator.nodeFeatureDiscovery","title":"Notes for <code>images.gpuOperator.nodeFeatureDiscovery</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.gpuOperator.operator","title":"Notes for <code>images.gpuOperator.operator</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.backupJob","title":"Notes for <code>images.harbor.backupJob</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.core","title":"Notes for <code>images.harbor.core</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.database","title":"Notes for <code>images.harbor.database</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.exporter","title":"Notes for <code>images.harbor.exporter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.initJob","title":"Notes for <code>images.harbor.initJob</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.jobservice","title":"Notes for <code>images.harbor.jobservice</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.mpuCleaner","title":"Notes for <code>images.harbor.mpuCleaner</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.portal","title":"Notes for <code>images.harbor.portal</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.redis","title":"Notes for <code>images.harbor.redis</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.registry","title":"Notes for <code>images.harbor.registry</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.registryController","title":"Notes for <code>images.harbor.registryController</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.harbor.trivyAdapter","title":"Notes for <code>images.harbor.trivyAdapter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.hnc.image","title":"Notes for <code>images.hnc.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.ingressNginx.admissionWebhooksPatch","title":"Notes for <code>images.ingressNginx.admissionWebhooksPatch</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.ingressNginx.controller","title":"Notes for <code>images.ingressNginx.controller</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.ingressNginx.controllerChroot","title":"Notes for <code>images.ingressNginx.controllerChroot</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.ingressNginx.defaultBackend","title":"Notes for <code>images.ingressNginx.defaultBackend</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.ingressNginx.fileCopier","title":"Notes for <code>images.ingressNginx.fileCopier</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.kured.image","title":"Notes for <code>images.kured.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.kyverno.crdsMigration","title":"Notes for <code>images.kyverno.crdsMigration</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.kyverno.init","title":"Notes for <code>images.kyverno.init</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.kyverno.main","title":"Notes for <code>images.kyverno.main</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.kyverno.webhooksCleanup","title":"Notes for <code>images.kyverno.webhooksCleanup</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.admissionWebhooksPatch","title":"Notes for <code>images.monitoring.admissionWebhooksPatch</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.alertmanager","title":"Notes for <code>images.monitoring.alertmanager</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.blackboxExporter","title":"Notes for <code>images.monitoring.blackboxExporter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.configReloader","title":"Notes for <code>images.monitoring.configReloader</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.grafana","title":"Notes for <code>images.monitoring.grafana</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.grafanaLabelEnforcer","title":"Notes for <code>images.monitoring.grafanaLabelEnforcer</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.grafanaSidecar","title":"Notes for <code>images.monitoring.grafanaSidecar</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.kubeStateMetrics","title":"Notes for <code>images.monitoring.kubeStateMetrics</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.metricsServer","title":"Notes for <code>images.monitoring.metricsServer</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.nodeExporter","title":"Notes for <code>images.monitoring.nodeExporter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.prometheus","title":"Notes for <code>images.monitoring.prometheus</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.prometheusOperator","title":"Notes for <code>images.monitoring.prometheusOperator</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.s3Exporter","title":"Notes for <code>images.monitoring.s3Exporter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.monitoring.trivyOperator","title":"Notes for <code>images.monitoring.trivyOperator</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.nodeLocalDns.image","title":"Notes for <code>images.nodeLocalDns.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.configurerJob","title":"Notes for <code>images.opensearch.configurerJob</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.curatorCronjob","title":"Notes for <code>images.opensearch.curatorCronjob</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.dashboards","title":"Notes for <code>images.opensearch.dashboards</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.exporter","title":"Notes for <code>images.opensearch.exporter</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.image","title":"Notes for <code>images.opensearch.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.opensearch.initSysctl","title":"Notes for <code>images.opensearch.initSysctl</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.rclone.image","title":"Notes for <code>images.rclone.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.tekton.controller","title":"Notes for <code>images.tekton.controller</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.tekton.remoteResolvers","title":"Notes for <code>images.tekton.remoteResolvers</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.tekton.webhook","title":"Notes for <code>images.tekton.webhook</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.thanos.image","title":"Notes for <code>images.thanos.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.image","title":"Notes for <code>images.velero.image</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.kubectl","title":"Notes for <code>images.velero.kubectl</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.pluginAws","title":"Notes for <code>images.velero.pluginAws</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.pluginAzure","title":"Notes for <code>images.velero.pluginAzure</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.pluginCsi","title":"Notes for <code>images.velero.pluginCsi</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:images.velero.pluginGcp","title":"Notes for <code>images.velero.pluginGcp</code>","text":"<p>Examples:</p> <pre><code>registry.k8s.io/ingress-nginx/controller-chroot:v1.12.1@sha256:90155c86548e0bb95b3abf1971cd687d8f5d43f340cfca0ad3484e2b8351096e\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#ingressnginx","title":"<code>ingressNginx</code>","text":"<p>Configure Ingress-NGINX, the ingress controller.</p> Key Type Default Description ingressNginx.controller\u00b6 object \u2014 Configure the controller daemonset of Ingress-NGINX. ingressNginx.controller.additionalConfig\u00b6 object \u2014 See note ingressNginx.controller.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. ingressNginx.controller.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. ingressNginx.controller.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). ingressNginx.controller.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). ingressNginx.controller.allowSnippetAnnotations\u00b6 boolean \u2014 See note ingressNginx.controller.chroot\u00b6 boolean <code>True</code> See note ingressNginx.controller.config\u00b6 object \u2014 Configure the Ingress-NGINX controller. ingressNginx.controller.config.annotationsRiskLevel\u00b6 string <code>Critical</code> See note ingressNginx.controller.config.useProxyProtocol\u00b6 boolean \u2014 \u2014 ingressNginx.controller.enableAnnotationValidations\u00b6 boolean <code>True</code> When enabled annotations on Ingress resources are validated.This is disabled by default due to the maturity of the feature and lack of documentation. ingressNginx.controller.enablepublishService\u00b6 boolean \u2014 See note ingressNginx.controller.extraArgs\u00b6 object \u2014 Configure extra args to pass to Ingress NGINX Controller. ingressNginx.controller.extraEnvs[]\u00b6 array \u2014 Configure extra environment variables to Ingress NGINX Controller. ingressNginx.controller.nodeSelector\u00b6 object \u2014 See note ingressNginx.controller.resources\u00b6 object \u2014 See note ingressNginx.controller.resources.limits\u00b6 object \u2014 \u2014 ingressNginx.controller.resources.requests\u00b6 object \u2014 \u2014 ingressNginx.controller.service\u00b6 object \u2014 Configure the Service for traffic to Ingress-NGINX. ingressNginx.controller.service.allocateLoadBalancerNodePorts\u00b6 boolean \u2014 See note ingressNginx.controller.service.annotations\u00b6 object \u2014 \u2014 ingressNginx.controller.service.clusterIP\u00b6 string \u2014 \u2014 ingressNginx.controller.service.enabled\u00b6 boolean \u2014 \u2014 ingressNginx.controller.service.internal\u00b6 object \u2014 Configure the Internal Service for traffic to Ingress-NGINX. ingressNginx.controller.service.internal.allocateLoadBalancerNodePorts\u00b6 boolean \u2014 See note ingressNginx.controller.service.internal.annotations\u00b6 object \u2014 \u2014 ingressNginx.controller.service.internal.clusterIP\u00b6 string \u2014 \u2014 ingressNginx.controller.service.internal.enabled\u00b6 boolean \u2014 \u2014 ingressNginx.controller.service.internal.ipFamilyPolicy\u00b6 string <code>SingleStack</code> See note ingressNginx.controller.service.internal.loadBalancerIP\u00b6 string \u2014 See note ingressNginx.controller.service.internal.loadBalancerSourceRanges[]\u00b6 array of string \u2014 Configure the source ranges to allow via the Load Balancer Service. ingressNginx.controller.service.internal.nodePorts\u00b6 object \u2014 Configure the node ports to allocate for the Service. ingressNginx.controller.service.internal.nodePorts.http\u00b6 integer \u2014 \u2014 ingressNginx.controller.service.internal.nodePorts.https\u00b6 integer \u2014 \u2014 ingressNginx.controller.service.internal.type\u00b6 string \u2014 See note ingressNginx.controller.service.ipFamilies[]\u00b6 array of string <code>['IPv4']</code> See note ingressNginx.controller.service.ipFamilyPolicy\u00b6 string <code>SingleStack</code> See note ingressNginx.controller.service.loadBalancerIP\u00b6 string \u2014 See note ingressNginx.controller.service.loadBalancerSourceRanges[]\u00b6 array of string \u2014 Configure the source ranges to allow via the Load Balancer Service. ingressNginx.controller.service.nodePorts\u00b6 object \u2014 Configure the node ports to allocate for the Service. ingressNginx.controller.service.nodePorts.http\u00b6 integer \u2014 \u2014 ingressNginx.controller.service.nodePorts.https\u00b6 integer \u2014 \u2014 ingressNginx.controller.service.type\u00b6 string \u2014 See note ingressNginx.controller.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration ingressNginx.controller.useHostPort\u00b6 boolean \u2014 When enabled ingress traffic is directly forwarded from target ports on the nodes to reach Ingress-NGINX.This requires the namespace to use Pod Security Standard <code>privileged</code>. ingressNginx.defaultBackend\u00b6 object \u2014 Configure the default backend deployment of Ingress-NGINX. ingressNginx.defaultBackend.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. ingressNginx.defaultBackend.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. ingressNginx.defaultBackend.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). ingressNginx.defaultBackend.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). ingressNginx.defaultBackend.nodeSelector\u00b6 object \u2014 See note ingressNginx.defaultBackend.resources\u00b6 object \u2014 See note ingressNginx.defaultBackend.resources.limits\u00b6 object \u2014 \u2014 ingressNginx.defaultBackend.resources.requests\u00b6 object \u2014 \u2014 ingressNginx.defaultBackend.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration ingressNginx.defaultBackend.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. ingressNginx.subDomain\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.additionalConfig","title":"Notes for <code>ingressNginx.controller.additionalConfig</code>","text":"<p>Configure additional configuration for Ingress-NGINX controller.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.allowSnippetAnnotations","title":"Notes for <code>ingressNginx.controller.allowSnippetAnnotations</code>","text":"<p>When enabled annotations on Ingress resources can add snippets to the config of NGINX.</p> <p>[!danger] Only enable this after evaluating the risks it poses.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.chroot","title":"Notes for <code>ingressNginx.controller.chroot</code>","text":"<p>When enabled NGINX itself will run in a chroot under the controller namespace for increased separation between the controller and the proxy.</p> <p>This requires a special seccomp profile to be available to give the controller the <code>SYS_ADMIN</code> capability, which will be provided by a separate daemon set.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.config.annotationsRiskLevel","title":"Notes for <code>ingressNginx.controller.config.annotationsRiskLevel</code>","text":"<p>Configure the accepted risk level of annotations on Ingress resources.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>Possible values:</p> <pre><code>Critical\n</code></pre> <pre><code>High\n</code></pre> <pre><code>Medium\n</code></pre> <pre><code>Low\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.enablepublishService","title":"Notes for <code>ingressNginx.controller.enablepublishService</code>","text":"<p>When enabled it allows customisation of the IP or FQDN to report the external address of the Service in the Ingress status field.</p> <p>When disabled it reports the IPs of the nodes where the controller pods are running.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.nodeSelector","title":"Notes for <code>ingressNginx.controller.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.resources","title":"Notes for <code>ingressNginx.controller.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.allocateLoadBalancerNodePorts","title":"Notes for <code>ingressNginx.controller.service.allocateLoadBalancerNodePorts</code>","text":"<p>When enabled node ports will be allocated for the Load Balancer Service.</p> <p>This should be enabled when the cluster is fronted by a proxy load balancer regardless if it is external or internal, and disabled if the cluster uses direct routing of ingress traffic.</p> <p>See reference</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.internal.allocateLoadBalancerNodePorts","title":"Notes for <code>ingressNginx.controller.service.internal.allocateLoadBalancerNodePorts</code>","text":"<p>When enabled node ports will be allocated for the Load Balancer Service.</p> <p>This should be enabled when the cluster is fronted by a proxy load balancer regardless if it is external or internal, and disabled if the cluster uses direct routing of ingress traffic.</p> <p>See reference</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.internal.ipFamilyPolicy","title":"Notes for <code>ingressNginx.controller.service.internal.ipFamilyPolicy</code>","text":"<p>Represents the dual-stack-ness requested or required by this Service. When utilizing an internal loadbalancer service (ie MetalLB), set this field to \"RequireDualStack\" if you want both IPv4 and IPv6 connectivity. The ipFamilies and clusterIPs fields depend on the value of this field.</p> <p>See reference</p> <p>Possible values:</p> <pre><code>SingleStack\n</code></pre> <pre><code>PreferDualStack\n</code></pre> <pre><code>RequireDualStack\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.internal.loadBalancerIP","title":"Notes for <code>ingressNginx.controller.service.internal.loadBalancerIP</code>","text":"<p>Configure the Load Balancer IP to use an existing IP if supported by the infrastructure provider.</p> <p>Important</p> <p>With OpenStack Octavia the floating IP can be created via the CLI beforehand, and one should set the annotation <code>loadbalancer.openstack.org/keep-floatingip: \"true\"</code> to prevent the floating IP to be deleted.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.internal.type","title":"Notes for <code>ingressNginx.controller.service.internal.type</code>","text":"<p>Configure the type of the Service.</p> <p>Possible values:</p> <pre><code>ClusterIP\n</code></pre> <pre><code>LoadBalancer\n</code></pre> <pre><code>NodePort\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.ipFamilies[]","title":"Notes for <code>ingressNginx.controller.service.ipFamilies[]</code>","text":"<p>List of IP families (e.g. IPv4, IPv6) assigned to the service. Default is IPv4 only. When utilizing an internal loadbalancer service (ie MetalLB), IPv6 would also need to be included in order for the ingress service to allocate an address in that family.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.ipFamilyPolicy","title":"Notes for <code>ingressNginx.controller.service.ipFamilyPolicy</code>","text":"<p>Represents the dual-stack-ness requested or required by this Service. When utilizing an internal loadbalancer service (ie MetalLB), set this field to \"RequireDualStack\" if you want both IPv4 and IPv6 connectivity. The ipFamilies and clusterIPs fields depend on the value of this field.</p> <p>See reference</p> <p>Possible values:</p> <pre><code>SingleStack\n</code></pre> <pre><code>PreferDualStack\n</code></pre> <pre><code>RequireDualStack\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.loadBalancerIP","title":"Notes for <code>ingressNginx.controller.service.loadBalancerIP</code>","text":"<p>Configure the Load Balancer IP to use an existing IP if supported by the infrastructure provider.</p> <p>Important</p> <p>With OpenStack Octavia the floating IP can be created via the CLI beforehand, and one should set the annotation <code>loadbalancer.openstack.org/keep-floatingip: \"true\"</code> to prevent the floating IP to be deleted.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.controller.service.type","title":"Notes for <code>ingressNginx.controller.service.type</code>","text":"<p>Configure the type of the Service.</p> <p>Possible values:</p> <pre><code>ClusterIP\n</code></pre> <pre><code>LoadBalancer\n</code></pre> <pre><code>NodePort\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.defaultBackend.nodeSelector","title":"Notes for <code>ingressNginx.defaultBackend.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:ingressNginx.defaultBackend.resources","title":"Notes for <code>ingressNginx.defaultBackend.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#issuers","title":"<code>issuers</code>","text":"<p>Configure issuers for cert-manager.</p> Key Type Default Description issuers.extraIssuers[]\u00b6 array \u2014 \u2014 issuers.letsencrypt\u00b6 object \u2014 Configure issuers for cert-manager using Let's Encrypt. issuers.letsencrypt.enabled\u00b6 boolean <code>True</code> \u2014 issuers.letsencrypt.prod\u00b6 object \u2014 Configure Let's Encrypt production issuer. issuers.letsencrypt.prod.solvers[]\u00b6 array \u2014 \u2014 issuers.letsencrypt.staging\u00b6 object \u2014 Configure Let's Encrypt staging issuer. issuers.letsencrypt.staging.solvers[]\u00b6 array \u2014 \u2014"},{"location":"operator-manual/schema/config/#kubestatemetrics","title":"<code>kubeStateMetrics</code>","text":"<p>Configure the kube-state-metrics exporter.</p> Key Type Default Description kubeStateMetrics.resources\u00b6 object \u2014 See note kubeStateMetrics.resources.limits\u00b6 object \u2014 \u2014 kubeStateMetrics.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:kubeStateMetrics.resources","title":"Notes for <code>kubeStateMetrics.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#kured","title":"<code>kured</code>","text":"<p>Configuration for Kured (Kubernetes Reboot Daemon).</p> <p>Kured orchestrates node reboots to allow nodes to automatically perform system updates and patches.</p> Key Type Default Description kured.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. kured.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. kured.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). kured.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). kured.configuration\u00b6 object \u2014 See note kured.configuration.drainTimeout\u00b6 string \u2014 See note kured.configuration.endTime\u00b6 string <code>86399</code> Schedule reboots only before this time of day. kured.configuration.lockReleaseDelay\u00b6 string \u2014 See note kured.configuration.period\u00b6 string \u2014 See note kured.configuration.rebootDays[]\u00b6 array of string <code>['mo', 'tu', 'we', 'th', 'fr', 'sa', 'su']</code> Only reboot on these days. kured.configuration.startTime\u00b6 string <code>0:00</code> Schedule reboots only after this time of day. kured.configuration.timeZone\u00b6 string <code>UTC</code> \u2014 kured.dsAnnotations\u00b6 object \u2014 \u2014 kured.enabled\u00b6 boolean \u2014 \u2014 kured.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container kured.extraEnvVars\u00b6 object \u2014 \u2014 kured.metrics\u00b6 object \u2014 Configuration for Kured metrics kured.metrics.enabled\u00b6 boolean <code>True</code> \u2014 kured.metrics.interval\u00b6 string \u2014 See note kured.metrics.labels\u00b6 object \u2014 \u2014 kured.nodeSelector\u00b6 object \u2014 See note kured.notification\u00b6 object \u2014 Send notification from Kured when nodes are rebooted. kured.notification.slack\u00b6 object \u2014 Send notification from Kured to Slack when nodes are rebooted. kured.notification.slack.channel\u00b6 string \u2014 \u2014 kured.notification.slack.enabled\u00b6 boolean \u2014 \u2014 kured.resources\u00b6 object \u2014 See note kured.resources.limits\u00b6 object \u2014 \u2014 kured.resources.requests\u00b6 object \u2014 \u2014 kured.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:kured.configuration","title":"Notes for <code>kured.configuration</code>","text":"<p>Kured configuration parameters.</p> <p>See the upstream documentation for reference. Most parameters are mapped from <code>camelCase</code> to <code>--kebab-case</code>, others can be set via <code>extraArgs</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.configuration.drainTimeout","title":"Notes for <code>kured.configuration.drainTimeout</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.configuration.lockReleaseDelay","title":"Notes for <code>kured.configuration.lockReleaseDelay</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.configuration.period","title":"Notes for <code>kured.configuration.period</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.metrics.interval","title":"Notes for <code>kured.metrics.interval</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.nodeSelector","title":"Notes for <code>kured.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kured.resources","title":"Notes for <code>kured.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#kyverno","title":"<code>kyverno</code>","text":"<p>Configure Kyverno and Kyverno Policies</p> Key Type Default Description kyverno.enabled\u00b6 boolean \u2014 \u2014 kyverno.nodeAffinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. kyverno.nodeAffinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. kyverno.nodeAffinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). kyverno.nodeAffinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). kyverno.nodeSelector\u00b6 object \u2014 See note kyverno.podAffinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. kyverno.podAffinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. kyverno.podAffinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). kyverno.podAffinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). kyverno.policies\u00b6 object \u2014 Kyverno policies configuration kyverno.policies.verifyImageSignature\u00b6 object \u2014 A policy that requires that all images in HNC controlled namespaces are signed kyverno.policies.verifyImageSignature.attestor\u00b6 string \u2014 See note kyverno.policies.verifyImageSignature.enabled\u00b6 boolean \u2014 \u2014 kyverno.policies.verifyImageSignature.ignoreRekorTlog\u00b6 boolean \u2014 \u2014 kyverno.policies.verifyImageSignature.type\u00b6 string \u2014 See note kyverno.resources\u00b6 object \u2014 See note kyverno.resources.limits\u00b6 object \u2014 \u2014 kyverno.resources.requests\u00b6 object \u2014 \u2014 kyverno.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration kyverno.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains."},{"location":"operator-manual/schema/config/#note:kyverno.nodeSelector","title":"Notes for <code>kyverno.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kyverno.policies.verifyImageSignature.attestor","title":"Notes for <code>kyverno.policies.verifyImageSignature.attestor</code>","text":"<p>A public key (Cosign) or certificate (Notary) used to verify image signatures</p> <p>Examples:</p> <pre><code>-----BEGIN PUBLIC KEY-----\nMFkwEwY...\n-----END PUBLIC KEY-----\n</code></pre> <pre><code>-----BEGIN CERTIFICATE-----\nMIIDTTCCA...\n-----END CERTIFICATE-----\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kyverno.policies.verifyImageSignature.type","title":"Notes for <code>kyverno.policies.verifyImageSignature.type</code>","text":"<p>Possible values:</p> <pre><code>Cosign\n</code></pre> <pre><code>Notary\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:kyverno.resources","title":"Notes for <code>kyverno.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#metricsserver","title":"<code>metricsServer</code>","text":"<p>Configure the metrics-server exporter, used to provide for the metrics API in Kubernetes.</p> Key Type Default Description metricsServer.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. metricsServer.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. metricsServer.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). metricsServer.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). metricsServer.enabled\u00b6 boolean <code>True</code> \u2014 metricsServer.resources\u00b6 object \u2014 See note metricsServer.resources.limits\u00b6 object \u2014 \u2014 metricsServer.resources.requests\u00b6 object \u2014 \u2014 metricsServer.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:metricsServer.resources","title":"Notes for <code>metricsServer.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#networkplugin","title":"<code>networkPlugin</code>","text":"<p>Configure the network plugin used in the cluster.</p> Key Type Default Description networkPlugin.calico\u00b6 object \u2014 Configuration when network plugin is set to calico networkPlugin.calico.calicoAccountant\u00b6 object \u2014 Configure Calico accountant, used to collect metrics about packets affected by Network Policies when using Calico. networkPlugin.calico.calicoAccountant.backend\u00b6 string <code>nftables</code> See note networkPlugin.calico.calicoAccountant.enabled\u00b6 boolean <code>True</code> \u2014 networkPlugin.calico.calicoAccountant.resources\u00b6 object \u2014 See note networkPlugin.calico.calicoAccountant.resources.limits\u00b6 object \u2014 \u2014 networkPlugin.calico.calicoAccountant.resources.requests\u00b6 object \u2014 \u2014 networkPlugin.calico.calicoAccountant.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration networkPlugin.calico.calicoFelixMetrics\u00b6 object \u2014 Configure Calico Felix metrics, used to collect metrics about Calico. networkPlugin.calico.calicoFelixMetrics.enabled\u00b6 boolean <code>True</code> \u2014 networkPlugin.type\u00b6 string \u2014 See note"},{"location":"operator-manual/schema/config/#note:networkPlugin.calico.calicoAccountant.backend","title":"Notes for <code>networkPlugin.calico.calicoAccountant.backend</code>","text":"<p>Possible values:</p> <pre><code>iptables\n</code></pre> <pre><code>nftables\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPlugin.calico.calicoAccountant.resources","title":"Notes for <code>networkPlugin.calico.calicoAccountant.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPlugin.type","title":"Notes for <code>networkPlugin.type</code>","text":"<p>Configure the type of network plugin</p> <p>Possible values:</p> <pre><code>calico\n</code></pre> <pre><code>cilium\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#networkpolicies","title":"<code>networkPolicies</code>","text":"<p>Configure Network Policies.</p> <p>Most common Network Policy rules can be updated by running <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> Key Type Default Description networkPolicies.additional\u00b6 string \u2014 Configure additional network policies. networkPolicies.additionalEgressPolicies[]\u00b6 array \u2014 \u2014 networkPolicies.additionalIngressPolicies[]\u00b6 array \u2014 \u2014 networkPolicies.alertmanager\u00b6 object \u2014 Configure Alertmanager network policy rules. networkPolicies.alertmanager.alertReceivers\u00b6 object \u2014 Network policy ruleKubernetes network policies networkPolicies.alertmanager.alertReceivers.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.alertmanager.alertReceivers.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.alertmanager.alertReceivers.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.alertmanager.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.allowedNameSpaces[]\u00b6 array \u2014 \u2014 networkPolicies.certManager\u00b6 object \u2014 Configure cert-manager network policy rules. networkPolicies.certManager.dns01\u00b6 object \u2014 Configure network policy rule to allow cert-manager perform DNS-01 challenges. networkPolicies.certManager.dns01.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.certManager.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.certManager.http01\u00b6 object \u2014 Configure network policy rule to allow cert-manager perform HTTP-01 challenges on other endpoints than the ingress-controller. networkPolicies.certManager.http01.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.certManager.letsencrypt\u00b6 object \u2014 See note networkPolicies.certManager.namespaces[]\u00b6 array of string \u2014 See note networkPolicies.coredns\u00b6 object \u2014 Configure CoreDNS network policy rules. networkPolicies.coredns.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.coredns.externalDns\u00b6 object \u2014 Configure network policy rule to allow CoreDNS to query the upstream DNS servers. networkPolicies.coredns.serviceIp\u00b6 object \u2014 Configure network policy rule to allow CoreDNS to query the internal service IP. networkPolicies.defaultDeny\u00b6 boolean \u2014 \u2014 networkPolicies.dex\u00b6 object \u2014 Configure Dex network policy rules. networkPolicies.dex.connectors\u00b6 object \u2014 Configure network policy rule to allow Dex to reach configured connectors. networkPolicies.dex.connectors.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.dex.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.dnsAutoscaler\u00b6 object \u2014 Configure DNS Autoscaler network policy rules. networkPolicies.dnsAutoscaler.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.enableAlerting\u00b6 boolean <code>True</code> \u2014 networkPolicies.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.externalDns\u00b6 object \u2014 Configure ExternalDNS network policy rules. networkPolicies.externalDns.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.externalDns.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.falco\u00b6 object \u2014 Configure Falco network policy rules. networkPolicies.falco.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.falco.plugins\u00b6 object \u2014 Configure network policy rules to allow Falco to install plugins during startup. networkPolicies.falco.plugins.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.fluentd\u00b6 object \u2014 Configure Fluentd network policy rules. networkPolicies.fluentd.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.fluentd.extraOutput\u00b6 object \u2014 Configure extra output egress rules.This may be used to allow application developers to send logs externally from user Fluentd with extra config and plugins. networkPolicies.fluentd.extraOutput.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.fluentd.extraOutput.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.gatekeeper\u00b6 object \u2014 Configure Gatekeeper network policy rules. networkPolicies.gatekeeper.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.global\u00b6 object \u2014 Configure global network policy rules. networkPolicies.global.externalLoadBalancer\u00b6 boolean \u2014 When enabled create Network Policy rules for ingress via external load balancer. networkPolicies.global.ingressUsingHostNetwork\u00b6 boolean \u2014 When enabled create Network Policy rules for ingress via host network. networkPolicies.global.objectStorage\u00b6 object \u2014 See note networkPolicies.global.objectStorage.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.objectStorage.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.global.objectStorageSwift\u00b6 object \u2014 See note networkPolicies.global.objectStorageSwift.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.global.scApiserver\u00b6 object \u2014 See note networkPolicies.global.scApiserver.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.scApiserver.port\u00b6 integer \u2014 \u2014 networkPolicies.global.scIngress\u00b6 object \u2014 See note networkPolicies.global.scIngress.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.scNodes\u00b6 object \u2014 See note networkPolicies.global.scNodes.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.trivy\u00b6 object \u2014 Configure Trivy network policy rules.Used for Trivy to fetch vulnerability databases both in Harbor and Trivy Operator. networkPolicies.global.trivy.port\u00b6 integer \u2014 \u2014 networkPolicies.global.wcApiserver\u00b6 object \u2014 See note networkPolicies.global.wcApiserver.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.wcApiserver.port\u00b6 integer \u2014 \u2014 networkPolicies.global.wcIngress\u00b6 object \u2014 See note networkPolicies.global.wcIngress.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.global.wcNodes\u00b6 object \u2014 See note networkPolicies.global.wcNodes.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.harbor\u00b6 object \u2014 Configure Harbor network policy rules. networkPolicies.harbor.database\u00b6 object \u2014 Configure network policies for the database used by Harbor. networkPolicies.harbor.database.externalEgress\u00b6 object \u2014 Configure network policy egress rules to the external database of Harbor. networkPolicies.harbor.database.externalEgress.peers[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.database.externalEgress.ports[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.database.internalIngress\u00b6 object \u2014 Configure network policy ingress rules to the internal database of Harbor. networkPolicies.harbor.database.internalIngress.peers[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.database.internalIngress.ports[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.harbor.jobservice\u00b6 object \u2014 Configure network policies for the job service in Harbor. networkPolicies.harbor.jobservice.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.harbor.redis\u00b6 object \u2014 Configure network policies for the Redis used by Harbor. networkPolicies.harbor.redis.externalEgress\u00b6 object \u2014 Configure network policy egress rules to the external Redis of Harbor. networkPolicies.harbor.redis.externalEgress.peers[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.redis.externalEgress.ports[]\u00b6 array \u2014 \u2014 networkPolicies.harbor.registries\u00b6 object \u2014 Configure network policies for external registries used by Harbor.Applies to harbor-core and harbor-jobservice when replication is enabled. networkPolicies.harbor.registries.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.harbor.trivy\u00b6 object \u2014 Configure network policies for the Trivy scanner in Harbor. networkPolicies.harbor.trivy.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.ingressNginx\u00b6 object \u2014 Configure Ingress NGINX network policy rules. networkPolicies.ingressNginx.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.kubeSystem\u00b6 object \u2014 Configure kube-system network policy rules. networkPolicies.kubeSystem.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.kubeSystem.openstack\u00b6 object \u2014 Configure OpenStack network policy rules. networkPolicies.kubeSystem.openstack.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.kubeSystem.openstack.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.kubeSystem.openstack.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.kubeSystem.upcloud\u00b6 object \u2014 Configure UpCloud network policy rules. networkPolicies.kubeSystem.upcloud.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.kubeSystem.upcloud.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.kubeSystem.upcloud.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.kured\u00b6 object \u2014 Configure Kured network policy rules. networkPolicies.kured.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.kured.notificationSlack\u00b6 object \u2014 Configure network policy rules to allow Kured to send Slack notifications. networkPolicies.kured.notificationSlack.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.kyverno\u00b6 object \u2014 Configure Kyverno network policy rules. networkPolicies.kyverno.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.kyverno.imageRegistry\u00b6 object \u2014 Configure network policy that allows Kyverno to access image registries. This is required for signed image verification. networkPolicies.monitoring\u00b6 object \u2014 Configure monitoring network policy rules. networkPolicies.monitoring.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.monitoring.grafana\u00b6 object \u2014 Configure Grafana network policy rules. networkPolicies.monitoring.grafana.externalDashboardProvider\u00b6 object \u2014 Configure network policy rules to allow Grafana to use external dashboards. networkPolicies.monitoring.grafana.externalDashboardProvider.ips[]\u00b6 array of string \u2014 List of IP netmasksA IP address with netmask networkPolicies.monitoring.grafana.externalDashboardProvider.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.monitoring.grafana.externalDataSources\u00b6 object \u2014 Configure network policy rules to allow Grafana to use external datasources. networkPolicies.monitoring.grafana.externalDataSources.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.opensearch\u00b6 object \u2014 Configure OpenSearch network policy rules. networkPolicies.opensearch.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.opensearch.plugins\u00b6 object \u2014 Configure network policy rules to allow OpenSearch to install plugins during startup. networkPolicies.opensearch.plugins.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.prometheus\u00b6 object \u2014 Configure Prometheus network policy rules. networkPolicies.prometheus.internalAccess\u00b6 object \u2014 See note networkPolicies.prometheus.internalAccess.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.prometheus.internalAccess.namespaces[]\u00b6 array of string \u2014 Configure the namespaces to allow internal access to Prometheus. networkPolicies.rclone\u00b6 object \u2014 Configure Rclone network policy rules. networkPolicies.rclone.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.rclone.sync\u00b6 object \u2014 Configure network policy rules to allow rclone to sync. networkPolicies.rclone.sync.objectStorage\u00b6 object \u2014 Configure network policy rules to allow rclone to sync object storage. networkPolicies.rclone.sync.objectStorage.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.rclone.sync.objectStorageSwift\u00b6 object \u2014 Configure network policy rules to allow rclone to sync object storage with Swift. networkPolicies.rclone.sync.objectStorageSwift.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.rclone.sync.secondaryUrl\u00b6 object \u2014 Configure network policy rules to allow rclone to sync with a secondary URL. networkPolicies.rclone.sync.secondaryUrl.ports[]\u00b6 array of integer \u2014 A 16 bit unsigned integer networkPolicies.rookCeph\u00b6 object \u2014 Configure Rook Ceph network policy rules. networkPolicies.rookCeph.enabled\u00b6 boolean \u2014 \u2014 networkPolicies.s3Exporter\u00b6 object \u2014 Configure S3 exporter network policy rules. networkPolicies.s3Exporter.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.tektonPipelines\u00b6 object \u2014 Enable network policies for tekton and the pipeline. networkPolicies.tektonPipelines.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.tektonPipelines.pipeline\u00b6 object \u2014 See note networkPolicies.thanos\u00b6 object \u2014 Configure Thanos network policy rules. networkPolicies.thanos.enabled\u00b6 boolean <code>True</code> \u2014 networkPolicies.velero\u00b6 object \u2014 Configure Velero network policy rules. networkPolicies.velero.enabled\u00b6 boolean <code>True</code> \u2014"},{"location":"operator-manual/schema/config/#note:networkPolicies.certManager.letsencrypt","title":"Notes for <code>networkPolicies.certManager.letsencrypt</code>","text":"<p>Configure network policy rule to allow cert-manager to reach Let's Encrypt.</p> <p>Note</p> <p>Let's Encrypt by choice does not publish a list of their endpoints, so this is required to be <code>ips: [ 0.0.0.0/0 ]</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.certManager.namespaces[]","title":"Notes for <code>networkPolicies.certManager.namespaces[]</code>","text":"<p>Configure namespaces to allow cert-manager HTTP-01 perform HTTP-01 challenges.</p> <p>Examples:</p> <pre><code>['dex', 'harbor', 'monitoring', 'opensearch-system', 'thanos']\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.objectStorage","title":"Notes for <code>networkPolicies.global.objectStorage</code>","text":"<p>Configure object storage network policy rules.</p> <p>This configuration should match the object storage service configured under <code>objectStorage</code>.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.objectStorageSwift","title":"Notes for <code>networkPolicies.global.objectStorageSwift</code>","text":"<p>Configure OpenStack Swift object storage network policy rules.</p> <p>This configuration should match the object storage service configured under <code>objectStorage.swift</code> if used by any component.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.scApiserver","title":"Notes for <code>networkPolicies.global.scApiserver</code>","text":"<p>Configure service cluster API server network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.scIngress","title":"Notes for <code>networkPolicies.global.scIngress</code>","text":"<p>Configure service cluster ingress network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.scNodes","title":"Notes for <code>networkPolicies.global.scNodes</code>","text":"<p>Configure service cluster nodes network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.wcApiserver","title":"Notes for <code>networkPolicies.global.wcApiserver</code>","text":"<p>Configure workload cluster API server network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.wcIngress","title":"Notes for <code>networkPolicies.global.wcIngress</code>","text":"<p>Configure workload cluster ingress network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.global.wcNodes","title":"Notes for <code>networkPolicies.global.wcNodes</code>","text":"<p>Configure workload cluster nodes network policy rules.</p> <p>Tip</p> <p>Automatically populated by <code>./bin/ck8s update-ips &lt;both|sc|wc&gt;</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.prometheus.internalAccess","title":"Notes for <code>networkPolicies.prometheus.internalAccess</code>","text":"<p>Configure network policy rules to allow internal access to Prometheus.</p> <p>This requires the allowed namespaces to be configured under <code>namespaces</code> and the allowed pods to be labeled <code>elastisys.io/prometheus-access: allowed</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:networkPolicies.tektonPipelines.pipeline","title":"Notes for <code>networkPolicies.tektonPipelines.pipeline</code>","text":"<p>Add required networkpolicies for the pipeline under the section pipeline.</p> <p>The networkpolicies should follow the network policies generator. As such, it is possible to use pre-defined network policies rules. The pre-defined rules can be found here.</p> <pre><code>  pipeline:\n    clone-config-pod:\n      podSelectorLabels:\n        tekton.dev/pipeline: upgrade-pipeline\n      ingress: {}\n      egress:\n        - rule: egress-rule-dns # pre-defined network policies rule.\n        - name: egress-rule-config-access\n          peers:\n            - cidr: 1.2.3.4/32\n          ports:\n            - tcp: 22\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#nodelocaldns","title":"<code>nodeLocalDns</code>","text":"<p>Configure node-local-dns, node local DNS resolving and caching.</p> Key Type Default Description nodeLocalDns.customConfig\u00b6 string \u2014 See note nodeLocalDns.hostZone\u00b6 object \u2014 Configure the host zone for node-local-dns nodeLocalDns.hostZone.extraConfig\u00b6 string \u2014 See note nodeLocalDns.resources\u00b6 object \u2014 See note nodeLocalDns.resources.limits\u00b6 object \u2014 \u2014 nodeLocalDns.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:nodeLocalDns.customConfig","title":"Notes for <code>nodeLocalDns.customConfig</code>","text":"<p>Configure custom options for the CoreDNS instance running as part of node-local-dns.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>Examples:</p> <pre><code>example.com:53 {\n  errors\n  cache 30\n  reload\n  loop\n  forward . 127.0.0.1:9005\n}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:nodeLocalDns.hostZone.extraConfig","title":"Notes for <code>nodeLocalDns.hostZone.extraConfig</code>","text":"<p>Configure extra config for the host zone .53 for node-local-dns.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>Examples:</p> <pre><code>template ANY ANY {\n  rcode NXDOMAIN\n}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:nodeLocalDns.resources","title":"Notes for <code>nodeLocalDns.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#objectstorage","title":"<code>objectStorage</code>","text":"<p>Configuration options for using object storage in Welkin</p> <p>This is used for:</p> <ul> <li>Fluentd audit logs</li> <li>Fluentd service cluster logs</li> <li>Harbor database backups and registry storage</li> <li>OpenSearch workload cluster log snapshots</li> <li>Rclone object storage sync source and restore destination</li> <li>Thanos metrics storage</li> <li>Velero resource backups and volume snapshots</li> </ul> <p>Harbor, Rclone, and Thanos have additional configuration to use Swift.</p> Key Type Default Description objectStorage.azure\u00b6 object \u2014 Only supports Azure Public Cloud. objectStorage.azure.resourceGroup\u00b6 string \u2014 Resource group of the storage account. objectStorage.azure.storageAccountName\u00b6 string \u2014 Name of the storage account objectStorage.buckets\u00b6 object \u2014 See note objectStorage.restore\u00b6 object \u2014 See note objectStorage.restore.addTargetsFromSync\u00b6 boolean \u2014 Automatically configure the restore from a secondary site to the primary site.Essentially this will configure Rclone restore to do the inverse of Rclone sync. objectStorage.restore.decrypt\u00b6 object \u2014 Encrypt data when syncing and decrypt data when restoring. objectStorage.restore.decrypt.directoryNames\u00b6 boolean \u2014 Encrypt directory names when syncing, requires file names to be encrypted. objectStorage.restore.decrypt.enabled\u00b6 boolean \u2014 \u2014 objectStorage.restore.decrypt.fileNames\u00b6 boolean \u2014 Encrypt file names when syncing. objectStorage.restore.destinations\u00b6 object \u2014 Allows for complete or partial overrides of the destinations of the restore, the main object storage configuration. objectStorage.restore.destinations.azure\u00b6 object \u2014 Only supports Azure Public Cloud. objectStorage.restore.destinations.azure.resourceGroup\u00b6 string \u2014 Resource group of the storage account. objectStorage.restore.destinations.azure.storageAccountName\u00b6 string \u2014 Name of the storage account objectStorage.restore.destinations.s3\u00b6 object \u2014 Configurations for using S3 storage. objectStorage.restore.destinations.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. objectStorage.restore.destinations.s3.region\u00b6 string \u2014 Region to store data. objectStorage.restore.destinations.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). objectStorage.restore.destinations.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. objectStorage.restore.destinations.swift\u00b6 object \u2014 &gt; [!note]&gt; Supported as an option only for Harbor, Rclone, and Thanos. objectStorage.restore.destinations.swift.authUrl\u00b6 string \u2014 OpenStack authentication URL.Make sure to prepend the protocol (e.g. <code>https://</code>) and append the authentication version (e.g. <code>/v3</code>). objectStorage.restore.destinations.swift.authVersion\u00b6 integer \u2014 OpenStack authentication version.Set <code>0</code> for auto detect from authentication url. objectStorage.restore.destinations.swift.domainId\u00b6 string \u2014 The user domain ID to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.restore.destinations.swift.domainName\u00b6 string \u2014 The user domain name to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.restore.destinations.swift.projectDomainId\u00b6 string \u2014 The project domain ID to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.restore.destinations.swift.projectDomainName\u00b6 string \u2014 The project domain name to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.restore.destinations.swift.projectId\u00b6 string \u2014 The project ID to use.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.restore.destinations.swift.projectName\u00b6 string \u2014 The project name to use, requires project domain to be set.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.restore.destinations.swift.region\u00b6 string \u2014 OpenStack region. objectStorage.restore.destinations.swift.segmentsContainerSuffix\u00b6 string <code>+segments</code> The container suffix to use for segment containers.These are created to store large objects with SLOs/DLOs, and with s3api middleware for multipart uploads. objectStorage.restore.dryrun\u00b6 boolean \u2014 Deploy Rclone with dryrun enabled. objectStorage.restore.enabled\u00b6 boolean \u2014 \u2014 objectStorage.restore.sources\u00b6 object \u2014 Allows for complete or partial overrides of the sources of the restore, the sync object storage configuration. objectStorage.restore.sources.azure\u00b6 object \u2014 Only supports Azure Public Cloud. objectStorage.restore.sources.azure.resourceGroup\u00b6 string \u2014 Resource group of the storage account. objectStorage.restore.sources.azure.storageAccountName\u00b6 string \u2014 Name of the storage account objectStorage.restore.sources.s3\u00b6 object \u2014 Configurations for using S3 storage. objectStorage.restore.sources.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. objectStorage.restore.sources.s3.region\u00b6 string \u2014 Region to store data. objectStorage.restore.sources.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). objectStorage.restore.sources.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. objectStorage.restore.sources.swift\u00b6 object \u2014 &gt; [!note]&gt; Supported as an option only for Harbor, Rclone, and Thanos. objectStorage.restore.sources.swift.authUrl\u00b6 string \u2014 OpenStack authentication URL.Make sure to prepend the protocol (e.g. <code>https://</code>) and append the authentication version (e.g. <code>/v3</code>). objectStorage.restore.sources.swift.authVersion\u00b6 integer \u2014 OpenStack authentication version.Set <code>0</code> for auto detect from authentication url. objectStorage.restore.sources.swift.domainId\u00b6 string \u2014 The user domain ID to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.restore.sources.swift.domainName\u00b6 string \u2014 The user domain name to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.restore.sources.swift.projectDomainId\u00b6 string \u2014 The project domain ID to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.restore.sources.swift.projectDomainName\u00b6 string \u2014 The project domain name to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.restore.sources.swift.projectId\u00b6 string \u2014 The project ID to use.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.restore.sources.swift.projectName\u00b6 string \u2014 The project name to use, requires project domain to be set.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.restore.sources.swift.region\u00b6 string \u2014 OpenStack region. objectStorage.restore.sources.swift.segmentsContainerSuffix\u00b6 string <code>+segments</code> The container suffix to use for segment containers.These are created to store large objects with SLOs/DLOs, and with s3api middleware for multipart uploads. objectStorage.restore.targets[]\u00b6 array of object \u2014 Targets to restoreDetails of a bucket to restore. objectStorage.restore.timestamp\u00b6 string \u2014 Perform point-in-time restore if possible.This is only supported for S3 sources. objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. objectStorage.s3.region\u00b6 string \u2014 Region to store data. objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. objectStorage.swift\u00b6 object \u2014 &gt; [!note]&gt; Supported as an option only for Harbor, Rclone, and Thanos. objectStorage.swift.authUrl\u00b6 string \u2014 OpenStack authentication URL.Make sure to prepend the protocol (e.g. <code>https://</code>) and append the authentication version (e.g. <code>/v3</code>). objectStorage.swift.authVersion\u00b6 integer \u2014 OpenStack authentication version.Set <code>0</code> for auto detect from authentication url. objectStorage.swift.domainId\u00b6 string \u2014 The user domain ID to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.swift.domainName\u00b6 string \u2014 The user domain name to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.swift.projectDomainId\u00b6 string \u2014 The project domain ID to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.swift.projectDomainName\u00b6 string \u2014 The project domain name to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.swift.projectId\u00b6 string \u2014 The project ID to use.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.swift.projectName\u00b6 string \u2014 The project name to use, requires project domain to be set.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.swift.region\u00b6 string \u2014 OpenStack region. objectStorage.swift.segmentsContainerSuffix\u00b6 string <code>+segments</code> The container suffix to use for segment containers.These are created to store large objects with SLOs/DLOs, and with s3api middleware for multipart uploads. objectStorage.sync\u00b6 object \u2014 Sync object storage from the primary site to a secondary site with Rclone. objectStorage.sync.activeDeadlineSeconds\u00b6 number <code>14400</code> The maximum amount of time that the Rclone job is allowed to run (in seconds). objectStorage.sync.azure\u00b6 object \u2014 Only supports Azure Public Cloud. objectStorage.sync.azure.resourceGroup\u00b6 string \u2014 Resource group of the storage account. objectStorage.sync.azure.storageAccountName\u00b6 string \u2014 Name of the storage account objectStorage.sync.buckets[]\u00b6 array of object \u2014 Additional buckets to sync.List of buckets to sync when <code>syncDefaultBuckets</code> is false objectStorage.sync.defaultSchedule\u00b6 string \u2014 \u2014 objectStorage.sync.destinationType\u00b6 string \u2014 See note objectStorage.sync.dryrun\u00b6 boolean \u2014 Deploy Rclone with dryrun enabled. objectStorage.sync.enabled\u00b6 boolean \u2014 \u2014 objectStorage.sync.encrypt\u00b6 object \u2014 Encrypt data when syncing and decrypt data when restoring. objectStorage.sync.encrypt.directoryNames\u00b6 boolean \u2014 Encrypt directory names when syncing, requires file names to be encrypted. objectStorage.sync.encrypt.enabled\u00b6 boolean \u2014 \u2014 objectStorage.sync.encrypt.fileNames\u00b6 boolean \u2014 Encrypt file names when syncing. objectStorage.sync.resources\u00b6 object \u2014 See note objectStorage.sync.resources.limits\u00b6 object \u2014 \u2014 objectStorage.sync.resources.requests\u00b6 object \u2014 \u2014 objectStorage.sync.s3\u00b6 object \u2014 Configurations for using S3 storage. objectStorage.sync.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. objectStorage.sync.s3.region\u00b6 string \u2014 Region to store data. objectStorage.sync.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). objectStorage.sync.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. objectStorage.sync.secondaryUrl\u00b6 string \u2014 \u2014 objectStorage.sync.sourceType\u00b6 string \u2014 See note objectStorage.sync.swift\u00b6 object \u2014 &gt; [!note]&gt; Supported as an option only for Harbor, Rclone, and Thanos. objectStorage.sync.swift.authUrl\u00b6 string \u2014 OpenStack authentication URL.Make sure to prepend the protocol (e.g. <code>https://</code>) and append the authentication version (e.g. <code>/v3</code>). objectStorage.sync.swift.authVersion\u00b6 integer \u2014 OpenStack authentication version.Set <code>0</code> for auto detect from authentication url. objectStorage.sync.swift.domainId\u00b6 string \u2014 The user domain ID to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.sync.swift.domainName\u00b6 string \u2014 The user domain name to use.User domain is required when authenticating with <code>username</code>, set either <code>domainId</code> or <code>domainName</code>. objectStorage.sync.swift.projectDomainId\u00b6 string \u2014 The project domain ID to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.sync.swift.projectDomainName\u00b6 string \u2014 The project domain name to use.Project domain is required when authenticating with <code>projectName</code>, set either <code>projectDomainId</code> or <code>projectDomainName</code>. objectStorage.sync.swift.projectId\u00b6 string \u2014 The project ID to use.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.sync.swift.projectName\u00b6 string \u2014 The project name to use, requires project domain to be set.Project is required when authenticating with <code>username</code>, set either <code>projectId</code> or <code>projectName</code>. objectStorage.sync.swift.region\u00b6 string \u2014 OpenStack region. objectStorage.sync.swift.segmentsContainerSuffix\u00b6 string <code>+segments</code> The container suffix to use for segment containers.These are created to store large objects with SLOs/DLOs, and with s3api middleware for multipart uploads. objectStorage.sync.syncDefaultBuckets\u00b6 boolean \u2014 Sync the buckets or containers set under <code>.objectStorage.buckets</code>. objectStorage.type\u00b6 string \u2014 See note"},{"location":"operator-manual/schema/config/#note:objectStorage.buckets","title":"Notes for <code>objectStorage.buckets</code>","text":"<p>Buckets or containers for each respective application to use for application data or backup storage.</p> <p>Keys are used as identifiers for buckets or containers, while the values are used as the bucket or container name.</p> <p>Additional entries added here will have monitoring enabled.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:objectStorage.restore","title":"Notes for <code>objectStorage.restore</code>","text":"<p>Restore object storage from a secondary site to the primary site with Rclone.</p> <p>Note</p> <p>When enabled this will disable Rclone sync to prevent it from modifying the secondary site.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:objectStorage.sync.destinationType","title":"Notes for <code>objectStorage.sync.destinationType</code>","text":"<p>Object storage type to use.</p> <p>Possible values:</p> <pre><code>azure\n</code></pre> <pre><code>gcs\n</code></pre> <pre><code>s3\n</code></pre> <pre><code>swift\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:objectStorage.sync.resources","title":"Notes for <code>objectStorage.sync.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:objectStorage.sync.sourceType","title":"Notes for <code>objectStorage.sync.sourceType</code>","text":"<p>Object storage type to use. Defaults to .objectStorage.type</p> <p>Examples:</p> <pre><code>azure\n</code></pre> <pre><code>gcs\n</code></pre> <pre><code>s3\n</code></pre> <pre><code>swift\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:objectStorage.type","title":"Notes for <code>objectStorage.type</code>","text":"<p>Object storage type to use.</p> <p>In addition to this Harbor, Rclone, and Thanos have additional configuration to use Swift.</p> <p>Possible values:</p> <pre><code>azure\n</code></pre> <pre><code>gcs\n</code></pre> <pre><code>s3\n</code></pre> <pre><code>none\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#opa","title":"<code>opa</code>","text":"<p>Configure Open Policy Agent, constraints and mutations enforced by Gatekeeper.</p> <p>Welkin contains multiple safeguards to make it easy to follow security best practices.</p> <p>This includes an implementation of constraints and mutations with similar behaviour as Pod Security Policies, and application developer centric safeguards.</p> Key Type Default Description opa.audit\u00b6 object \u2014 Configure the Audit deployment of OPA Gatekeeper. opa.audit.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opa.audit.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opa.audit.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opa.audit.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opa.audit.nodeSelector\u00b6 object \u2014 See note opa.audit.resources\u00b6 object \u2014 See note opa.audit.resources.limits\u00b6 object \u2014 \u2014 opa.audit.resources.requests\u00b6 object \u2014 \u2014 opa.audit.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opa.audit.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. opa.audit.writeToRAMDisk\u00b6 boolean \u2014 \u2014 opa.auditChunkSize\u00b6 number <code>500</code> \u2014 opa.auditFromCache\u00b6 boolean \u2014 \u2014 opa.auditIntervalSeconds\u00b6 number <code>600</code> \u2014 opa.constraintViolationsLimit\u00b6 number <code>20</code> \u2014 opa.controllerManager\u00b6 object \u2014 This is meant to describe the base class if you will, for Welkin resources. opa.controllerManager.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opa.controllerManager.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opa.controllerManager.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opa.controllerManager.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opa.controllerManager.enabled\u00b6 boolean \u2014 \u2014 opa.controllerManager.extraArgs[]\u00b6 array of string \u2014 Extra arguments passed to a container opa.controllerManager.nodeSelector\u00b6 object \u2014 See note opa.controllerManager.resources\u00b6 object \u2014 See note opa.controllerManager.resources.limits\u00b6 object \u2014 \u2014 opa.controllerManager.resources.requests\u00b6 object \u2014 \u2014 opa.controllerManager.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opa.controllerManager.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. opa.disallowedTags\u00b6 object \u2014 See note opa.disallowedTags.enabled\u00b6 boolean <code>True</code> \u2014 opa.disallowedTags.enforcement\u00b6 string <code>deny</code> See note opa.disallowedTags.tags[]\u00b6 array of string \u2014 Configure the tags that should be disallowed by the constraint. opa.imageRegistry\u00b6 object \u2014 See note opa.imageRegistry.URL[]\u00b6 array of string \u2014 See note opa.imageRegistry.enabled\u00b6 boolean <code>True</code> \u2014 opa.imageRegistry.enforcement\u00b6 string <code>warn</code> See note opa.minimumDeploymentReplicas\u00b6 object \u2014 See note opa.minimumDeploymentReplicas.enabled\u00b6 boolean <code>True</code> \u2014 opa.minimumDeploymentReplicas.enforcement\u00b6 string <code>warn</code> See note opa.mutatingWebhookTimeoutSeconds\u00b6 number <code>5</code> \u2014 opa.mutations\u00b6 object \u2014 Configure mutations to set defaults in deployed resources. opa.mutations.enabled\u00b6 boolean <code>True</code> \u2014 opa.mutations.jobTTL\u00b6 object \u2014 See note opa.mutations.jobTTL.enabled\u00b6 boolean <code>True</code> \u2014 opa.mutations.jobTTL.ttlSeconds\u00b6 number <code>604800</code> \u2014 opa.mutations.ndots\u00b6 object \u2014 Configure mutations to set ndots on deployed Pods. opa.mutations.ndots.enabled\u00b6 boolean \u2014 \u2014 opa.mutations.ndots.labelSelector\u00b6 object \u2014 Configure the label selector for pods to be targeted by this mutation. opa.mutations.ndots.labelSelector.matchLabels\u00b6 object \u2014 Configure the label selector for pods to be targeted by this mutation.Default <code>{}</code> targets all Pods. opa.mutations.ndots.ndotAmount\u00b6 integer <code>3</code> \u2014 opa.networkPolicies\u00b6 object \u2014 See note opa.networkPolicies.enabled\u00b6 boolean <code>True</code> \u2014 opa.networkPolicies.enforcement\u00b6 string <code>warn</code> See note opa.preventAccidentalDeletion\u00b6 object \u2014 Configure constraint to reject deletion of sensitive resources. opa.preventAccidentalDeletion.enabled\u00b6 boolean \u2014 \u2014 opa.preventAccidentalDeletion.enforcement\u00b6 string <code>deny</code> See note opa.rejectLoadBalancerService\u00b6 object \u2014 See note opa.rejectLoadBalancerService.enabled\u00b6 boolean \u2014 \u2014 opa.rejectLoadBalancerService.enforcement\u00b6 string <code>deny</code> See note opa.rejectLocalStorageEmptyDir\u00b6 object \u2014 See note opa.rejectLocalStorageEmptyDir.enabled\u00b6 boolean \u2014 \u2014 opa.rejectLocalStorageEmptyDir.enforcement\u00b6 string <code>warn</code> See note opa.rejectPodWithoutController\u00b6 object \u2014 See note opa.rejectPodWithoutController.enabled\u00b6 boolean \u2014 \u2014 opa.rejectPodWithoutController.enforcement\u00b6 string <code>warn</code> See note opa.resourceRequests\u00b6 object \u2014 See note opa.resourceRequests.enabled\u00b6 boolean <code>True</code> \u2014 opa.resourceRequests.enforcement\u00b6 string <code>deny</code> See note opa.restrictPodDisruptionBudgets\u00b6 object \u2014 See note opa.restrictPodDisruptionBudgets.enabled\u00b6 boolean <code>True</code> \u2014 opa.restrictPodDisruptionBudgets.enforcement\u00b6 string <code>deny</code> See note opa.validatingWebhookTimeoutSeconds\u00b6 number <code>5</code> \u2014"},{"location":"operator-manual/schema/config/#note:opa.audit.nodeSelector","title":"Notes for <code>opa.audit.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.audit.resources","title":"Notes for <code>opa.audit.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.controllerManager.nodeSelector","title":"Notes for <code>opa.controllerManager.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.controllerManager.resources","title":"Notes for <code>opa.controllerManager.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.disallowedTags","title":"Notes for <code>opa.disallowedTags</code>","text":"<p>Configure constraint to disallow configured tags on container images.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.disallowedTags.enforcement","title":"Notes for <code>opa.disallowedTags.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.imageRegistry","title":"Notes for <code>opa.imageRegistry</code>","text":"<p>Configure constraint to only allow configured registries for container images.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.imageRegistry.URL[]","title":"Notes for <code>opa.imageRegistry.URL[]</code>","text":"<p>Configure the registries that should be trusted by the constraint.</p> <p>Note</p> <p>To support issuing certificates with HTTP-01 challenges the registry <code>quay.io/jetstack/cert-manager-acmesolver</code> must be added.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.imageRegistry.enforcement","title":"Notes for <code>opa.imageRegistry.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.minimumDeploymentReplicas","title":"Notes for <code>opa.minimumDeploymentReplicas</code>","text":"<p>Configure constraint to only allow Deployments and StatefulSets with more than one replica.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.minimumDeploymentReplicas.enforcement","title":"Notes for <code>opa.minimumDeploymentReplicas.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.mutations.jobTTL","title":"Notes for <code>opa.mutations.jobTTL</code>","text":"<p>Configure mutations to set time to live on deployed Jobs.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.networkPolicies","title":"Notes for <code>opa.networkPolicies</code>","text":"<p>Configure constraint to only allow Pods targeted by NetworkPolicies.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.networkPolicies.enforcement","title":"Notes for <code>opa.networkPolicies.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.preventAccidentalDeletion.enforcement","title":"Notes for <code>opa.preventAccidentalDeletion.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectLoadBalancerService","title":"Notes for <code>opa.rejectLoadBalancerService</code>","text":"<p>Configure constraint to reject creation of Services with the type LoadBalancer.</p> <p>Advantageous if the cluster cannot automatically provision LoadBalancers, e.g. because the infrastructure provider do not offer such Kubernetes integration.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectLoadBalancerService.enforcement","title":"Notes for <code>opa.rejectLoadBalancerService.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectLocalStorageEmptyDir","title":"Notes for <code>opa.rejectLocalStorageEmptyDir</code>","text":"<p>Configure constraint to reject usage of local storage emptydir.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectLocalStorageEmptyDir.enforcement","title":"Notes for <code>opa.rejectLocalStorageEmptyDir.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectPodWithoutController","title":"Notes for <code>opa.rejectPodWithoutController</code>","text":"<p>Configure constraint to reject pods without a controller.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.rejectPodWithoutController.enforcement","title":"Notes for <code>opa.rejectPodWithoutController.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.resourceRequests","title":"Notes for <code>opa.resourceRequests</code>","text":"<p>Configure constraint to only allow Pods configured with resource requests.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.resourceRequests.enforcement","title":"Notes for <code>opa.resourceRequests.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.restrictPodDisruptionBudgets","title":"Notes for <code>opa.restrictPodDisruptionBudgets</code>","text":"<p>Configure constraint to reject PodDisruptionBudgets and connected Pod controllers if the PDB does not allow for at least 1 pod disruption.</p> <p>Note</p> <p>See the dev docs for context.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opa.restrictPodDisruptionBudgets.enforcement","title":"Notes for <code>opa.restrictPodDisruptionBudgets.enforcement</code>","text":"<p>Possible values:</p> <pre><code>deny\n</code></pre> <pre><code>warn\n</code></pre> <pre><code>dryrun\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#opensearch","title":"<code>opensearch</code>","text":"<p>Configuration for OpenSearch.</p> <p>OpenSearch ingests logs sent from Fluentd in the workload cluster, and presents them in OpenSearch Dashboards.</p> <p>Note</p> <p>OpenSearch and its components are installed in the service cluster, so this configuration mainly applies there.</p> Key Type Default Description opensearch.additionalTemplates\u00b6 object \u2014 See note opensearch.clientNode\u00b6 object \u2014 Configures the client stateful set of OpenSearch that takes on the roll to ingest and query logs. opensearch.clientNode.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opensearch.clientNode.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opensearch.clientNode.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opensearch.clientNode.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opensearch.clientNode.count\u00b6 number <code>1</code> \u2014 opensearch.clientNode.dedicatedPods\u00b6 boolean <code>True</code> When disabled the master nodes will take on these rolls. opensearch.clientNode.javaOpts\u00b6 string <code>-Xms512m -Xmx512m</code> See note opensearch.clientNode.nodeSelector\u00b6 object \u2014 See note opensearch.clientNode.resources\u00b6 object \u2014 See note opensearch.clientNode.resources.limits\u00b6 object \u2014 \u2014 opensearch.clientNode.resources.requests\u00b6 object \u2014 \u2014 opensearch.clientNode.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.clusterName\u00b6 string <code>opensearch</code> \u2014 opensearch.createIndices\u00b6 boolean <code>True</code> See note opensearch.curator\u00b6 object \u2014 Configures the CronJob that removes indices. opensearch.curator.activeDeadlineSeconds\u00b6 number <code>2700</code> \u2014 opensearch.curator.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opensearch.curator.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opensearch.curator.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opensearch.curator.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opensearch.curator.enabled\u00b6 boolean <code>True</code> \u2014 opensearch.curator.nodeSelector\u00b6 object \u2014 See note opensearch.curator.resources\u00b6 object \u2014 See note opensearch.curator.resources.limits\u00b6 object \u2014 \u2014 opensearch.curator.resources.requests\u00b6 object \u2014 \u2014 opensearch.curator.retention[]\u00b6 array of object <code>[{'pattern': 'authlog-*', 'ageDays': 30, 'sizeGB': 1}, {'pattern': 'kubeaudit-*', 'ageDays': 30, 'sizeGB': 50}, {'pattern': 'kubernetes-*', 'ageDays': 30, 'sizeGB': 50}, {'pattern': 'other-*', 'ageDays': 7, 'sizeGB': 1}, {'pattern': 'security-auditlog-*', 'ageDays': 7, 'sizeGB': 1}]</code> Configures the retention of indices in OpenSearch.Configures the retention of indices in OpenSearch. opensearch.curator.startingDeadlineSeconds\u00b6 number <code>600</code> \u2014 opensearch.curator.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.dashboards\u00b6 object \u2014 Configures the Dashboards deployment of OpenSearch providing the UI to view and query logs. opensearch.dashboards.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opensearch.dashboards.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opensearch.dashboards.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opensearch.dashboards.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opensearch.dashboards.autocompleteTerminateAfter\u00b6 number \u2014 \u2014 opensearch.dashboards.contentSecurityPolicy\u00b6 object \u2014 Configure Content-Security-Policy header rulesReference: https://content-security-policy.com/ opensearch.dashboards.cookieTtl\u00b6 integer \u2014 Time-to-live for the session cookie in milliseconds. Overrides OpenSearch Dashboards internal default if set. opensearch.dashboards.nodeSelector\u00b6 object \u2014 See note opensearch.dashboards.resources\u00b6 object \u2014 See note opensearch.dashboards.resources.limits\u00b6 object \u2014 \u2014 opensearch.dashboards.resources.requests\u00b6 object \u2014 \u2014 opensearch.dashboards.sessionKeepalive\u00b6 boolean \u2014 Whether the session TTL should be extended upon user activity. Overrides OpenSearch Dashboards internal default if set. opensearch.dashboards.sessionTtl\u00b6 integer \u2014 Time-to-live for the session itself in milliseconds. Overrides OpenSearch Dashboards internal default if set. opensearch.dashboards.subdomain\u00b6 string <code>opensearch</code> See note opensearch.dashboards.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.dashboards.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. opensearch.dataNode\u00b6 object \u2014 Configures the data stateful set of OpenSearch that takes on the roll to index and store logs. opensearch.dataNode.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opensearch.dataNode.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opensearch.dataNode.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opensearch.dataNode.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opensearch.dataNode.count\u00b6 number <code>2</code> \u2014 opensearch.dataNode.dedicatedPods\u00b6 boolean <code>True</code> When disabled the master nodes will take on these rolls. opensearch.dataNode.javaOpts\u00b6 string <code>-Xms512m -Xmx512m</code> See note opensearch.dataNode.nodeSelector\u00b6 object \u2014 See note opensearch.dataNode.resources\u00b6 object \u2014 See note opensearch.dataNode.resources.limits\u00b6 object \u2014 \u2014 opensearch.dataNode.resources.requests\u00b6 object \u2014 \u2014 opensearch.dataNode.storageClass\u00b6 -string- -null- \u2014 See note opensearch.dataNode.storageSize\u00b6 string \u2014 Configure the requested size of the persistent volume for this OpenSerch node. opensearch.dataNode.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.defaultTemplates\u00b6 boolean <code>True</code> See note opensearch.enabled\u00b6 boolean <code>True</code> &gt; [!note]&gt; Must be set for both service and workload cluster. opensearch.exporter\u00b6 object \u2014 Configures the exporter exposing metrics from OpenSearch. opensearch.exporter.resources\u00b6 object \u2014 See note opensearch.exporter.resources.limits\u00b6 object \u2014 \u2014 opensearch.exporter.resources.requests\u00b6 object \u2014 \u2014 opensearch.exporter.serviceMonitor\u00b6 object \u2014 Configures the service monitor of the exporter. opensearch.exporter.serviceMonitor.interval\u00b6 string <code>30s</code> \u2014 opensearch.exporter.serviceMonitor.scrapeTimeout\u00b6 string \u2014 \u2014 opensearch.exporter.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.extraRoleMappings[]\u00b6 array of object \u2014 See note opensearch.extraRoles[]\u00b6 array of object \u2014 See note opensearch.indexPerNamespace\u00b6 boolean \u2014 See note opensearch.ingress\u00b6 object \u2014 Configures the ingress for OpenSearch master or client nodes. opensearch.ingress.maxbodysize\u00b6 string <code>32m</code> \u2014 opensearch.ism\u00b6 object \u2014 Configures index state management in OpenSearch. opensearch.ism.additionalPolicies\u00b6 object \u2014 See note opensearch.ism.authlog\u00b6 object \u2014 Configures rollover for <code>authlog</code> index opensearch.ism.authlog.rolloverAgeDays\u00b6 number <code>1</code> Configures the age a write index must reach before it is rolled over to a new one. opensearch.ism.authlog.rolloverSizeMB\u00b6 number <code>1000</code> Configures the size a write index must reach before it is rolled over to a new one. opensearch.ism.defaultPolicies\u00b6 boolean <code>True</code> See note opensearch.ism.kubeaudit\u00b6 object \u2014 Configures rollover for <code>kubeaudit</code> index opensearch.ism.kubeaudit.rolloverAgeDays\u00b6 number <code>1</code> Configures the age a write index must reach before it is rolled over to a new one. opensearch.ism.kubeaudit.rolloverSizeMB\u00b6 number <code>1000</code> Configures the size a write index must reach before it is rolled over to a new one. opensearch.ism.kubernetes\u00b6 object \u2014 Configures rollover for <code>kubernetes</code> index opensearch.ism.kubernetes.rolloverAgeDays\u00b6 number <code>1</code> Configures the age a write index must reach before it is rolled over to a new one. opensearch.ism.kubernetes.rolloverSizeMB\u00b6 number <code>1000</code> Configures the size a write index must reach before it is rolled over to a new one. opensearch.ism.other\u00b6 object \u2014 Configures rollover for <code>other</code> index opensearch.ism.other.rolloverAgeDays\u00b6 number <code>1</code> Configures the age a write index must reach before it is rolled over to a new one. opensearch.ism.other.rolloverSizeMB\u00b6 number <code>1000</code> Configures the size a write index must reach before it is rolled over to a new one. opensearch.ism.overwritePolicies\u00b6 boolean <code>True</code> When set OpenSearch can be configured with index state management policies via <code>additionalPolicies</code> that overwrite the ones configured via <code>defaultPolicies</code>. opensearch.masterNode\u00b6 object \u2014 Configures the main stateful set of OpenSearch that takes on all roles not provided by other nodes (<code>dataNode</code>, <code>clientNode</code>). opensearch.masterNode.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. opensearch.masterNode.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. opensearch.masterNode.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). opensearch.masterNode.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). opensearch.masterNode.count\u00b6 number <code>1</code> \u2014 opensearch.masterNode.javaOpts\u00b6 string <code>-Xms512m -Xmx512m</code> See note opensearch.masterNode.nodeSelector\u00b6 object \u2014 See note opensearch.masterNode.resources\u00b6 object \u2014 See note opensearch.masterNode.resources.limits\u00b6 object \u2014 \u2014 opensearch.masterNode.resources.requests\u00b6 object \u2014 \u2014 opensearch.masterNode.storageClass\u00b6 -string- -null- \u2014 See note opensearch.masterNode.storageSize\u00b6 string \u2014 Configure the requested size of the persistent volume for this OpenSerch node. opensearch.masterNode.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration opensearch.maxClauseCount\u00b6 number <code>1024</code> Configures the maximum number of clauses permitted in a query. opensearch.maxShardsPerNode\u00b6 number <code>1000</code> Configures the maximum number of shards permitted on one node. opensearch.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to OpenSearch. opensearch.objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. opensearch.objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. opensearch.objectStorage.s3.region\u00b6 string \u2014 Region to store data. opensearch.objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). opensearch.objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. opensearch.overwriteTemplates\u00b6 boolean <code>True</code> When set OpenSearch can be configured with index templates via <code>additionalTemplates</code> that overwrite the ones configured via <code>defaultTemplates</code>. opensearch.plugins\u00b6 object \u2014 Configures plugins used in OpenSearch. opensearch.plugins.additionalPlugins[]\u00b6 array \u2014 Configures OpenSearch to install plugins when it starts.In an air-gapped environment this can be used to install plugins from known sources. opensearch.plugins.installExternalObjectStoragePlugin\u00b6 boolean \u2014 See note opensearch.promIndexAlerts[]\u00b6 array of object <code>[{'prefix': 'authlog-default', 'alertSizeMB': 3}, {'prefix': 'kubeaudit-default', 'alertSizeMB': 5500}, {'prefix': 'kubernetes-default', 'alertSizeMB': 5500}, {'prefix': 'other-default', 'alertSizeMB': 400}]</code> Configures the index alerts monitoring the function of index state management.Configures the index alert monitoring the function of index state management. opensearch.securityadmin\u00b6 object \u2014 Configures the Job that initialises OpenSearch Security. opensearch.securityadmin.activeDeadlineSeconds\u00b6 number <code>1200</code> \u2014 opensearch.securityadmin.enabled\u00b6 boolean <code>True</code> \u2014 opensearch.securityadmin.resources\u00b6 object \u2014 See note opensearch.securityadmin.resources.limits\u00b6 object \u2014 \u2014 opensearch.securityadmin.resources.requests\u00b6 object \u2014 \u2014 opensearch.snapshot\u00b6 object \u2014 Configure OpenSearch snapshot creation and retention.This requires that <code>objectStorage</code> is configured, and will use the bucket or container set in <code>objectStorage.buckets.opensearch</code>. opensearch.snapshot.backupSchedule\u00b6 string \u2014 \u2014 opensearch.snapshot.enabled\u00b6 boolean <code>True</code> \u2014 opensearch.snapshot.max\u00b6 number <code>14</code> \u2014 opensearch.snapshot.min\u00b6 number <code>7</code> \u2014 opensearch.snapshot.repository\u00b6 string <code>opensearch-snapshots</code> \u2014 opensearch.snapshot.retentionAge\u00b6 string <code>10d</code> \u2014 opensearch.snapshot.retentionSchedule\u00b6 string \u2014 \u2014 opensearch.sso\u00b6 object \u2014 Configures Single Sign On to OpenSearch via Dex. opensearch.sso.enabled\u00b6 boolean \u2014 \u2014 opensearch.sso.rolesKey\u00b6 string <code>groups</code> \u2014 opensearch.sso.scope\u00b6 string <code>openid profile email groups</code> \u2014 opensearch.sso.subjectKey\u00b6 string <code>email</code> \u2014 opensearch.subdomain\u00b6 string <code>opensearch</code> See note"},{"location":"operator-manual/schema/config/#note:opensearch.additionalTemplates","title":"Notes for <code>opensearch.additionalTemplates</code>","text":"<p>When set OpenSearch will be configured with additional index templates.</p> <p>The keys will be used as the name of the index templates.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.clientNode.javaOpts","title":"Notes for <code>opensearch.clientNode.javaOpts</code>","text":"<p>Set Java Virtual Machine Options to control the memory allocation of OpenSearch.</p> <p>As a rule of thumb the minimum allocation <code>-Xms</code> and maximum allocation <code>-Xmx</code> arguments should be the same to be more predictable. Additionally until memory allocation is at 2 GiB and more it is recommended that the memory limit set in Kubernetes is twice the allocation as OpenSearch uses this for cache.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.clientNode.nodeSelector","title":"Notes for <code>opensearch.clientNode.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.clientNode.resources","title":"Notes for <code>opensearch.clientNode.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.createIndices","title":"Notes for <code>opensearch.createIndices</code>","text":"<p>When enabled OpenSearch will be configured with initial indices for:</p> <ul> <li><code>authlog</code></li> <li><code>kubeaudit</code></li> <li><code>kubernetes</code></li> <li><code>other</code></li> </ul> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.curator.nodeSelector","title":"Notes for <code>opensearch.curator.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.curator.resources","title":"Notes for <code>opensearch.curator.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dashboards.nodeSelector","title":"Notes for <code>opensearch.dashboards.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dashboards.resources","title":"Notes for <code>opensearch.dashboards.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dashboards.subdomain","title":"Notes for <code>opensearch.dashboards.subdomain</code>","text":"<p>Subdomain of <code>baseDomain</code> that the Ingress to OpenSearch Dashboards will be created with.</p> <p>Note</p> <p>Must be set for both service and workload cluster.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dataNode.javaOpts","title":"Notes for <code>opensearch.dataNode.javaOpts</code>","text":"<p>Set Java Virtual Machine Options to control the memory allocation of OpenSearch.</p> <p>As a rule of thumb the minimum allocation <code>-Xms</code> and maximum allocation <code>-Xmx</code> arguments should be the same to be more predictable. Additionally until memory allocation is at 2 GiB and more it is recommended that the memory limit set in Kubernetes is twice the allocation as OpenSearch uses this for cache.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dataNode.nodeSelector","title":"Notes for <code>opensearch.dataNode.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dataNode.resources","title":"Notes for <code>opensearch.dataNode.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.dataNode.storageClass","title":"Notes for <code>opensearch.dataNode.storageClass</code>","text":"<p>Set storage class for OpenSearch.</p> <ul> <li>If set to <code>null</code>, the default storage class will be used to provision the volumes.</li> <li>If set to <code>-</code>, no storage class will be used to provision the volumes.</li> </ul> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.defaultTemplates","title":"Notes for <code>opensearch.defaultTemplates</code>","text":"<p>When enabled OpenSearch will be configured with the default index templates for:</p> <ul> <li><code>authlog</code></li> <li><code>kubeaudit</code></li> <li><code>kubernetes</code></li> <li><code>other</code></li> </ul> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.exporter.resources","title":"Notes for <code>opensearch.exporter.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.extraRoleMappings[]","title":"Notes for <code>opensearch.extraRoleMappings[]</code>","text":"<p>Configures extra role mappings for OpenSearch Security.</p> <p>Extra users can be configured in <code>secrets.yaml</code> under <code>extraUsers</code> and extra roles under <code>extraRoles</code>.</p> <p>Configures a role mapping for OpenSearch Security.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.extraRoles[]","title":"Notes for <code>opensearch.extraRoles[]</code>","text":"<p>Configures extra roles for OpenSearch Security.</p> <p>Configures a role for OpenSearch Security.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.indexPerNamespace","title":"Notes for <code>opensearch.indexPerNamespace</code>","text":"<p>When enabled logs are ingested into multiple indices per namespace. When disabled logs are ingested into a single <code>kubernetes</code> index.</p> <p>Important</p> <p>When enabling this feature, you must also add an entry to <code>opensearch.curator.retention</code> in <code>sc-config.yaml</code> with <code>pattern: ^[^.].*</code> which matches all non-system indices.</p> <p>Note</p> <p>Must be set for both service and workload cluster.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.ism.additionalPolicies","title":"Notes for <code>opensearch.ism.additionalPolicies</code>","text":"<p>When set OpenSearch will be configured with additional index state management policies.</p> <p>The keys will be used as the name of the index state management policy.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.ism.defaultPolicies","title":"Notes for <code>opensearch.ism.defaultPolicies</code>","text":"<p>When enabled OpenSearch will be configured with the default index state management policies for:</p> <ul> <li><code>authlog</code></li> <li><code>kubeaudit</code></li> <li><code>kubernetes</code></li> <li><code>other</code></li> </ul> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.masterNode.javaOpts","title":"Notes for <code>opensearch.masterNode.javaOpts</code>","text":"<p>Set Java Virtual Machine Options to control the memory allocation of OpenSearch.</p> <p>As a rule of thumb the minimum allocation <code>-Xms</code> and maximum allocation <code>-Xmx</code> arguments should be the same to be more predictable. Additionally until memory allocation is at 2 GiB and more it is recommended that the memory limit set in Kubernetes is twice the allocation as OpenSearch uses this for cache.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.masterNode.nodeSelector","title":"Notes for <code>opensearch.masterNode.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.masterNode.resources","title":"Notes for <code>opensearch.masterNode.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.masterNode.storageClass","title":"Notes for <code>opensearch.masterNode.storageClass</code>","text":"<p>Set storage class for OpenSearch.</p> <ul> <li>If set to <code>null</code>, the default storage class will be used to provision the volumes.</li> <li>If set to <code>-</code>, no storage class will be used to provision the volumes.</li> </ul> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.plugins.installExternalObjectStoragePlugin","title":"Notes for <code>opensearch.plugins.installExternalObjectStoragePlugin</code>","text":"<p>When enabled OpenSearch will install the required object storage plugin when it starts.</p> <p>In an air-gapped environment where the nodes are not connected to the Internet, set this to <code>false</code> to prevent downloading any external object storage plugins.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.securityadmin.resources","title":"Notes for <code>opensearch.securityadmin.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:opensearch.subdomain","title":"Notes for <code>opensearch.subdomain</code>","text":"<p>Subdomain of <code>opsDomain</code> that the Ingress to OpenSearch will be created with.</p> <p>Note</p> <p>Must be set for both service and workload cluster.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#openstackmonitoring","title":"<code>openstackMonitoring</code>","text":"<p>Configure the collection of metrics for OpenStack components.</p> Key Type Default Description openstackMonitoring.enabled\u00b6 boolean \u2014 \u2014"},{"location":"operator-manual/schema/config/#prometheus","title":"<code>prometheus</code>","text":"<p>Configure Prometheus.</p> <p>Prometheus automatically collects metrics via ServiceMonitors, PodMonitors, and Probes, and pushes metrics to Thanos for long term storage. Additionally Prometheus evaluates recording rules for both service and workload cluster, and all alerting rules for the workload cluster.</p> <p>Note</p> <p>Prometheus is installed in both service cluster and workload cluster, so this configuration applies there with some exceptions.</p> Key Type Default Description prometheus.additionalScrapeConfigs[]\u00b6 array \u2014 See note prometheus.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. prometheus.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. prometheus.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). prometheus.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). prometheus.alertmanagerSpec\u00b6 object \u2014 See note prometheus.alertmanagerSpec.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. prometheus.alertmanagerSpec.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. prometheus.alertmanagerSpec.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). prometheus.alertmanagerSpec.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). prometheus.alertmanagerSpec.groupBy[]\u00b6 array of string \u2014 See note prometheus.alertmanagerSpec.replicas\u00b6 number <code>2</code> \u2014 prometheus.alertmanagerSpec.resources\u00b6 object \u2014 See note prometheus.alertmanagerSpec.resources.limits\u00b6 object \u2014 \u2014 prometheus.alertmanagerSpec.resources.requests\u00b6 object \u2014 \u2014 prometheus.alertmanagerSpec.storage\u00b6 object \u2014 Configure persistent storage for Alertmanager. prometheus.alertmanagerSpec.storage.volumeClaimTemplate\u00b6 object \u2014 Configure persistent storage for Alertmanager. prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec\u00b6 object \u2014 Configure persistent storage for Alertmanager. prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.accessModes[]\u00b6 array of string \u2014 Configure the access mode of the persistent storage for Alertmanager. prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources\u00b6 object \u2014 See note prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.limits\u00b6 object \u2014 \u2014 prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources.requests\u00b6 object \u2014 \u2014 prometheus.alertmanagerSpec.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration prometheus.alertmanagerSpec.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. prometheus.autoscaledNodeGroupAlerts\u00b6 object \u2014 Configure whether to split KubeletDownForXm alerts into autoscaled and non-autoscaled nodes groups. prometheus.autoscaledNodeGroupAlerts.enabled\u00b6 boolean <code>True</code> \u2014 prometheus.autoscaledNodeGroupAlerts.groupLabel\u00b6 string <code>node-restriction.kubernetes.io/autoscaled-node-type</code> The label to identity whether a node belongs to an autoscaled node group. prometheus.autoscaledNodeGroupAlerts.groupLabelValues[]\u00b6 array of string \u2014 The label values to a autoscaled node group if their are multiple autoscaled node groups. prometheus.capacityManagementAlerts\u00b6 object \u2014 Configure capacity management alerts. prometheus.capacityManagementAlerts.disklimit\u00b6 number <code>75</code> Alert when a disk's usage reaches the limit in percent. prometheus.capacityManagementAlerts.enabled\u00b6 boolean <code>True</code> \u2014 prometheus.capacityManagementAlerts.nodeGroupRequestsExcludePattern\u00b6 string \u2014 See note prometheus.capacityManagementAlerts.persistentVolume\u00b6 object \u2014 Configure capacity management alerts on persistent volumes. prometheus.capacityManagementAlerts.persistentVolume.enabled\u00b6 boolean <code>True</code> \u2014 prometheus.capacityManagementAlerts.persistentVolume.limit\u00b6 number <code>75</code> Alert when a persistent volume's usage reaches the limit in percent. prometheus.capacityManagementAlerts.predictUsage\u00b6 boolean \u2014 \u2014 prometheus.capacityManagementAlerts.requestLimit\u00b6 object \u2014 Alert when a node's resource requests reaches the limits in percent. prometheus.capacityManagementAlerts.requestLimit.cpu\u00b6 number <code>80</code> Configure a CPU request percentage limit to alert for. prometheus.capacityManagementAlerts.requestLimit.memory\u00b6 number <code>80</code> Configure a memory request percentage limit to alert for. prometheus.capacityManagementAlerts.usagelimit\u00b6 number <code>95</code> \u2014 prometheus.devAlertmanager\u00b6 object \u2014 Configuration options for deploying an application developer-specific Alertmanager. Configuration shared with the service cluster alertmanager can be configured via <code>.alertmanagerSpec</code>. prometheus.devAlertmanager.enabled\u00b6 boolean \u2014 Allows to enable alertmanager for application developer. prometheus.devAlertmanager.ingressEnabled\u00b6 boolean \u2014 Allows to have ingress for application developer alertmanager with basic auth prometheus.devAlertmanager.namespace\u00b6 string <code>alertmanager</code> Allows to have alertmanager running in custom namespace prometheus.devAlertmanager.username\u00b6 string \u2014 \u2014 prometheus.diskAlerts\u00b6 object \u2014 Configure disk alerts. prometheus.diskAlerts.inode\u00b6 object \u2014 Configure disk alerts based on inode usage. prometheus.diskAlerts.inode.predictLinear[]\u00b6 array of object \u2014 See note prometheus.diskAlerts.inode.space[]\u00b6 array of object \u2014 See note prometheus.diskAlerts.perf\u00b6 object \u2014 Configure performance disk alerts. prometheus.diskAlerts.perf.enabled\u00b6 boolean \u2014 \u2014 prometheus.diskAlerts.perf.queueSizeThreshold\u00b6 number <code>5</code> \u2014 prometheus.diskAlerts.perf.readWaitTimeThreshold\u00b6 number <code>1</code> \u2014 prometheus.diskAlerts.perf.writeWaitTimeThreshold\u00b6 number <code>1</code> \u2014 prometheus.diskAlerts.storage\u00b6 object \u2014 Configure disk alerts based on storage usage. prometheus.diskAlerts.storage.predictLinear[]\u00b6 array of object \u2014 See note prometheus.diskAlerts.storage.space[]\u00b6 array of object \u2014 See note prometheus.nodeSelector\u00b6 object \u2014 See note prometheus.replicas\u00b6 number <code>1</code> \u2014 prometheus.resources\u00b6 object \u2014 See note prometheus.resources.limits\u00b6 object \u2014 \u2014 prometheus.resources.requests\u00b6 object \u2014 \u2014 prometheus.retention\u00b6 object \u2014 Configure retention for Prometheus. prometheus.retention.age\u00b6 string <code>3d</code> Configure the time range Prometheus will retain metrics for. prometheus.retention.alertmanager\u00b6 string \u2014 See note prometheus.retention.size\u00b6 string <code>4GiB</code> Configure the total size Prometheus will retain metrics for. prometheus.s3BucketAlerts\u00b6 object \u2014 Configure S3 bucket alerts. prometheus.s3BucketAlerts.buckets[]\u00b6 array of object \u2014 Definitions for specific S3 bucket alerts.S3 Bucket Alert configuration for specific bucket prometheus.s3BucketAlerts.exclude[]\u00b6 array of string \u2014 Exclude buckets from S3 alerts. prometheus.s3BucketAlerts.objects\u00b6 object \u2014 Alert when an S3 buckets reaches the set percentage of the set number of objects. prometheus.s3BucketAlerts.objects.count\u00b6 number <code>1638400</code> \u2014 prometheus.s3BucketAlerts.objects.enabled\u00b6 boolean \u2014 \u2014 prometheus.s3BucketAlerts.objects.percent\u00b6 number \u2014 Percentage, 0% - 100% prometheus.s3BucketAlerts.size\u00b6 object \u2014 Alert when an S3 bucket reaches the set percentage of the set size. prometheus.s3BucketAlerts.size.enabled\u00b6 boolean \u2014 \u2014 prometheus.s3BucketAlerts.size.percent\u00b6 number \u2014 Percentage, 0% - 100% prometheus.s3BucketAlerts.size.sizeQuotaGB\u00b6 number <code>1000</code> \u2014 prometheus.s3BucketAlerts.totalSize\u00b6 object \u2014 Alert when all S3 buckets reaches the set percentage of the set size. prometheus.s3BucketAlerts.totalSize.enabled\u00b6 boolean \u2014 \u2014 prometheus.s3BucketAlerts.totalSize.percent\u00b6 number \u2014 Percentage, 0% - 100% prometheus.s3BucketAlerts.totalSize.sizeQuotaGB\u00b6 number <code>1000</code> \u2014 prometheus.storage\u00b6 object \u2014 Configure the persistent volume claim used for Promtheus storage. prometheus.storage.enabled\u00b6 boolean \u2014 By default Prometheus instances run without storage and are treated as ephemeral.See ADR-0007 for context. prometheus.storage.size\u00b6 string <code>5Gi</code> \u2014 prometheus.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration prometheus.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. prometheus.webhookAlerts\u00b6 object \u2014 Configure webhook alerts. prometheus.webhookAlerts.enabled\u00b6 boolean <code>True</code> \u2014"},{"location":"operator-manual/schema/config/#note:prometheus.additionalScrapeConfigs[]","title":"Notes for <code>prometheus.additionalScrapeConfigs[]</code>","text":"<p>Configure additional scrape configs for Prometheus.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.alertmanagerSpec","title":"Notes for <code>prometheus.alertmanagerSpec</code>","text":"<p>Configure service cluster &amp; workload cluster Alertmanager.</p> <p>Alertmanager receives alerts from Prometheus and Thanos and forwards them to the configured notification channel.</p> <p>Note</p> <p>Alertmanager is installed in both service cluster and workload cluster, however this configuration key only applies to the service cluster, use <code>user.alertmanager</code> to configure it in the workload cluster.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.alertmanagerSpec.groupBy[]","title":"Notes for <code>prometheus.alertmanagerSpec.groupBy[]</code>","text":"<p>Configure Alertmanager to group certain alerts based on labels.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.alertmanagerSpec.resources","title":"Notes for <code>prometheus.alertmanagerSpec.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources","title":"Notes for <code>prometheus.alertmanagerSpec.storage.volumeClaimTemplate.spec.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.capacityManagementAlerts.nodeGroupRequestsExcludePattern","title":"Notes for <code>prometheus.capacityManagementAlerts.nodeGroupRequestsExcludePattern</code>","text":"<p>Configure a pattern of node groups to exclude from the resource request alerts. This can be used to exclude certain node groups from request alerts, while still getting usage alerts for those node groups.</p> <p>Examples:</p> <pre><code>.*redis.*|.*postgres.*\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.diskAlerts.inode.predictLinear[]","title":"Notes for <code>prometheus.diskAlerts.inode.predictLinear[]</code>","text":"<p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>The <code>hours</code> key is only supported when configured under <code>predictLinear</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.diskAlerts.inode.space[]","title":"Notes for <code>prometheus.diskAlerts.inode.space[]</code>","text":"<p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>The <code>hours</code> key is only supported when configured under <code>predictLinear</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.diskAlerts.storage.predictLinear[]","title":"Notes for <code>prometheus.diskAlerts.storage.predictLinear[]</code>","text":"<p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>The <code>hours</code> key is only supported when configured under <code>predictLinear</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.diskAlerts.storage.space[]","title":"Notes for <code>prometheus.diskAlerts.storage.space[]</code>","text":"<p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>Configure disk alerts when disk usage is predicted to reach the limit.</p> <p>The <code>hours</code> key is only supported when configured under <code>predictLinear</code>.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.nodeSelector","title":"Notes for <code>prometheus.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.resources","title":"Notes for <code>prometheus.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheus.retention.alertmanager","title":"Notes for <code>prometheus.retention.alertmanager</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#prometheusblackboxexporter","title":"<code>prometheusBlackboxExporter</code>","text":"<p>Configure Prometheus Blackbox Exporter, the exporter used for probing endpoints.</p> Key Type Default Description prometheusBlackboxExporter.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. prometheusBlackboxExporter.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. prometheusBlackboxExporter.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). prometheusBlackboxExporter.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). prometheusBlackboxExporter.customKubeapiTargets[]\u00b6 array of object \u2014 Configure custom Kube API targets Prometheus Blackbox Exporter should probe.Custom Kube API target Prometheus Blackbox Exporter should probe. prometheusBlackboxExporter.hostAliases[]\u00b6 array of object \u2014 Configure host aliases to resolve internally within the Pod.Configure a host alias to resolve internally within the Pod. prometheusBlackboxExporter.resources\u00b6 object \u2014 See note prometheusBlackboxExporter.resources.limits\u00b6 object \u2014 \u2014 prometheusBlackboxExporter.resources.requests\u00b6 object \u2014 \u2014 prometheusBlackboxExporter.targets\u00b6 object \u2014 Configure the targets Prometheus Blackbox Exporter should probe. prometheusBlackboxExporter.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:prometheusBlackboxExporter.resources","title":"Notes for <code>prometheusBlackboxExporter.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#prometheusnodeexporter","title":"<code>prometheusNodeExporter</code>","text":"<p>Configure Prometheus Node Exporter, the exporter used for collecting node metrics.</p> Key Type Default Description prometheusNodeExporter.resources\u00b6 object \u2014 See note prometheusNodeExporter.resources.limits\u00b6 object \u2014 \u2014 prometheusNodeExporter.resources.requests\u00b6 object \u2014 \u2014 prometheusNodeExporter.scrapeTimeout\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:prometheusNodeExporter.resources","title":"Notes for <code>prometheusNodeExporter.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#prometheusoperator","title":"<code>prometheusOperator</code>","text":"<p>Configure Prometheus Operator.</p> Key Type Default Description prometheusOperator.prometheusConfigReloader\u00b6 object \u2014 Configure Prometheus Operator config reloader. prometheusOperator.prometheusConfigReloader.resources\u00b6 object \u2014 See note prometheusOperator.prometheusConfigReloader.resources.limits\u00b6 object \u2014 \u2014 prometheusOperator.prometheusConfigReloader.resources.requests\u00b6 object \u2014 \u2014 prometheusOperator.resources\u00b6 object \u2014 See note prometheusOperator.resources.limits\u00b6 object \u2014 \u2014 prometheusOperator.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:prometheusOperator.prometheusConfigReloader.resources","title":"Notes for <code>prometheusOperator.prometheusConfigReloader.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:prometheusOperator.resources","title":"Notes for <code>prometheusOperator.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#rookceph","title":"<code>rookCeph</code>","text":"<p>Configure support for Rook Ceph.</p> <p>This is deprecated and should be configured via compliantkubernetes-kubespray if used.</p> Key Type Default Description rookCeph.gatekeeperPsp\u00b6 object \u2014 Configure Pod Security Policies for Rook Ceph. rookCeph.gatekeeperPsp.enabled\u00b6 boolean \u2014 \u2014 rookCeph.monitoring\u00b6 object \u2014 Configure Monitoring for Rook Ceph. rookCeph.monitoring.enabled\u00b6 boolean \u2014 \u2014"},{"location":"operator-manual/schema/config/#s3exporter","title":"<code>s3Exporter</code>","text":"<p>Configure S3 exporter, used to collect metrics about S3 usage.</p> Key Type Default Description s3Exporter.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. s3Exporter.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. s3Exporter.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). s3Exporter.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). s3Exporter.enabled\u00b6 boolean <code>True</code> \u2014 s3Exporter.interval\u00b6 string <code>60m</code> \u2014 s3Exporter.nodeSelector\u00b6 object \u2014 See note s3Exporter.resources\u00b6 object \u2014 See note s3Exporter.resources.limits\u00b6 object \u2014 \u2014 s3Exporter.resources.requests\u00b6 object \u2014 \u2014 s3Exporter.scrapeTimeout\u00b6 string \u2014 \u2014 s3Exporter.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration"},{"location":"operator-manual/schema/config/#note:s3Exporter.nodeSelector","title":"Notes for <code>s3Exporter.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:s3Exporter.resources","title":"Notes for <code>s3Exporter.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#storageclasses","title":"<code>storageClasses</code>","text":"<p>Configuration options for using block storage in Welkin</p> Key Type Default Description storageClasses.default\u00b6 string <code>default</code> The StorageClass to use for all persistent volumes in Welkin."},{"location":"operator-manual/schema/config/#tektonpipelines","title":"<code>tektonPipelines</code>","text":"<p>Configure Tekton Pipelines</p> Key Type Default Description tektonPipelines.controller\u00b6 object \u2014 Configure the Tekton Controller tektonPipelines.controller.replicas\u00b6 integer \u2014 \u2014 tektonPipelines.controller.resources\u00b6 object \u2014 See note tektonPipelines.controller.resources.limits\u00b6 object \u2014 \u2014 tektonPipelines.controller.resources.requests\u00b6 object \u2014 \u2014 tektonPipelines.customConfigDefaults\u00b6 object \u2014 See note tektonPipelines.enabled\u00b6 boolean \u2014 \u2014 tektonPipelines.remoteResolvers\u00b6 object \u2014 Configure the Tekton Remote Resolver tektonPipelines.remoteResolvers.replicas\u00b6 integer \u2014 \u2014 tektonPipelines.remoteResolvers.resources\u00b6 object \u2014 See note tektonPipelines.remoteResolvers.resources.limits\u00b6 object \u2014 \u2014 tektonPipelines.remoteResolvers.resources.requests\u00b6 object \u2014 \u2014 tektonPipelines.webhook\u00b6 object \u2014 Configure the Tekton Webhook tektonPipelines.webhook.replicas\u00b6 integer \u2014 \u2014 tektonPipelines.webhook.resources\u00b6 object \u2014 See note tektonPipelines.webhook.resources.limits\u00b6 object \u2014 \u2014 tektonPipelines.webhook.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:tektonPipelines.controller.resources","title":"Notes for <code>tektonPipelines.controller.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:tektonPipelines.customConfigDefaults","title":"Notes for <code>tektonPipelines.customConfigDefaults</code>","text":"<p>Configure custom default options for Tekton</p> <p>Note</p> <p>See the upstream documentation for available default config options.</p> <p>Examples:</p> <pre><code>{'default-timeout-minutes': '30'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:tektonPipelines.remoteResolvers.resources","title":"Notes for <code>tektonPipelines.remoteResolvers.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:tektonPipelines.webhook.resources","title":"Notes for <code>tektonPipelines.webhook.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#thanos","title":"<code>thanos</code>","text":"<p>Configuration for Thanos.</p> <p>Thanos ingests metrics sent from Prometheus in both the service and workload clusters, and stores them in object storage.</p> <p>This requires that <code>objectStorage</code> is configured, and will use the bucket or container set in <code>objectStorage.buckets.thanos</code>.</p> <p>Note</p> <p>Thanos and its components are installed in the service cluster, so this configuration mainly applies there.</p> Key Type Default Description thanos.bucketweb\u00b6 object \u2014 Configure Thanos Bucket Web, the UI to view the state of the bucket or container in use by Thanos. thanos.bucketweb.resources\u00b6 object \u2014 See note thanos.bucketweb.resources.limits\u00b6 object \u2014 \u2014 thanos.bucketweb.resources.requests\u00b6 object \u2014 \u2014 thanos.compactor\u00b6 object \u2014 See note thanos.compactor.deduplication\u00b6 string <code>none</code> See note thanos.compactor.persistence\u00b6 object \u2014 Configure persistence for Thanos Compactor. thanos.compactor.persistence.enabled\u00b6 boolean <code>True</code> \u2014 thanos.compactor.persistence.size\u00b6 string <code>8Gi</code> \u2014 thanos.compactor.resources\u00b6 object \u2014 See note thanos.compactor.resources.limits\u00b6 object \u2014 \u2014 thanos.compactor.resources.requests\u00b6 object \u2014 \u2014 thanos.compactor.retentionResolution1h\u00b6 string \u2014 See note thanos.compactor.retentionResolution5m\u00b6 string \u2014 See note thanos.compactor.retentionResolutionRaw\u00b6 string \u2014 See note thanos.compactor.verticalCompaction\u00b6 boolean \u2014 When enabled series of metrics from multiple replicas will be merged into one. thanos.enabled\u00b6 boolean <code>True</code> &gt; [!note]&gt; Must be set for both service and workload cluster. thanos.metrics\u00b6 object \u2014 Configure metrics collected from Thanos. thanos.metrics.enabled\u00b6 boolean <code>True</code> \u2014 thanos.metrics.serviceMonitor\u00b6 object \u2014 Configure the service monitor used to collect metrics from Thanos. thanos.metrics.serviceMonitor.enabled\u00b6 boolean <code>True</code> \u2014 thanos.objectStorage\u00b6 object \u2014 Configure Object Storage for Thanos.Allows for using OpenStack Swift as the object storage backend type. Also allows use separate configuration of s3 specific to Thanos. thanos.objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. thanos.objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. thanos.objectStorage.s3.region\u00b6 string \u2014 Region to store data. thanos.objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). thanos.objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. thanos.objectStorage.type\u00b6 string \u2014 See note thanos.query\u00b6 object \u2014 Configure Thanos Query, the component executing metric queries. thanos.query.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. thanos.query.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. thanos.query.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). thanos.query.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). thanos.query.enabled\u00b6 boolean <code>True</code> \u2014 thanos.query.replicaCount\u00b6 number <code>1</code> \u2014 thanos.query.resources\u00b6 object \u2014 See note thanos.query.resources.limits\u00b6 object \u2014 \u2014 thanos.query.resources.requests\u00b6 object \u2014 \u2014 thanos.query.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. thanos.queryFrontend\u00b6 object \u2014 Configure Thanos Query Frontend, the component serving query requests from Grafana. thanos.queryFrontend.resources\u00b6 object \u2014 See note thanos.queryFrontend.resources.limits\u00b6 object \u2014 \u2014 thanos.queryFrontend.resources.requests\u00b6 object \u2014 \u2014 thanos.receiveDistributor\u00b6 object \u2014 Configure Thanos Receive Distributor, the component serving remote write requests from Prometheus.Also called routing receiver upstream. thanos.receiveDistributor.extraFlags[]\u00b6 array \u2014 See note thanos.receiveDistributor.receiveHashringsAlgorithm\u00b6 string <code>ketama</code> See note thanos.receiveDistributor.receiveMaxConcurrency\u00b6 integer <code>5</code> Maximum number of concurrent write requests allowed by Thanos receiveDistributor. thanos.receiveDistributor.replicaCount\u00b6 integer <code>3</code> \u2014 thanos.receiveDistributor.replicationFactor\u00b6 number <code>1</code> Requires that incoming remote write requests are replicated <code>(replicationFactor + 1) / 2</code>. thanos.receiveDistributor.resources\u00b6 object \u2014 See note thanos.receiveDistributor.resources.limits\u00b6 object \u2014 \u2014 thanos.receiveDistributor.resources.requests\u00b6 object \u2014 \u2014 thanos.receiver\u00b6 object \u2014 Configure Thanos Receiver, the component ingesting metrics collected by Prometheus and storing them in object storage.Also called ingesting receiver upstream. thanos.receiver.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. thanos.receiver.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. thanos.receiver.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). thanos.receiver.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). thanos.receiver.basic_auth\u00b6 object \u2014 Configure authentication to Thanos Receiver, thanos.receiver.basic_auth.username\u00b6 string <code>thanos</code> See note thanos.receiver.enabled\u00b6 boolean <code>True</code> \u2014 thanos.receiver.mode\u00b6 string <code>dual-mode</code> See note thanos.receiver.outOfOrderTimeWindow\u00b6 string <code>600s</code> \u2014 thanos.receiver.persistence\u00b6 object \u2014 Configure persistence for Thanos Receiver. thanos.receiver.persistence.enabled\u00b6 boolean <code>True</code> \u2014 thanos.receiver.persistence.size\u00b6 string <code>50Gi</code> \u2014 thanos.receiver.replicaCount\u00b6 number <code>2</code> \u2014 thanos.receiver.resources\u00b6 object \u2014 See note thanos.receiver.resources.limits\u00b6 object \u2014 \u2014 thanos.receiver.resources.requests\u00b6 object \u2014 \u2014 thanos.receiver.subdomain\u00b6 string <code>thanos-receiver</code> See note thanos.receiver.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. thanos.receiver.tsdbRetention\u00b6 string <code>15d</code> \u2014 thanos.ruler\u00b6 object \u2014 Configure Thanos Ruler, the component evaluating alerting and recording rules. thanos.ruler.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. thanos.ruler.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. thanos.ruler.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). thanos.ruler.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). thanos.ruler.configReloader\u00b6 object \u2014 Configure the config reloader sidecar for Thanos Ruler. thanos.ruler.configReloader.resources\u00b6 object \u2014 See note thanos.ruler.configReloader.resources.limits\u00b6 object \u2014 \u2014 thanos.ruler.configReloader.resources.requests\u00b6 object \u2014 \u2014 thanos.ruler.enabled\u00b6 boolean <code>True</code> \u2014 thanos.ruler.persistence\u00b6 object \u2014 Configure persistence for Thanos Ruler. thanos.ruler.persistence.enabled\u00b6 boolean \u2014 \u2014 thanos.ruler.persistence.size\u00b6 string <code>8Gi</code> \u2014 thanos.ruler.replicaCount\u00b6 number <code>2</code> \u2014 thanos.ruler.resources\u00b6 object \u2014 See note thanos.ruler.resources.limits\u00b6 object \u2014 \u2014 thanos.ruler.resources.requests\u00b6 object \u2014 \u2014 thanos.ruler.topologySpreadConstraints[]\u00b6 array \u2014 TopologySpreadConstraints describes how pods should spread across topology domains. thanos.storegateway\u00b6 object \u2014 Configure Thanos Store Gateway, the component fetching metrics from object storage. thanos.storegateway.persistence\u00b6 object \u2014 Configure persistence for Thanos Store Gateway. thanos.storegateway.persistence.size\u00b6 string <code>8Gi</code> \u2014 thanos.storegateway.resources\u00b6 object \u2014 See note thanos.storegateway.resources.limits\u00b6 object \u2014 \u2014 thanos.storegateway.resources.requests\u00b6 object \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:thanos.bucketweb.resources","title":"Notes for <code>thanos.bucketweb.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor","title":"Notes for <code>thanos.compactor</code>","text":"<p>Configure Thanos Compactor, the component compacting and deduplicating metrics stored by Thanos.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor.deduplication","title":"Notes for <code>thanos.compactor.deduplication</code>","text":"<p>Configure deduplication of metrics.</p> <p>Possible values:</p> <pre><code>none\n</code></pre> <pre><code>receiverReplicas\n</code></pre> <pre><code>prometheusReplicas\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor.resources","title":"Notes for <code>thanos.compactor.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor.retentionResolution1h","title":"Notes for <code>thanos.compactor.retentionResolution1h</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor.retentionResolution5m","title":"Notes for <code>thanos.compactor.retentionResolution5m</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.compactor.retentionResolutionRaw","title":"Notes for <code>thanos.compactor.retentionResolutionRaw</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.objectStorage.type","title":"Notes for <code>thanos.objectStorage.type</code>","text":"<p>Possible values:</p> <pre><code>swift\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.query.resources","title":"Notes for <code>thanos.query.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.queryFrontend.resources","title":"Notes for <code>thanos.queryFrontend.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiveDistributor.extraFlags[]","title":"Notes for <code>thanos.receiveDistributor.extraFlags[]</code>","text":"<p>When set, the arguments will be passed onto the component as command-line flags. Refer to the upstream doc for more details.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiveDistributor.receiveHashringsAlgorithm","title":"Notes for <code>thanos.receiveDistributor.receiveHashringsAlgorithm</code>","text":"<p>Algorithm used for distributing writes across Thanos receive replicas.</p> <p>Possible values:</p> <pre><code>hashmod\n</code></pre> <pre><code>ketama\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiveDistributor.resources","title":"Notes for <code>thanos.receiveDistributor.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiver.basic_auth.username","title":"Notes for <code>thanos.receiver.basic_auth.username</code>","text":"<p>Configure the username for authenticating to Thanos Receiver.</p> <p>Note</p> <p>Must be set for both service and workload clusters.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiver.mode","title":"Notes for <code>thanos.receiver.mode</code>","text":"<p>Possible values:</p> <pre><code>standalone\n</code></pre> <pre><code>dual-mode\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiver.resources","title":"Notes for <code>thanos.receiver.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.receiver.subdomain","title":"Notes for <code>thanos.receiver.subdomain</code>","text":"<p>Subdomain of <code>opsDomain</code> that the Ingress to Thanos Receive will be created with.</p> <p>Note</p> <p>Must be set for both service and workload clusters.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.ruler.configReloader.resources","title":"Notes for <code>thanos.ruler.configReloader.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.ruler.resources","title":"Notes for <code>thanos.ruler.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:thanos.storegateway.resources","title":"Notes for <code>thanos.storegateway.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#trivy","title":"<code>trivy</code>","text":"<p>Configure Trivy Operator.</p> <p>Trivy automatically scans the cluster for vulnerabilities, misconfigurations, and exposed secrets.</p> Key Type Default Description trivy.affinity\u00b6 object \u2014 Affinity is a group of affinity scheduling rules. trivy.affinity.nodeAffinity\u00b6 \u2014 \u2014 Describes node affinity scheduling rules for the pod. trivy.affinity.podAffinity\u00b6 \u2014 \u2014 Describes pod affinity scheduling rules (e.g. co-locate this pod in the same node, zone, etc. as some other pod(s)). trivy.affinity.podAntiAffinity\u00b6 \u2014 \u2014 Describes pod anti-affinity scheduling rules (e.g. avoid putting this pod in the same node, zone, etc. as some other pod(s)). trivy.enabled\u00b6 boolean <code>True</code> \u2014 trivy.excludeNamespaces\u00b6 string \u2014 Configure a comma separated list of namespaces (or glob patterns) to be excluded from Trivy scanners. trivy.nodeCollector\u00b6 object \u2014 Configure the node collector created by Trivy. trivy.nodeCollector.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration trivy.resources\u00b6 object \u2014 See note trivy.resources.limits\u00b6 object \u2014 \u2014 trivy.resources.requests\u00b6 object \u2014 \u2014 trivy.scanJobs\u00b6 object \u2014 Configure the scan jobs created by Trivy. trivy.scanJobs.concurrentLimit\u00b6 number <code>1</code> \u2014 trivy.scanJobs.retryDelay\u00b6 string <code>1m</code> \u2014 trivy.scanJobs.timeout\u00b6 string <code>5m</code> \u2014 trivy.scanner\u00b6 object \u2014 See note trivy.scanner.dbRegistry\u00b6 string \u2014 \u2014 trivy.scanner.dbRepository\u00b6 string \u2014 \u2014 trivy.scanner.dbRepositoryInsecure\u00b6 boolean \u2014 \u2014 trivy.scanner.imagePullSecret\u00b6 object \u2014 Configure an image pull secret for Trivy to use.Create the secret in the <code>monitoring</code> namespace then configure the name here. trivy.scanner.imagePullSecret.name\u00b6 string \u2014 \u2014 trivy.scanner.javaDbRegistry\u00b6 string \u2014 \u2014 trivy.scanner.javaDbRepository\u00b6 string \u2014 \u2014 trivy.scanner.offlineScanEnabled\u00b6 boolean \u2014 \u2014 trivy.scanner.registry\u00b6 object \u2014 Configure registries for Trivy. trivy.scanner.registry.mirror\u00b6 object \u2014 See note trivy.scanner.resources\u00b6 object \u2014 See note trivy.scanner.resources.limits\u00b6 object \u2014 \u2014 trivy.scanner.resources.requests\u00b6 object \u2014 \u2014 trivy.scanner.timeout\u00b6 string \u2014 See note trivy.serviceMonitor\u00b6 object \u2014 Configure the service monitor collecting metrics from Trivy. trivy.serviceMonitor.enabled\u00b6 boolean <code>True</code> \u2014 trivy.serviceMonitor.interval\u00b6 string \u2014 See note trivy.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration trivy.vulnerabilityScanner\u00b6 object \u2014 Configure the vulnerability scanner for Trivy. trivy.vulnerabilityScanner.scanOnlyCurrentRevisions\u00b6 boolean <code>True</code> \u2014 trivy.vulnerabilityScanner.scannerReportTTL\u00b6 string \u2014 See note"},{"location":"operator-manual/schema/config/#note:trivy.resources","title":"Notes for <code>trivy.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.scanner","title":"Notes for <code>trivy.scanner</code>","text":"<p>Configure the scanner used by Trivy.</p> <p>Note</p> <p>Many of these must be configured to support an air-gapped environment. See the admin documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.scanner.registry.mirror","title":"Notes for <code>trivy.scanner.registry.mirror</code>","text":"<p>Configure registry mirrors for Trivy.</p> <p>The key represents the original registry and the value the mirror registry.</p> <p>Examples:</p> <pre><code>{'docker.io': 'registry.example.com:5000', 'gcr.io': 'registry.example.com:5000', 'ghcr.io': 'registry.example.com:5000', 'index.docker.io': 'registry.example.com:5000', 'quay.io': 'registry.example.com:5000', 'registry.k8s.io': 'registry.example.com:5000'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.scanner.resources","title":"Notes for <code>trivy.scanner.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.scanner.timeout","title":"Notes for <code>trivy.scanner.timeout</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.serviceMonitor.interval","title":"Notes for <code>trivy.serviceMonitor.interval</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:trivy.vulnerabilityScanner.scannerReportTTL","title":"Notes for <code>trivy.vulnerabilityScanner.scannerReportTTL</code>","text":"<p>An amount of time</p> <p>Examples:</p> <pre><code>300s\n</code></pre> <pre><code>72h\n</code></pre> <pre><code>3d\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#user","title":"<code>user</code>","text":"<p>Configuration for Application Developers (users), that use the workload cluster</p> Key Type Default Description user.adminGroups[]\u00b6 array of string \u2014 List of groups that Application Developers are apart of that should have access to the cluster. user.adminUsers[]\u00b6 array of string \u2014 List of Application Developers that should have access to the cluster. user.constraints\u00b6 object \u2014 See note user.createNamespaces\u00b6 boolean \u2014 This only controls if the namespaces should be created, user RBAC is always created. user.extraClusterRoleBindings\u00b6 object \u2014 Configure extra ClusterRoleBindings for Application Developers user.extraClusterRoles\u00b6 object \u2014 Configure extra ClusterRoles that are not originally part of WelkinThese are intended to be used for Application Developers user.extraRoleBindings\u00b6 object \u2014 Configure extra RoleBindings for Application DevelopersThe RoleBindings are added to all Application Developer namespaces configured in user.namespaces user.extraRoles\u00b6 object \u2014 Configure extra Roles for Application DevelopersThe Roles are added to all Application Developer namespaces configured in user.namespaces user.fluxv2\u00b6 object \u2014 Installs required cluster resources needed to install fluxv2.Requires that <code>gatekeeper.allowUserCRDs.enabled</code> is enabled. user.fluxv2.enabled\u00b6 boolean \u2014 \u2014 user.jaeger\u00b6 object \u2014 Installs required cluster resources needed to install jaeger.Requires that <code>gatekeeper.allowUserCRDs.enabled</code> is enabled. user.jaeger.enabled\u00b6 boolean \u2014 \u2014 user.kafka\u00b6 object \u2014 Installs required cluster resources needed to install kafka-operator.Requires that <code>gatekeeper.allowUserCRDs.enabled</code> is enabled. user.kafka.enabled\u00b6 boolean \u2014 \u2014 user.mongodb\u00b6 object \u2014 Installs required cluster resources needed to install MongoDB.Requires that <code>gatekeeper.allowUserCRDs.enabled</code> is enabled. user.mongodb.enabled\u00b6 boolean \u2014 \u2014 user.namespaces[]\u00b6 array of string \u2014 See note user.sealedSecrets\u00b6 object \u2014 Installs required cluster resources needed to install sealedSecrets.Requires that <code>gatekeeper.allowUserCRDs.enabled</code> is enabled. user.sealedSecrets.enabled\u00b6 boolean \u2014 \u2014 user.serviceAccounts[]\u00b6 array of string \u2014 See note"},{"location":"operator-manual/schema/config/#note:user.constraints","title":"Notes for <code>user.constraints</code>","text":"<p>Any namespace listed in constraints are exempted from HNC managed namespaces.</p> <p>This to override the Pod Security Admission level.</p> <p>Example of constraint can be found here: <code>Example Constraint</code></p> <p>The only extra label `psaLevel: `` is shown in the following example: <pre><code>&lt;namespace&gt;:\n  psaLevel: &lt;baseline/privileged&gt;\n  &lt;service-name&gt;:\n    ...\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:user.namespaces[]","title":"Notes for <code>user.namespaces[]</code>","text":"<p>List of namespaces that should be created for Application Developer.</p> <p>It is common to create one namespace for the Application Developer and then create namespaces via HNC.</p> <p>Requires that <code>user.createNamespaces</code> is enabled.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:user.serviceAccounts[]","title":"Notes for <code>user.serviceAccounts[]</code>","text":"<p>List of serviceAccounts to create RBAC rules for, used for dev situations.</p> <p>Application developer kube-config for contributors</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#velero","title":"<code>velero</code>","text":"<p>Configure Velero, the backup and snapshot tool for Kubernetes resources and volumes.</p> <p>This requires that <code>objectStorage</code> is configured, and will use the bucket or container set in <code>objectStorage.buckets.velero</code>.</p> Key Type Default Description velero.enabled\u00b6 boolean <code>True</code> \u2014 velero.excludedExtraNamespaces[]\u00b6 array of string \u2014 Configure dynamic namespaces to exclude from backups, prefer this for overrides over <code>excludedNamespaces</code>. velero.excludedNamespaces[]\u00b6 array of string \u2014 Configure system namespaces to exclude from backups. velero.nodeAgent\u00b6 object \u2014 Configure the node agent of Velero, used to take snapshots of volumes. velero.nodeAgent.resources\u00b6 object \u2014 See note velero.nodeAgent.resources.limits\u00b6 object \u2014 \u2014 velero.nodeAgent.resources.requests\u00b6 object \u2014 \u2014 velero.nodeAgent.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration velero.nodeSelector\u00b6 object \u2014 See note velero.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to Velero. velero.objectStorage.s3\u00b6 object \u2014 Configurations for using S3 storage. velero.objectStorage.s3.forcePathStyle\u00b6 boolean \u2014 Force the use of path style access instead of virtual host style access.Generally <code>false</code> when using AWS, Exoscale, and UpCloud and <code>true</code> for other providers. velero.objectStorage.s3.region\u00b6 string \u2014 Region to store data. velero.objectStorage.s3.regionEndpoint\u00b6 string \u2014 Endpoint to reach the S3 service, mainly applicable for non-AWS implementations.Make sure to prepend the protocol (e.g. <code>https://</code>). velero.objectStorage.s3.v2Auth\u00b6 boolean \u2014 Force the use of v2 authentication, will default to using v4 authentication otherwise. velero.resources\u00b6 object \u2014 See note velero.resources.limits\u00b6 object \u2014 \u2014 velero.resources.requests\u00b6 object \u2014 \u2014 velero.restoreResourcePriorities[]\u00b6 array of string \u2014 See note velero.retentionPeriod\u00b6 string \u2014 See note velero.schedule\u00b6 string \u2014 \u2014 velero.storagePrefix\u00b6 string \u2014 See note velero.tolerations[]\u00b6 array \u2014 Kubernetes TolerationsKubernetes taint and toleration velero.uploaderType\u00b6 string \u2014 See note velero.useVolumeSnapshots\u00b6 boolean \u2014 \u2014"},{"location":"operator-manual/schema/config/#note:velero.nodeAgent.resources","title":"Notes for <code>velero.nodeAgent.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.nodeSelector","title":"Notes for <code>velero.nodeSelector</code>","text":"<p>Kubernetes node selector</p> <p>Kubernetes assign pod node</p> <p>Examples:</p> <pre><code>{'kubernetes.io/os': 'linux'}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.resources","title":"Notes for <code>velero.resources</code>","text":"<p>Resource requests are used by the kube-scheduler to pick a node to schedule pods on.</p> <p>Limits are enforced. Resources are commonly 'cpu' and 'memory'.</p> <p>Examples:</p> <pre><code>{'requests': {'memory': '128Mi', 'cpu': '100m'}, 'limits': {'memory': '256Mi', 'cpu': '250m'}}\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.restoreResourcePriorities[]","title":"Notes for <code>velero.restoreResourcePriorities[]</code>","text":"<p>Configure restore order for resources</p> <p>Note</p> <p>See upstream documentation for reference</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.retentionPeriod","title":"Notes for <code>velero.retentionPeriod</code>","text":"<p>A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"-1.5h\" or \"2h45m\".</p> <p>Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".</p> <p>Examples:</p> <pre><code>2h45m0s\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.storagePrefix","title":"Notes for <code>velero.storagePrefix</code>","text":"<p>Configure unique storage prefix for this cluster when storing backups and snapshots in object storage.</p> <p>When multiple workload clusters share the same bucket or container ensure that they use separate storage prefixes.</p> <p>Examples:</p> <pre><code>service-cluster\n</code></pre> <pre><code>workload-cluster\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#note:velero.uploaderType","title":"Notes for <code>velero.uploaderType</code>","text":"<p>Possible values:</p> <pre><code>kopia\n</code></pre> <pre><code>restic\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/config/#wcprobeingress","title":"<code>wcProbeIngress</code>","text":"<p>Configure a probe for the workload cluster Ingress Controller.</p> Key Type Default Description wcProbeIngress.enabled\u00b6 boolean \u2014 \u2014"},{"location":"operator-manual/schema/config/#welcomingdashboard","title":"<code>welcomingDashboard</code>","text":"<p>If you want to add extra text to the grafana/opensearch \"welcoming dashboards\" then write the text in these values as a one-line string. Note, first line of the string is a header, not all characters are supported. For newline in Grafana dashboard use format <code>\\\\n</code></p> Key Type Default Description welcomingDashboard.extraTextGrafana\u00b6 string \u2014 See note welcomingDashboard.extraTextOpensearch\u00b6 string \u2014 Extra text added to the Opensearch welcoming dashboard. welcomingDashboard.extraVersions[]\u00b6 array of object \u2014 List of additional components to list on the welcoming dashboard.Additional component to list on the welcoming dashboard."},{"location":"operator-manual/schema/config/#note:welcomingDashboard.extraTextGrafana","title":"Notes for <code>welcomingDashboard.extraTextGrafana</code>","text":"<p>Extra text added to the Grafana welcoming dashboard.</p> <p>Examples:</p> <pre><code>Hello\\n\\n[This is an example link](https:/elastisys.io)\n</code></pre> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/","title":"Secrets","text":"<p>This table was generated from secrets.yaml.</p> <p>Cells marked with \"\u2014\" mean \"not specified in schema\".</p>"},{"location":"operator-manual/schema/secrets/#alerts","title":"<code>alerts</code>","text":"<p>Configure secrets for alerting.</p> Key Type Default Description alerts.opsGenie\u00b6 object \u2014 Configure secrets for alerting with OpsGenie. alerts.opsGenie.apiKey\u00b6 \u2014 \u2014 \u2014 alerts.slack\u00b6 object \u2014 Configure secrets for alerting with Slack. alerts.slack.apiUrl\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#dex","title":"<code>dex</code>","text":"<p>Configure secrets for Dex.</p> Key Type Default Description dex.additionalStaticClients[]\u00b6 array of object \u2014 See note dex.connectors[]\u00b6 array of object \u2014 See note dex.extraStaticLogins[]\u00b6 array of object \u2014 Configure additional static logins for Dex.Additional static logins for Dex. dex.kubeloginClientSecret\u00b6 string \u2014 \u2014 dex.staticPassword\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#note:dex.additionalStaticClients[]","title":"Notes for <code>dex.additionalStaticClients[]</code>","text":"<p>Configure additional static clients in Dex.</p> <p>Clients in this case is application that wants to allow users to authenticate via Dex.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>Configure an additional static client in Dex.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#note:dex.connectors[]","title":"Notes for <code>dex.connectors[]</code>","text":"<p>Configure upstream Identity Providers.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>Configure an upstream Identity Provider.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#externaldns","title":"<code>externalDns</code>","text":"<p>Configure secrets for External DNS.</p> Key Type Default Description externalDns.awsRoute53\u00b6 object \u2014 Configure AWS Route 53 secrets for External DNS. externalDns.awsRoute53.accessKey\u00b6 string \u2014 \u2014 externalDns.awsRoute53.secretKey\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#fluentd","title":"<code>fluentd</code>","text":"<p>Secret configuration options for Fluentd.</p> Key Type Default Description fluentd.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to Fluentd. fluentd.objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. fluentd.objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. fluentd.objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with."},{"location":"operator-manual/schema/secrets/#grafana","title":"<code>grafana</code>","text":"<p>Configure secrets for Grafana.</p> Key Type Default Description grafana.clientSecret\u00b6 string \u2014 \u2014 grafana.ops\u00b6 object \u2014 Configure secrets for Admin Grafana. grafana.ops.envRenderSecret[]\u00b6 array \u2014 \u2014 grafana.opsClientSecret\u00b6 string \u2014 \u2014 grafana.password\u00b6 string \u2014 \u2014 grafana.user\u00b6 object \u2014 Configure secrets for Dev Grafana. grafana.user.envRenderSecret[]\u00b6 array \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#harbor","title":"<code>harbor</code>","text":"<p>Secret configuration options for Harbor.</p> Key Type Default Description harbor.clientSecret\u00b6 string \u2014 \u2014 harbor.coreSecret\u00b6 string \u2014 \u2014 harbor.external\u00b6 object \u2014 External database password config. harbor.external.databasePassword\u00b6 string \u2014 \u2014 harbor.internal\u00b6 object \u2014 Internal database password config. harbor.internal.databasePassword\u00b6 string \u2014 \u2014 harbor.jobserviceSecret\u00b6 string \u2014 \u2014 harbor.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to harbor. harbor.objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. harbor.objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. harbor.objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. harbor.password\u00b6 string \u2014 \u2014 harbor.registrySecret\u00b6 string \u2014 \u2014 harbor.xsrf\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#issuers","title":"<code>issuers</code>","text":"<p>Configure secrets for issuers.</p> Key Type Default Description issuers.secrets\u00b6 object \u2014 See note"},{"location":"operator-manual/schema/secrets/#note:issuers.secrets","title":"Notes for <code>issuers.secrets</code>","text":"<p>Configure secrets for issuers.</p> <p>This must match the configuration set on the issuers.</p> <p>Keys become the name of the secret, and the value the data of the secret.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#kubeapimetricspassword","title":"<code>kubeapiMetricsPassword</code>","text":"<p>None</p> Key Type Default Description"},{"location":"operator-manual/schema/secrets/#kured","title":"<code>kured</code>","text":"<p>Notification secrets for Kured (Kubernetes Reboot Daemon).</p> Key Type Default Description kured.slack\u00b6 object \u2014 Notification secrets to send notifications from Kured to Slack. kured.slack.botToken\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#objectstorage","title":"<code>objectStorage</code>","text":"<p>Configuration options for using object storage in Welkin.</p> Key Type Default Description objectStorage.azure\u00b6 object \u2014 Secrets for using Azure as object storage in Welkin. objectStorage.azure.storageAccountKey\u00b6 string \u2014 Storage account key to authenticate with. objectStorage.azure.storageAccountName\u00b6 string \u2014 Storage account name to authenticate with. objectStorage.restore\u00b6 object \u2014 Secrets for restoring object storage from a secondary site to the primary site with Rclone. objectStorage.restore.decrypt\u00b6 object \u2014 Secrets for encrypt data when syncing. objectStorage.restore.decrypt.password\u00b6 string \u2014 Crypt password, generate with <code>pwgen 32 1</code>. objectStorage.restore.decrypt.passwordObscured\u00b6 string \u2014 Obscured crypt password, generate with <code>rclone obscure &lt;password&gt;</code>. objectStorage.restore.decrypt.salt\u00b6 string \u2014 Crypt salt, generate with <code>pwgen 32 1</code>. objectStorage.restore.decrypt.saltObscured\u00b6 string \u2014 Obscured crypt salt, generate with <code>rclone obscure &lt;salt&gt;</code>. objectStorage.restore.destinations\u00b6 object \u2014 Allows for complete or partial overrides of the destinations of the restore, the main object storage configuration. objectStorage.restore.destinations.azure\u00b6 object \u2014 Secrets for using Azure as object storage in Welkin. objectStorage.restore.destinations.azure.storageAccountKey\u00b6 string \u2014 Storage account key to authenticate with. objectStorage.restore.destinations.azure.storageAccountName\u00b6 string \u2014 Storage account name to authenticate with. objectStorage.restore.destinations.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. objectStorage.restore.destinations.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. objectStorage.restore.destinations.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. objectStorage.restore.destinations.swift\u00b6 object \u2014 See note objectStorage.restore.destinations.swift.applicationCredentialID\u00b6 string \u2014 Application Credential ID to authenticate with. objectStorage.restore.destinations.swift.applicationCredentialName\u00b6 string \u2014 Application Credential Name to authenticate with, requires username to be set. objectStorage.restore.destinations.swift.applicationCredentialSecret\u00b6 string \u2014 Application Credential Secret to authenticate with, requires username to be set. objectStorage.restore.destinations.swift.password\u00b6 string \u2014 \u2014 objectStorage.restore.destinations.swift.username\u00b6 string \u2014 \u2014 objectStorage.restore.sources\u00b6 object \u2014 Allows for complete or partial overrides of the sources of the restore, the sync object storage configuration. objectStorage.restore.sources.azure\u00b6 object \u2014 Secrets for using Azure as object storage in Welkin. objectStorage.restore.sources.azure.storageAccountKey\u00b6 string \u2014 Storage account key to authenticate with. objectStorage.restore.sources.azure.storageAccountName\u00b6 string \u2014 Storage account name to authenticate with. objectStorage.restore.sources.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. objectStorage.restore.sources.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. objectStorage.restore.sources.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. objectStorage.restore.sources.swift\u00b6 object \u2014 See note objectStorage.restore.sources.swift.applicationCredentialID\u00b6 string \u2014 Application Credential ID to authenticate with. objectStorage.restore.sources.swift.applicationCredentialName\u00b6 string \u2014 Application Credential Name to authenticate with, requires username to be set. objectStorage.restore.sources.swift.applicationCredentialSecret\u00b6 string \u2014 Application Credential Secret to authenticate with, requires username to be set. objectStorage.restore.sources.swift.password\u00b6 string \u2014 \u2014 objectStorage.restore.sources.swift.username\u00b6 string \u2014 \u2014 objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. objectStorage.swift\u00b6 object \u2014 See note objectStorage.swift.applicationCredentialID\u00b6 string \u2014 Application Credential ID to authenticate with. objectStorage.swift.applicationCredentialName\u00b6 string \u2014 Application Credential Name to authenticate with, requires username to be set. objectStorage.swift.applicationCredentialSecret\u00b6 string \u2014 Application Credential Secret to authenticate with, requires username to be set. objectStorage.swift.password\u00b6 string \u2014 \u2014 objectStorage.swift.username\u00b6 string \u2014 \u2014 objectStorage.sync\u00b6 object \u2014 Secrets for syncing object storage from the primary site to a secondary site with Rclone. objectStorage.sync.azure\u00b6 object \u2014 Secrets for using Azure as object storage in Welkin. objectStorage.sync.azure.storageAccountKey\u00b6 string \u2014 Storage account key to authenticate with. objectStorage.sync.azure.storageAccountName\u00b6 string \u2014 Storage account name to authenticate with. objectStorage.sync.encrypt\u00b6 object \u2014 Secrets for encrypt data when syncing. objectStorage.sync.encrypt.password\u00b6 string \u2014 Crypt password, generate with <code>pwgen 32 1</code>. objectStorage.sync.encrypt.passwordObscured\u00b6 string \u2014 Obscured crypt password, generate with <code>rclone obscure &lt;password&gt;</code>. objectStorage.sync.encrypt.salt\u00b6 string \u2014 Crypt salt, generate with <code>pwgen 32 1</code>. objectStorage.sync.encrypt.saltObscured\u00b6 string \u2014 Obscured crypt salt, generate with <code>rclone obscure &lt;salt&gt;</code>. objectStorage.sync.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. objectStorage.sync.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. objectStorage.sync.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. objectStorage.sync.swift\u00b6 object \u2014 See note objectStorage.sync.swift.applicationCredentialID\u00b6 string \u2014 Application Credential ID to authenticate with. objectStorage.sync.swift.applicationCredentialName\u00b6 string \u2014 Application Credential Name to authenticate with, requires username to be set. objectStorage.sync.swift.applicationCredentialSecret\u00b6 string \u2014 Application Credential Secret to authenticate with, requires username to be set. objectStorage.sync.swift.password\u00b6 string \u2014 \u2014 objectStorage.sync.swift.username\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#note:objectStorage.restore.destinations.swift","title":"Notes for <code>objectStorage.restore.destinations.swift</code>","text":"<p>Secrets for using Swift as object storage in Welkin.</p> <p>Important</p> <p>Currently Harbor only supports <code>username</code> and <code>password</code> authentication.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#note:objectStorage.restore.sources.swift","title":"Notes for <code>objectStorage.restore.sources.swift</code>","text":"<p>Secrets for using Swift as object storage in Welkin.</p> <p>Important</p> <p>Currently Harbor only supports <code>username</code> and <code>password</code> authentication.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#note:objectStorage.swift","title":"Notes for <code>objectStorage.swift</code>","text":"<p>Secrets for using Swift as object storage in Welkin.</p> <p>Important</p> <p>Currently Harbor only supports <code>username</code> and <code>password</code> authentication.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#note:objectStorage.sync.swift","title":"Notes for <code>objectStorage.sync.swift</code>","text":"<p>Secrets for using Swift as object storage in Welkin.</p> <p>Important</p> <p>Currently Harbor only supports <code>username</code> and <code>password</code> authentication.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#opensearch","title":"<code>opensearch</code>","text":"<p>Secrets for OpenSearch.</p> Key Type Default Description opensearch.adminHash\u00b6 string \u2014 \u2014 opensearch.adminPassword\u00b6 string \u2014 \u2014 opensearch.clientSecret\u00b6 string \u2014 \u2014 opensearch.configurerHash\u00b6 string \u2014 \u2014 opensearch.configurerPassword\u00b6 string \u2014 \u2014 opensearch.curatorPassword\u00b6 string \u2014 \u2014 opensearch.dashboardsCookieEncKey\u00b6 string \u2014 \u2014 opensearch.dashboardsHash\u00b6 string \u2014 \u2014 opensearch.dashboardsPassword\u00b6 string \u2014 \u2014 opensearch.extraUsers[]\u00b6 array of object \u2014 See note opensearch.fluentdPassword\u00b6 string \u2014 \u2014 opensearch.metricsExporterPassword\u00b6 string \u2014 \u2014 opensearch.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to opensearch. opensearch.objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. opensearch.objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. opensearch.objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. opensearch.snapshotterPassword\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#note:opensearch.extraUsers[]","title":"Notes for <code>opensearch.extraUsers[]</code>","text":"<p>Configures extra users for OpenSearch Security.</p> <p>Configures extra user for OpenSearch Security.</p> <p>Note</p> <p>See the upstream documentation for reference.</p> <p>\u21a9</p>"},{"location":"operator-manual/schema/secrets/#thanos","title":"<code>thanos</code>","text":"<p>Secrets for Thanos.</p> Key Type Default Description thanos.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to thanos. thanos.objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. thanos.objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. thanos.objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with. thanos.receiver\u00b6 object \u2014 Secrets for Thanos Receiver. thanos.receiver.basic_auth\u00b6 object \u2014 Configure authentication to Thanos Receiver, thanos.receiver.basic_auth.password\u00b6 string \u2014 Configure the password for authenticating to Thanos Receiver."},{"location":"operator-manual/schema/secrets/#user","title":"<code>user</code>","text":"<p>Admin password for user Grafana and user Alertmanager.</p> Key Type Default Description user.alertmanagerPassword\u00b6 string \u2014 \u2014 user.grafanaPassword\u00b6 string \u2014 \u2014"},{"location":"operator-manual/schema/secrets/#velero","title":"<code>velero</code>","text":"<p>Secret configuration options for Velero.</p> Key Type Default Description velero.objectStorage\u00b6 object \u2014 Configuration options for using object storage specific to Velero. velero.objectStorage.s3\u00b6 object \u2014 Secrets for using S3 as object storage in Welkin. velero.objectStorage.s3.accessKey\u00b6 string \u2014 Access key to authenticate with. velero.objectStorage.s3.secretKey\u00b6 string \u2014 Secret key to authenticate with."},{"location":"release-notes/","title":"Welkin Release Cycle","text":"<p>Welkin consists of several layers each with their own release cycles.</p> <p>Important</p> <p>Patch versions may be released at any time if Customer systems and Customer Data is at risk.</p> <p>For more details, see ToS 3.6 Vulnerability Management.</p>"},{"location":"release-notes/#welkin-apps","title":"Welkin Apps","text":"<p>For the Apps layer, we aim to release 10 times per year. This corresponds to approximately once a month except for Swedish holiday months, i.e., July and December.</p>"},{"location":"release-notes/#welkin-kubespray","title":"Welkin Kubespray","text":"<p>For the Kubespray layer, we aim to release as soon as there is a new upstream release.</p>"},{"location":"release-notes/#cluster-api","title":"Cluster API","text":"<p>For the Cluster API layer, we aim to release as soon as there is a new upstream release for either:</p> <ul> <li>Cluster API;</li> <li>Cluster API Provider OpenStack; and</li> <li>Cluster API Provider Azure.</li> </ul>"},{"location":"release-notes/argocd/","title":"Release Notes","text":""},{"location":"release-notes/argocd/#welkin-argo-cd","title":"Welkin Argo CD","text":"<ul> <li>v2.14.3-ck8s1 - 2025-03-05</li> <li>v2.12.0-ck8s1 - 2024-09-02</li> <li>v2.9.9-ck8s1 - 2024-03-20</li> <li>v2.9.5-ck8s1 - 2024-01-30</li> <li>v2.7.14-ck8s1 - 2023-10-31</li> <li>v2.4.20-ck8s1 - 2023-03-29</li> </ul>"},{"location":"release-notes/argocd/#v2143-ck8s1","title":"v2.14.3-ck8s1","text":"<p>Released 2025-03-05</p>"},{"location":"release-notes/argocd/#features","title":"Feature(s)","text":"<ul> <li>Added support for read-only local users</li> </ul>"},{"location":"release-notes/argocd/#improvements","title":"Improvement(s)","text":"<ul> <li>Upgraded ArgoCD to v2.14.3</li> </ul>"},{"location":"release-notes/argocd/#v2120-ck8s1","title":"v2.12.0-ck8s1","text":"<p>Released 2024-09-02</p>"},{"location":"release-notes/argocd/#improvements_1","title":"Improvement(s)","text":"<ul> <li>Updated Argo CD to <code>v2.12.0</code>.</li> </ul>"},{"location":"release-notes/argocd/#v299-ck8s1","title":"v2.9.9-ck8s1","text":"<p>Released 2024-03-20</p> <p>Security Notice(s)</p> <ul> <li>This upgrade mitigates the following high and medium security vulnerabilities:</li> <li>Denial of Service (DoS) Vulnerability Due to Unsafe Array Modification in Multi-threaded Environment</li> <li>Bypassing Rate Limit and Brute Force Protection Using Cache Overflow</li> <li>Bypassing Brute Force Protection via Application Crash and In-Memory Data Loss</li> </ul>"},{"location":"release-notes/argocd/#improvements_2","title":"Improvement(s)","text":"<ul> <li>Patch ArgoCD to v2.9.9 - See security notice above.</li> <li>Allow application developers to delete and watch ArgoCD resources they create declaratively.</li> </ul>"},{"location":"release-notes/argocd/#others","title":"Other(s)","text":"<ul> <li>Correct Network Policies for applicationset controller and repo server to allow ApplicationSets to be created.</li> <li>Correct Gatekeeper policies to allow ArgoCD secrets to be restored.</li> </ul>"},{"location":"release-notes/argocd/#v295-ck8s1","title":"v2.9.5-ck8s1","text":"<p>Released 2024-01-30</p>"},{"location":"release-notes/argocd/#updated","title":"Updated","text":"<ul> <li>Updated ArgoCD to <code>v2.9.5</code>.</li> </ul>"},{"location":"release-notes/argocd/#changed","title":"Changed","text":"<ul> <li>Secret <code>helm-secrets-private-keys</code> in the <code>argocd-system</code> namespace (used for storing encryption keys) now uses a different label-value than before. The label has to be: <code>argocd.argoproj.io/secret-type=helm-secrets</code></li> </ul>"},{"location":"release-notes/argocd/#added","title":"Added","text":"<ul> <li>Added NetworkPolicies for Argo CD components</li> <li>Added support for age encrypted helm secrets</li> <li>Added vals as a secret backend</li> </ul>"},{"location":"release-notes/argocd/#v2714-ck8s1","title":"v2.7.14-ck8s1","text":"<p>Released 2023-10-31</p>"},{"location":"release-notes/argocd/#updated_1","title":"Updated","text":"<ul> <li>Updated ArgoCD to <code>v2.7.14</code>.</li> </ul>"},{"location":"release-notes/argocd/#added_1","title":"Added","text":"<ul> <li>Added support for using ArgoCD declaratively</li> <li>Added support for managing secrets via helm-secrets through ArgoCD</li> <li>Added support for managing secrets via sealed secrets through ArgoCD</li> </ul>"},{"location":"release-notes/argocd/#v2420-ck8s1","title":"v2.4.20-ck8s1","text":"<p>Released 2023-03-29</p> <p>First release of Argo CD with version <code>2.4.20</code>!</p>"},{"location":"release-notes/capi/","title":"Release Notes","text":""},{"location":"release-notes/capi/#welkin-cluster-api","title":"Welkin Cluster API","text":"<ul> <li>v0.7.2 - 2025-09-15</li> <li>v0.7.1 - 2025-08-05</li> <li>v0.7.0 - 2025-06-23</li> <li>v0.6.2 - 2025-05-21</li> <li>v0.6.1 - 2025-04-24</li> <li>v0.6.0 - 2025-04-04</li> <li>v0.5.1 - 2025-03-28</li> <li>v0.5.0 - 2025-02-05</li> <li>v0.4.0 - 2024-11-28</li> <li>v0.3.0 - 2024-08-23</li> <li>v0.2.0 - 2024-06-28</li> <li>v0.1.0 - 2024-01-24</li> </ul>"},{"location":"release-notes/capi/#v072","title":"v0.7.2","text":"<p>Released 2025-09-15</p>"},{"location":"release-notes/capi/#improvements","title":"Improvement(s)","text":"<ul> <li>Update Cloud Provider for Azure to 1.33.3</li> </ul>"},{"location":"release-notes/capi/#others","title":"Other(s)","text":"<ul> <li>Update image builder template file</li> <li>Update image.bash script to use OS_AUTH_TYPE</li> </ul>"},{"location":"release-notes/capi/#v071","title":"v0.7.1","text":"<p>Released 2025-08-05</p>"},{"location":"release-notes/capi/#improvements_1","title":"Improvement(s)","text":"<ul> <li>Make ippool blocksize configurable</li> </ul>"},{"location":"release-notes/capi/#v070","title":"v0.7.0","text":"<p>Released 2025-06-23</p>"},{"location":"release-notes/capi/#features","title":"Feature(s)","text":"<ul> <li>Added templating for using local volumes</li> <li>Opened SSH access to Nodes</li> <li>Added OpenStack floatingIP pool for stable egress</li> <li>Added ability to configure custom NTP servers</li> <li>Implemented Kubespray -&gt; ClusterAPI NTP settings migration</li> </ul>"},{"location":"release-notes/capi/#improvements_2","title":"Improvement(s)","text":"<ul> <li>Upgraded cert-manager chart to v1.17.1</li> <li>Reconfigured the containerd registry mirror authentication method</li> <li>Implemented marketplace solution for Azure</li> <li>Unified registry mirror configuration for OpenStack and Azure</li> <li>Upgraded to CAPI v1.10 and CAPO v1.12</li> <li>Allowed skipping health monitor creation for OpenStack Load Balancers</li> <li>Added support for migrating local volumes from Kubespray to capi</li> <li>Added default NTP servers</li> <li>Azure: added configuration options to set disk type and size</li> </ul>"},{"location":"release-notes/capi/#others_1","title":"Other(s)","text":"<ul> <li>Removed all internal references from configuration and docs</li> <li>Cleanup: purged all references to yq3</li> <li>Cleanup: exposed affinity configuration for the local volume provisioner</li> <li>Docs: added DR section for broken API server load balancer</li> <li>Upgraded openstack-cloud-controller-manager to v2.32.0</li> <li>Upgraded openstack-cinder-csi to v2.32.0</li> <li>Fix: labeled CoreDNS PDB Helm release with type=application</li> </ul>"},{"location":"release-notes/capi/#v062","title":"v0.6.2","text":"<p>Released 2025-05-21</p>"},{"location":"release-notes/capi/#otherss","title":"Others(s)","text":"<ul> <li>Remove all internal refs in the repository</li> </ul>"},{"location":"release-notes/capi/#v061","title":"v0.6.1","text":"<p>Released 2025-04-24</p>"},{"location":"release-notes/capi/#improvements_3","title":"Improvement(s)","text":"<ul> <li>update VMs images with containerd v1.7.27</li> </ul>"},{"location":"release-notes/capi/#v060","title":"v0.6.0","text":"<p>Released 2025-04-04</p>"},{"location":"release-notes/capi/#features_1","title":"Feature(s)","text":"<ul> <li>Added support for load balancers to use predefined floating IP addresses.</li> <li>Added local volume provisioner chart.</li> </ul>"},{"location":"release-notes/capi/#improvements_4","title":"Improvement(s)","text":"<ul> <li>Improved migration documentation to allow for less disruptive upgrades.</li> <li>Images built for Elastx now uses VirtIO-SCSI, allowing for more than 25 volumes to be attached to a Node.</li> </ul>"},{"location":"release-notes/capi/#upgraded","title":"Upgraded","text":"<ul> <li>Bumped default Kubernetes version to v1.31.7.</li> </ul>"},{"location":"release-notes/capi/#v051","title":"v0.5.1","text":"<p>Released 2025-03-28</p>"},{"location":"release-notes/capi/#improvements_5","title":"Improvement(s)","text":"<ul> <li>Improved migration documentation to allow for less disruptive upgrades</li> </ul>"},{"location":"release-notes/capi/#v050","title":"v0.5.0","text":"<p>Released 2025-02-05</p>"},{"location":"release-notes/capi/#upgraded_1","title":"Upgraded","text":"<ul> <li>Upgraded Ubuntu images to 24.04</li> <li>Bumped default Kubernetes version to v1.30.9</li> </ul>"},{"location":"release-notes/capi/#v040","title":"v0.4.0","text":"<p>Released 2024-11-28</p>"},{"location":"release-notes/capi/#featuress","title":"Features(s)","text":"<ul> <li>Azure environments now only use one resource group.</li> </ul>"},{"location":"release-notes/capi/#improvements_6","title":"Improvement(s)","text":"<ul> <li><code>kube-system</code> Pods are now prevented from scheduling on autoscaled Nodes, to ensure that the Nodes can be automatically scaled down again.</li> </ul>"},{"location":"release-notes/capi/#v030","title":"v0.3.0","text":"<p>Released 2024-08-23</p>"},{"location":"release-notes/capi/#featuress_1","title":"Features(s)","text":"<ul> <li>Azure now has support for <code>ReadWriteMany</code> volumes. Use the StorageClass <code>azurefile-nfs-premium-lrs</code>.</li> <li>Enabled unattended upgrades for security patching.</li> </ul>"},{"location":"release-notes/capi/#v020","title":"v0.2.0","text":"<p>Released 2024-06-28</p>"},{"location":"release-notes/capi/#features_2","title":"Feature(s)","text":"<ul> <li>Add Azure Cloud as a infrastructure provider</li> </ul>"},{"location":"release-notes/capi/#v010","title":"v0.1.0","text":"<p>Released 2024-01-24</p> <p>First stable release!</p> <p>Compatibility Notice(s)</p> <ul> <li>This version of Welkin Cluster API only supports Cleura and Elastx as infrastructure providers.</li> </ul>"},{"location":"release-notes/jaeger/","title":"Release Notes","text":""},{"location":"release-notes/jaeger/#welkin-jaeger","title":"Welkin Jaeger","text":"<ul> <li>v1.52.0-ck8s1 - 2024-04-26</li> <li>v1.39.0-ck8s3 - 2023-08-24</li> <li>v1.39.0-ck8s2 - 2023-06-12</li> <li>v1.39.0-ck8s1 - 2023-03-07</li> </ul>"},{"location":"release-notes/jaeger/#v1520-ck8s1","title":"v1.52.0-ck8s1","text":"<p>Released 2024-04-26</p> <p>Application Developer Notice(s)</p> <ul> <li>The Jaeger dashboard in Grafana was changed to a new upstream dashboard due to the removal of certain metrics used by the previous dashboard.</li> </ul>"},{"location":"release-notes/jaeger/#improvements","title":"Improvement(s)","text":"<ul> <li>Upgraded Jaeger Operator to v1.52.0.<ul> <li>For a full list of changes, see the changelog.</li> </ul> </li> <li>Upgraded Jaeger OpenSearch and OpenSearch Dashboards to v2.8.0</li> <li>Changed to new upstream Grafana dashboard</li> </ul>"},{"location":"release-notes/jaeger/#v1390-ck8s3","title":"v1.39.0-ck8s3","text":"<p>Released 2023-08-24</p>"},{"location":"release-notes/jaeger/#added","title":"Added","text":"<ul> <li>Add alert for Jaeger OpenSearch</li> </ul>"},{"location":"release-notes/jaeger/#changed","title":"Changed","text":"<ul> <li>Changed location for OpenSearch-curator image.</li> </ul>"},{"location":"release-notes/jaeger/#v1390-ck8s2","title":"v1.39.0-ck8s2","text":"<p>Released 2023-06-12</p>"},{"location":"release-notes/jaeger/#added_1","title":"Added","text":"<ul> <li>Add gatekeeper PSPs</li> </ul>"},{"location":"release-notes/jaeger/#updated","title":"Updated","text":"<ul> <li>Updated Prometheus-elasticsearch-exporter helm chart to 5.1.1</li> </ul>"},{"location":"release-notes/jaeger/#fixed","title":"Fixed","text":"<ul> <li>Increased the proxy buffer size for the oauth Ingress</li> </ul>"},{"location":"release-notes/jaeger/#removed","title":"Removed","text":"<ul> <li>Disabled k8s PSPs from the chart</li> </ul>"},{"location":"release-notes/jaeger/#v1390-ck8s1","title":"v1.39.0-ck8s1","text":"<p>Released 2023-03-07</p> <p>First release using Jaeger operator version 1.39.0!</p>"},{"location":"release-notes/kubespray/","title":"Release Notes","text":""},{"location":"release-notes/kubespray/#welkin-kubespray","title":"Welkin Kubespray","text":"<ul> <li>v2.28.0-ck8s1 - 2025-08-22</li> <li>v2.27.0-ck8s2 - 2025-05-13</li> <li>v2.26.0-ck8s5 - 2025-05-13</li> <li>v2.27.0-ck8s1 - 2025-03-28</li> <li>v2.26.0-ck8s3 - 2025-02-06</li> <li>v2.26.0-ck8s2 - 2025-01-14</li> <li>v2.26.0-ck8s1 - 2024-11-08</li> <li>v2.25.0-ck8s4 - 2024-09-04</li> <li>v2.25.0-ck8s3 - 2024-07-26</li> <li>v2.24.1-ck8s4 - 2024-07-25</li> <li>v2.25.0-ck8s2 - 2024-07-23</li> <li>v2.24.1-ck8s3 - 2024-07-22</li> <li>v2.25.0-ck8s1 - 2024-07-09</li> <li>v2.24.1-ck8s2 - 2024-06-26</li> <li>v2.24.1-ck8s1 - 2024-03-26</li> <li>v2.23.0-ck8s3 - 2024-02-27</li> <li>v2.24.0-ck8s1 - 2024-02-08</li> <li>v2.23.0-ck8s2 - 2024-01-11</li> <li>v2.23.0-ck8s1 - 2023-10-16</li> <li>v2.22.1-ck8s1 - 2023-07-27</li> <li>v2.21.0-ck8s1 - 2023-02-06</li> <li>v2.20.0-ck8s2 - 2022-10-24</li> <li>v2.20.0-ck8s1 - 2022-10-10</li> <li>v2.19.0-ck8s3 - 2022-09-23</li> <li>v2.19.0-ck8s2 - 2022-07-22</li> <li>v2.19.0-ck8s1 - 2022-06-27</li> <li>v2.18.1-ck8s1 - 2022-04-26</li> <li>v2.18.0-ck8s1 - 2022-02-18</li> <li>v2.17.1-ck8s1 - 2021-11-11</li> <li>v2.17.0-ck8s1 - 2021-10-21</li> <li>v2.16.0-ck8s1 - 2021-07-02</li> <li>v2.15.0-ck8s1 - 2021-05-27</li> </ul> <p>Note</p> <p>For a more detailed look check out the full changelog.</p>"},{"location":"release-notes/kubespray/#v2280-ck8s1","title":"v2.28.0-ck8s1","text":"<p>Released 2025-08-22</p>"},{"location":"release-notes/kubespray/#updated","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.28.0</code>    Kubernetes version upgraded to v1.32.5. </li> </ul>"},{"location":"release-notes/kubespray/#features","title":"Feature(s)","text":"<ul> <li>Support for Cilium as network plugin for new Clusters</li> </ul>"},{"location":"release-notes/kubespray/#v2270-ck8s2","title":"v2.27.0-ck8s2","text":"<p>Released 2025-05-13</p>"},{"location":"release-notes/kubespray/#updated_1","title":"Updated","text":"<ul> <li>Updated containerd-version image to <code>v1.7.27</code>   Update containerd-version image to v1.7.27</li> </ul>"},{"location":"release-notes/kubespray/#v2260-ck8s5","title":"v2.26.0-ck8s5","text":"<p>Released 2025-05-13</p>"},{"location":"release-notes/kubespray/#updated_2","title":"Updated","text":"<ul> <li>Updated containerd-version image to <code>v1.7.27</code>   Update containerd-version image to v1.7.27</li> </ul>"},{"location":"release-notes/kubespray/#v2270-ck8s1","title":"v2.27.0-ck8s1","text":"<p>Released 2025-03-28</p>"},{"location":"release-notes/kubespray/#updated_3","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.27.0</code>    Kubernetes version upgraded to v1.31.4. </li> </ul>"},{"location":"release-notes/kubespray/#features_1","title":"Feature(s)","text":"<ul> <li>Added support for Ubuntu 24.04 on UpCloud</li> </ul>"},{"location":"release-notes/kubespray/#improvements","title":"Improvement(s)","text":"<ul> <li>Support multiple LoadBalancers on UpCloud</li> </ul>"},{"location":"release-notes/kubespray/#others","title":"Other(s)","text":"<ul> <li>Default reserved memory for worker Nodes has been increased by <code>256Mi</code>.<ul> <li>Worker Nodes will have <code>256Mi</code> less allocatable memory per Node.</li> </ul> </li> </ul>"},{"location":"release-notes/kubespray/#v2260-ck8s3","title":"v2.26.0-ck8s3","text":""},{"location":"release-notes/kubespray/#improvements_1","title":"Improvement(s)","text":"<ul> <li>Added guide to migrate to Ubuntu 24.04</li> </ul>"},{"location":"release-notes/kubespray/#v2260-ck8s2","title":"v2.26.0-ck8s2","text":""},{"location":"release-notes/kubespray/#improvements_2","title":"Improvement(s)","text":"<ul> <li>Fixed issues that could occur when removing Nodes or replacing control plane Nodes.</li> </ul>"},{"location":"release-notes/kubespray/#v2260-ck8s1","title":"v2.26.0-ck8s1","text":""},{"location":"release-notes/kubespray/#updated_4","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.26.0</code>    Kubernetes version upgraded to v1.30.4. </li> </ul>"},{"location":"release-notes/kubespray/#features_2","title":"Feature(s)","text":"<ul> <li>Support for using existing floating IPs when provisioning Nodes with the Openstack terraform module.</li> <li>Support for provisioning Nodes without public IPs with the Upcloud terraform module.</li> </ul>"},{"location":"release-notes/kubespray/#v2250-ck8s4","title":"v2.25.0-ck8s4","text":""},{"location":"release-notes/kubespray/#improvements_3","title":"Improvement(s)","text":"<ul> <li>Update calico to v3.27.4 to fix high cpu issues</li> <li>Multiple tunnels per connection in UpCloud Gateways</li> </ul>"},{"location":"release-notes/kubespray/#v2250-ck8s3","title":"v2.25.0-ck8s3","text":""},{"location":"release-notes/kubespray/#improvements_4","title":"Improvement(s)","text":"<ul> <li>Fixed UpCloud LoadBalancer to work with legacy network setup</li> </ul>"},{"location":"release-notes/kubespray/#v2241-ck8s4","title":"v2.24.1-ck8s4","text":""},{"location":"release-notes/kubespray/#improvements_5","title":"Improvement(s)","text":"<ul> <li>Fixed UpCloud LoadBalancer to work with legacy network setup</li> </ul>"},{"location":"release-notes/kubespray/#v2250-ck8s2","title":"v2.25.0-ck8s2","text":""},{"location":"release-notes/kubespray/#features_3","title":"Feature(s)","text":"<ul> <li>Added support for UpCloud Router and Gateway</li> </ul>"},{"location":"release-notes/kubespray/#v2241-ck8s3","title":"v2.24.1-ck8s3","text":""},{"location":"release-notes/kubespray/#features_4","title":"Feature(s)","text":"<ul> <li>Added support for UpCloud Router and Gateway</li> </ul>"},{"location":"release-notes/kubespray/#v2250-ck8s1","title":"v2.25.0-ck8s1","text":""},{"location":"release-notes/kubespray/#updated_5","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.25.0</code>    Kubernetes version upgraded to v1.29.5. </li> <li>The default <code>topologySpreadConstraints</code> for kube-scheduler is changed. When upgrading to this version, you may want to review existing Pod scheduling constraints as they could now be redundant.</li> </ul>"},{"location":"release-notes/kubespray/#v2241-ck8s2","title":"v2.24.1-ck8s2","text":"<p>Released 2024-06-26</p>"},{"location":"release-notes/kubespray/#improvements_6","title":"Improvement(s)","text":"<ul> <li>Update Kubespray submodule with private cloud lb and anti affinity changes for UpCloud</li> </ul>"},{"location":"release-notes/kubespray/#v2241-ck8s1","title":"v2.24.1-ck8s1","text":"<p>Released 2024-03-26</p>"},{"location":"release-notes/kubespray/#features_5","title":"Feature(s)","text":"<ul> <li>rook: Add ability to set RBD image features</li> </ul>"},{"location":"release-notes/kubespray/#improvements_7","title":"Improvement(s)","text":"<ul> <li>Update Kubespray submodule to v2.24.1</li> </ul>"},{"location":"release-notes/kubespray/#v2230-ck8s3","title":"v2.23.0-ck8s3","text":""},{"location":"release-notes/kubespray/#updated_6","title":"Updated","text":"<ul> <li>Upgrade containerd to 1.7.13, runc to 1.1.12 and Kubernetes to 1.27.10</li> </ul> <p>This is needed to fix CVE-2024-21626</p>"},{"location":"release-notes/kubespray/#v2240-ck8s1","title":"v2.24.0-ck8s1","text":""},{"location":"release-notes/kubespray/#updated_7","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.24.0</code>    Kubernetes version upgraded to v1.28.6. </li> </ul>"},{"location":"release-notes/kubespray/#v2230-ck8s2","title":"v2.23.0-ck8s2","text":"<p>Released 2024-01-11</p>"},{"location":"release-notes/kubespray/#updated_8","title":"Updated","text":"<ul> <li>Fixed a bug that was making Kubespray fail when running the <code>scale.yml</code> playbook.</li> </ul>"},{"location":"release-notes/kubespray/#v2230-ck8s1","title":"v2.23.0-ck8s1","text":"<p>Released 2023-10-16</p>"},{"location":"release-notes/kubespray/#updated_9","title":"Updated","text":"<ul> <li>Rook version v1.11.9 and Ceph v17.2.6</li> <li>Updated Kubespray to <code>v2.23.0</code>    Kubernetes version upgraded to v1.27.5. </li> </ul>"},{"location":"release-notes/kubespray/#v2221-ck8s1","title":"v2.22.1-ck8s1","text":"<p>Released 2023-07-27</p>"},{"location":"release-notes/kubespray/#updated_10","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.22.1</code>    Kubernetes version upgraded to v1.26.7.    This version requires at least terraform version <code>1.3.0</code> in order to provision infrastructure using the Kubespray provided terraform modules.</li> </ul>"},{"location":"release-notes/kubespray/#changed","title":"Changed","text":"<ul> <li>Updated the Kubernetes audit log policy file</li> </ul>"},{"location":"release-notes/kubespray/#v2210-ck8s1","title":"v2.21.0-ck8s1","text":"<p>Released 2023-02-06</p>"},{"location":"release-notes/kubespray/#updated_11","title":"Updated","text":"<ul> <li>Updated Kubespray to <code>v2.21.0</code>    Kubernetes version upgraded to v1.25.6 in which Pod Security Policies (PSPs) are removed. You should not upgrade to this version if you are using PSPs. To deploy Welkin Apps on this version it needs to be on a compatible version which depends on this issue.    This version requires at least terraform version <code>0.14.0</code> in order to provision infrastructure using the Kubespray provided terraform modules.</li> <li>Upgraded rook-ceph operator to <code>v1.10.5</code> and ceph to <code>v17.2.5</code>    If you are using the rook-ceph operator you can read the migration docs on how to upgrade these components.</li> </ul>"},{"location":"release-notes/kubespray/#changed_1","title":"Changed","text":"<ul> <li>Improved setup for OpenStack with additional server groups    This allows anti-affinity to be set between arbitrary Nodes, improving scheduling and stability.</li> <li>Switched from using upstream Kubespray repository as submodule to the Elastisys fork</li> </ul>"},{"location":"release-notes/kubespray/#added","title":"Added","text":"<ul> <li>Added a get-requirements file to standardize which terraform version to use, <code>1.2.9</code></li> <li>Added ntp.se as standard ntp server</li> </ul>"},{"location":"release-notes/kubespray/#v2200-ck8s2","title":"v2.20.0-ck8s2","text":"<p>Released 2022-10-24</p>"},{"location":"release-notes/kubespray/#changed_2","title":"Changed","text":"<ul> <li>Changed a Kubespray variable which is required for upgrading Clusters on cloud providers that don't have external IPs on their control plane Nodes</li> </ul>"},{"location":"release-notes/kubespray/#v2200-ck8s1","title":"v2.20.0-ck8s1","text":"<p>Released 2022-10-10</p>"},{"location":"release-notes/kubespray/#updated_12","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.20.0</code>    Kubernetes version upgraded to v1.24.6.</li> </ul>"},{"location":"release-notes/kubespray/#changed_3","title":"Changed","text":"<ul> <li>Scripts are now using yq version 4, this requires <code>yq4</code> as an alias to yq v4</li> </ul>"},{"location":"release-notes/kubespray/#fixed","title":"Fixed","text":"<ul> <li>Fixed multiple kube-bench fails (01.03.07, 01.04.01, 01.04.02)</li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s3","title":"v2.19.0-ck8s3","text":"<p>Released 2022-09-23</p>"},{"location":"release-notes/kubespray/#updated_13","title":"Updated","text":"<ul> <li>Bumped UpCloud csi driver to <code>v0.3.3</code></li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s2","title":"v2.19.0-ck8s2","text":"<p>Released 2022-07-22</p>"},{"location":"release-notes/kubespray/#added_1","title":"Added","text":"<ul> <li>Added option to clusteradmin kubeconfigs to use OIDC for authentication</li> <li>Added New Ansible playbooks to manage kubeconfigs and some RBAC</li> </ul>"},{"location":"release-notes/kubespray/#v2190-ck8s1","title":"v2.19.0-ck8s1","text":"<p>Released 2022-06-27.</p>"},{"location":"release-notes/kubespray/#updated_14","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.19.0</code>    Kubernetes version upgraded to 1.23.7.</li> </ul>"},{"location":"release-notes/kubespray/#v2181-ck8s1","title":"v2.18.1-ck8s1","text":"<p>Released 2022-04-26.</p>"},{"location":"release-notes/kubespray/#updated_15","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.18.1</code>    This introduces some fixes for Cluster using containerd as container manager.</li> <li>Updated default etcd version to <code>3.5.3</code>   This fixes an issue where etcd data might get corrupted.</li> </ul>"},{"location":"release-notes/kubespray/#v2180-ck8s1","title":"v2.18.0-ck8s1","text":"<p>Released 2022-02-18.</p>"},{"location":"release-notes/kubespray/#updated_16","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.18.0</code>    Kubernetes upgraded to version 1.22.5.   This introduces new features and fixes, including security updates.   There's also a lot of deprecated API's that were removed in this version so take a good look at these notes before upgrading.</li> </ul>"},{"location":"release-notes/kubespray/#v2171-ck8s1","title":"v2.17.1-ck8s1","text":"<p>Released 2021-11-11.</p>"},{"location":"release-notes/kubespray/#updated_17","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.17.1</code>    Kubernetes version upgraded to 1.21.6, this patch is mostly minor fixes.</li> </ul>"},{"location":"release-notes/kubespray/#v2170-ck8s1","title":"v2.17.0-ck8s1","text":"<p>Released 2021-10-21.</p>"},{"location":"release-notes/kubespray/#updated_18","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.17.0</code>    Kubernetes version upgraded to 1.21.5, this introduces new features and fixes, including security updates and storage capacity tracking.</li> </ul>"},{"location":"release-notes/kubespray/#v2160-ck8s1","title":"v2.16.0-ck8s1","text":"<p>Released 2021-07-02.</p>"},{"location":"release-notes/kubespray/#updated_19","title":"Updated","text":"<ul> <li>Kubespray updated to <code>v2.16.0</code>   Kubernetes version upgraded to 1.20.7, this introduces new features and fixes, including API and component updates.</li> </ul>"},{"location":"release-notes/kubespray/#v2150-ck8s1","title":"v2.15.0-ck8s1","text":"<p>Released 2021-05-27.</p> <p>First stable release!</p>"},{"location":"release-notes/postgres/","title":"Release Notes","text":""},{"location":"release-notes/postgres/#welkin-postgresql","title":"Welkin PostgreSQL","text":"<ul> <li>v1.14.0-ck8s1 - 2025-06-25</li> <li>v1.12.2-ck8s2 - 2025-04-23</li> <li>v1.12.2-ck8s1 - 2024-09-25</li> <li>v1.8.2-ck8s1 - 2022-08-24</li> <li>v1.7.1-ck8s2 - 2022-04-26</li> <li>v1.7.1-ck8s1 - 2021-12-21</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p> <p>Note</p> <p>The public changelog has not been kept up-to-date with development and new releases. Expect the naming schema to change and the content of new releases to be more full and descriptive.</p>"},{"location":"release-notes/postgres/#v1140-ck8s1","title":"v1.14.0-ck8s1","text":"<p>Released 2025-06-25</p> <p>Default PostgreSQL versions:</p> <ul> <li>17.5</li> <li>16.9</li> <li>15.13</li> <li>14.18</li> <li>13.21</li> </ul> <p>Features:</p> <ul> <li>PgBouncer is now available as part of the Managed Service. See this page for more details.</li> <li>PostgreSQL version 17 is now available.</li> <li>New \"developer\" role is available, granting users/groups RBAC to port-forward to the PostgreSQL Cluster without having access to the Secret containing admin credentials.</li> </ul> <p>Changes:</p> <ul> <li>TimescaleDB version bumped to <code>2.20.3</code></li> </ul>"},{"location":"release-notes/postgres/#v1122-ck8s2","title":"v1.12.2-ck8s2","text":"<p>Released 2025-04-23</p> <p>Default PostgreSQL versions:</p> <ul> <li>16.5</li> <li>15.9</li> <li>14.14</li> <li>13.17</li> </ul> <p>Changes:</p> <ul> <li><code>Spilo</code> image version bumped to <code>spilo-16:3.3-p1-24-11-18</code></li> </ul>"},{"location":"release-notes/postgres/#v1122-ck8s1","title":"v1.12.2-ck8s1","text":"<p>Released 2024-09-25</p> <p>Default PostgreSQL versions:</p> <ul> <li>16.4</li> <li>15.8</li> <li>14.13</li> <li>13.16</li> </ul> <p>Changes:</p> <ul> <li>TimescaleDB version bumped to <code>2.14.2</code></li> <li>pgvector version bumped to <code>0.7.4</code></li> </ul>"},{"location":"release-notes/postgres/#v182-ck8s1","title":"v1.8.2-ck8s1","text":"<p>Released 2022-08-24</p> <p>Changes:</p> <ul> <li>Upgraded postgres-operator to version <code>v1.8.2</code></li> <li>Added a service which allows users to port-forward to the service instead of directly to Pods</li> </ul>"},{"location":"release-notes/postgres/#v171-ck8s2","title":"v1.7.1-ck8s2","text":"<p>Released 2022-04-26</p> <p>Changes:</p> <ul> <li>Fixed a vulnerability with logical backups</li> </ul>"},{"location":"release-notes/postgres/#v171-ck8s1","title":"v1.7.1-ck8s1","text":"<p>Released 2021-12-21</p> <p>First stable release!</p>"},{"location":"release-notes/rabbitmq/","title":"Release Notes","text":""},{"location":"release-notes/rabbitmq/#welkin-rabbitmq","title":"Welkin RabbitMQ","text":"<ul> <li>v4.1.4-ck8s1 - 2025-10-16</li> <li>v4.0.6-ck8s3 - 2025-05-20</li> <li>v4.0.6-ck8s2 - 2025-04-25</li> <li>v4.0.6-ck8s1 - 2025-02-28</li> <li>v3.13.7-ck8s1 - 2024-12-09</li> <li>v3.12.6-ck8s1 - 2024-01-17</li> <li>v3.11.18-ck8s1 - 2023-07-03</li> <li>v3.10.7-ck8s1 - 2022-09-21</li> <li>v1.11.1-ck8s2 - 2022-06-08</li> <li>v1.11.1-ck8s1 - 2022-03-11</li> <li>v1.7.0-ck8s1 - 2021-12-23</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p>"},{"location":"release-notes/rabbitmq/#v414-ck8s1","title":"v4.1.4-ck8s1","text":"<p>Released 2025-10-16</p> <p>Application Developer Notice(s)</p> <ul> <li>RabbitMQ has been upgraded to version 4.1. Read through the breaking changes of this version.<ul> <li>Most notably users of amqplib (a popular library for node.js) must upgrade to a compatible version.</li> </ul> </li> <li>Network policies for external traffic now properly adapt based on exposed ports.</li> <li>Allow applications to use the HTTP(S) management API from application developer namespaces.</li> </ul>"},{"location":"release-notes/rabbitmq/#improvements","title":"Improvement(s)","text":"<ul> <li>RabbitMQ has been upgraded to v4.1.</li> <li>A <code>PodDisruptionBudget</code> is now configured for the RabbitMQ Cluster to ensure high availability during voluntary disruptions.</li> <li>The \"RabbitMQ-Overview\" Grafana dashboard has been updated.</li> </ul>"},{"location":"release-notes/rabbitmq/#v406-ck8s3","title":"v4.0.6-ck8s3","text":"<p>Released 2025-05-20</p> <p>Application Developer Notice(s)</p> <ul> <li>Allow applications to use the HTTP(S) management API from application developer namespaces.</li> <li>Network policies for external traffic now properly adapt based on exposed ports.</li> </ul>"},{"location":"release-notes/rabbitmq/#improvements_1","title":"Improvement(s)","text":"<ul> <li>deploy: Add Network Policy enabled flag, enabled by default</li> <li>deploy: Allow internal management from application developer namespaces</li> </ul>"},{"location":"release-notes/rabbitmq/#others","title":"Other(s)","text":"<ul> <li>bug - deploy: Rework internal and external Network Policy rules</li> <li>bug - scripts: Handle CNAME records for object storage hosts</li> </ul>"},{"location":"release-notes/rabbitmq/#v406-ck8s2","title":"v4.0.6-ck8s2","text":"<p>Released 2025-04-25</p>"},{"location":"release-notes/rabbitmq/#others_1","title":"Other(s)","text":"<ul> <li>Correct script to update Cluster DNS for Network Policies, no Application Developer facing change.</li> </ul>"},{"location":"release-notes/rabbitmq/#v406-ck8s1","title":"v4.0.6-ck8s1","text":"<p>Released 2025-02-28</p> <p>Application Developer Notice(s)</p> <ul> <li>Added Network Policies:<ul> <li>For new clusters, these will deny access to the RabbitMQ cluster by default. To gain access to the RabbitMQ clusters, add this label to your pods: <code>elastisys.io/rabbitmq-&lt;cluster_name&gt;-access: allow</code></li> <li>For existing clusters, these will not deny access to the RabbitMQ cluster by default. Once your pods have been labeled with <code>elastisys.io/rabbitmq-&lt;cluster_name&gt;-access: allow</code>, enforcement can be enabled by sending a Service Request to your Platform Administrator.</li> </ul> </li> <li>AMQP 1.0 is now a core protocol, and is always enabled.Classic queue mirroring is now removed.</li> </ul>"},{"location":"release-notes/rabbitmq/#features","title":"Feature(s)","text":"<ul> <li>Network Policies are added to the Cluster deployments which will deny access by default.</li> </ul>"},{"location":"release-notes/rabbitmq/#improvements_2","title":"Improvement(s)","text":"<ul> <li>RabbitMQ is upgraded v4.0, which is a new major release.</li> </ul>"},{"location":"release-notes/rabbitmq/#v3137-ck8s1","title":"v3.13.7-ck8s1","text":"<p>Released 2024-12-09</p>"},{"location":"release-notes/rabbitmq/#improvements_3","title":"Improvement(s)","text":"<ul> <li>Updated Cluster operator to <code>2.9.0</code>, updated server to <code>3.13.7</code> and monitoring to <code>2.9.0</code>.</li> <li>Updated the backup process to use <code>rabbitmqctl</code> for improved reliability and compatibility.</li> <li>Added a <code>LICENSE</code> file to the repository to provide clarity on usage and distribution.</li> </ul>"},{"location":"release-notes/rabbitmq/#v3126-ck8s1","title":"v3.12.6-ck8s1","text":"<p>Released 2024-01-17</p>"},{"location":"release-notes/rabbitmq/#updated","title":"Updated","text":"<ul> <li>Updated server to <code>3.12.6</code></li> <li>Update release process and use new changelog generator</li> </ul>"},{"location":"release-notes/rabbitmq/#v31118-ck8s1","title":"v3.11.18-ck8s1","text":"<p>Released 2023-07-03</p>"},{"location":"release-notes/rabbitmq/#updated_1","title":"Updated","text":"<ul> <li>Updated Cluster operator to <code>v2.3.0</code>, updated server to <code>3.11.18</code></li> <li>Include updated Overview dashboard</li> </ul>"},{"location":"release-notes/rabbitmq/#v3107-ck8s2","title":"v3.10.7-ck8s2","text":"<p>Released 2022-11-28</p>"},{"location":"release-notes/rabbitmq/#fixed","title":"Fixed","text":"<ul> <li>Corrected the Queue Details Grafana dashboard and improved alerting</li> </ul>"},{"location":"release-notes/rabbitmq/#v3107-ck8s1","title":"v3.10.7-ck8s1","text":"<p>Released 2022-09-21</p> <p>Note</p> <p>From this release the version tracks the RabbitMQ server version rather than the RabbitMQ cluster operator version.</p>"},{"location":"release-notes/rabbitmq/#updated_2","title":"Updated","text":"<ul> <li>Upgraded RabbitMQ server version to <code>3.10.7</code>    This is a two minor version jump that introduces new upstream features while remaining compatible with current clients.   The most exciting features includes the new Stream queue type tuned for bulk messaging, and much improved efficiency for Quorum and Classic queue types.   See the upstream changelog for more detailed information.</li> </ul>"},{"location":"release-notes/rabbitmq/#added","title":"Added","text":"<ul> <li>Added support for external access    Using either a LoadBalancer or NodePort Service, additionally with a self-signed chain of trust to enable TLS and host verification.</li> </ul>"},{"location":"release-notes/rabbitmq/#changed","title":"Changed","text":"<ul> <li>Improved observability    Improved the alerting and replaced the per queue metrics source and dashboard, removing the need for an external exporter.</li> </ul>"},{"location":"release-notes/rabbitmq/#v1111-ck8s2","title":"v1.11.1-ck8s2","text":"<p>Released 2022-06-08</p>"},{"location":"release-notes/rabbitmq/#changed_1","title":"Changed","text":"<ul> <li>Reworked monitoring    Added additional metrics collection and a new dashboard to show metrics per queue, and fixed those added by the previous release.</li> <li>Tuned performance    Configured and tuned the performance according to RabbitMQ upstream production checklist.   Including better constraints to improve scheduling for redundancy.</li> </ul>"},{"location":"release-notes/rabbitmq/#v1111-ck8s1","title":"v1.11.1-ck8s1","text":"<p>Released 2022-03-11</p>"},{"location":"release-notes/rabbitmq/#updated_3","title":"Updated","text":"<ul> <li>Upgraded RabbitMQ to version <code>3.8.21</code>    Using Cluster operator version <code>1.11.1</code> providing bugfixes.</li> </ul>"},{"location":"release-notes/rabbitmq/#added_1","title":"Added","text":"<ul> <li>Added definitions-exporter    Taking daily backups of the RabbitMQ messaging topology and users for quick and easy reconfiguring in case of disaster.</li> </ul>"},{"location":"release-notes/rabbitmq/#changed_2","title":"Changed","text":"<ul> <li>Reduced RabbitMQ privilege for security.</li> <li>Improved RabbitMQ observability through better monitoring.</li> </ul>"},{"location":"release-notes/rabbitmq/#v170-ck8s1","title":"v1.7.0-ck8s1","text":"<p>Released 2021-12-23</p> <p>First stable release using RabbitMQ version <code>3.8.16</code>!</p>"},{"location":"release-notes/valkey/","title":"Release Notes","text":""},{"location":"release-notes/valkey/#welkin-valkey-previously-welkin-redis","title":"Welkin Valkey (previously Welkin Redis)","text":"<ul> <li>v8.0.3-ck8s1 - 2025-06-03</li> <li>v7.2.7-ck8s1 - 2025-01-07</li> <li>v7.2.5-ck8s1 - 2024-10-07</li> <li>v6.2.6-ck8s4 - 2024-05-03</li> <li>v6.2.6-ck8s1 - 2023-05-10</li> <li>v1.1.1-ck8s4 - 2022-12-09</li> <li>v1.1.1-ck8s3 - 2022-10-04</li> <li>v1.1.1-ck8s2 - 2022-08-23</li> <li>v1.1.1-ck8s1 - 2022-03-07</li> <li>v1.0.0-ck8s1 - 2021-12-23</li> </ul> <p>Note</p> <p>These are only the user-facing changes.</p>"},{"location":"release-notes/valkey/#v803-ck8s1","title":"v8.0.3-ck8s1","text":"<p>Released 2025-06-03</p>"},{"location":"release-notes/valkey/#release-highlights","title":"Release highlights","text":"<ul> <li>Redis has been replaced with Valkey.</li> </ul>"},{"location":"release-notes/valkey/#v727-ck8s1","title":"v7.2.7-ck8s1","text":"<p>Released 2025-01-07</p>"},{"location":"release-notes/valkey/#release-highlights_1","title":"Release highlights","text":"<ul> <li>Security patch for CVE-2024-46981</li> </ul>"},{"location":"release-notes/valkey/#improvements","title":"Improvement(s)","text":"<ul> <li>Added more alerts to capture more scenarios</li> </ul>"},{"location":"release-notes/valkey/#v725-ck8s1","title":"v7.2.5-ck8s1","text":"<p>Released 2024-10-07</p>"},{"location":"release-notes/valkey/#release-highlights_2","title":"Release highlights","text":"<ul> <li>Redis version will be upgraded from v6.2.6 to v7.2.5</li> </ul>"},{"location":"release-notes/valkey/#features","title":"Feature(s)","text":"<ul> <li>Added netpolicy for Harbor HA</li> </ul>"},{"location":"release-notes/valkey/#improvements_1","title":"Improvement(s)","text":"<ul> <li>Split CPU usage alert by container (Redis and sentinel)</li> <li>Update Redis image to 7.2.5<ul> <li>Redis version will be upgraded from v6.2.6 to v7.2.5</li> </ul> </li> </ul>"},{"location":"release-notes/valkey/#others","title":"Other(s)","text":"<ul> <li>Add Redis_Cluster_NAME to user-access-ConfigMap</li> <li>Fixed the sentinel metrics port</li> </ul>"},{"location":"release-notes/valkey/#v626-ck8s4","title":"v6.2.6-ck8s4","text":"<p>Released 2024-05-03</p> <p>Application Developer Notice(s)</p> <ul> <li>From now on Network Policies will deny access to the Redis cluster by default. To gain access to the Redis clusters add this label to your pods: <code>elastisys.io/redis-&lt;cluster_name&gt;-access: allow</code></li> </ul>"},{"location":"release-notes/valkey/#features_1","title":"Feature(s)","text":"<ul> <li>Network Policies are added to the Cluster deployments which will deny access by default.</li> </ul>"},{"location":"release-notes/valkey/#improvements_2","title":"Improvement(s)","text":"<ul> <li>Added stricter Sentinel scheduling for better resilience to Node failure.</li> <li>Scaled down <code>maxmemory</code> to better prevent Redis Pods from getting OOMKilled.</li> </ul>"},{"location":"release-notes/valkey/#v626-ck8s1","title":"v6.2.6-ck8s1","text":"<p>Released 2023-05-10</p> <p>Note</p> <p>From this release the version tracks the Redis version rather than the Redis operator version.</p> <p>Changes:</p> <ul> <li>Changed to standard timezone in Grafana dashboard</li> <li>Upgraded the redis-operator to <code>v1.2.4</code> and Chart version to <code>v3.2.8</code></li> </ul> <p>Added:</p> <ul> <li>Added RBAC for users to be able to port-forward to Redis</li> <li>Added nodeAffinity for the label <code>elastisys.io/ams-cluster-name</code> which will be set for each Cluster in <code>values.yaml</code></li> </ul>"},{"location":"release-notes/valkey/#v111-ck8s4","title":"v1.1.1-ck8s4","text":"<p>Released 2022-12-09</p> <p>Changes:</p> <ul> <li>Improved alerting and scheduling for better operational management and safety.</li> </ul>"},{"location":"release-notes/valkey/#v111-ck8s3","title":"v1.1.1-ck8s3","text":"<p>Released 2022-10-04</p> <p>Changes:</p> <ul> <li>Fixed the safety of replication when master has persistence turned off</li> </ul>"},{"location":"release-notes/valkey/#v111-ck8s2","title":"v1.1.1-ck8s2","text":"<p>Released 2022-08-23</p> <p>Changes:</p> <ul> <li>Improved support for running multiple Redis Clusters in one Kubernetes environment.</li> </ul>"},{"location":"release-notes/valkey/#v111-ck8s1","title":"v1.1.1-ck8s1","text":"<p>Released 2022-03-07</p> <p>Changes:</p> <ul> <li>Upgraded redis-operator to <code>v1.1.1</code></li> </ul>"},{"location":"release-notes/valkey/#v100-ck8s1","title":"v1.0.0-ck8s1","text":"<p>Released 2021-12-23</p> <p>First stable release!</p>"},{"location":"release-notes/welkin/","title":"Welkin","text":""},{"location":"release-notes/welkin/#release-notes","title":"Release Notes","text":""},{"location":"release-notes/welkin/#welkin","title":"Welkin","text":"<ul> <li>v0.49.0 - 2025-10-07</li> <li>v0.48.1 - 2025-10-15</li> <li>v0.48.0 - 2025-08-05</li> <li>v0.47.3 - 2025-09-30</li> <li>v0.47.2 - 2025-07-31</li> <li>v0.47.1 - 2025-07-17</li> <li>v0.47.0 - 2025-06-25</li> <li>v0.46.1 - 2025-07-17</li> <li>v0.46.0 - 2025-05-15</li> <li>v0.45.1 - 2025-03-25</li> <li>v0.45.0 - 2025-03-21</li> <li>v0.44.2 - 2025-03-25</li> <li>v0.44.1 - 2025-03-12</li> <li>v0.44.0 - 2025-02-21</li> <li>v0.43.3 - 2025-03-28</li> <li>v0.43.2 - 2025-03-25</li> <li>v0.43.1 - 2025-03-12</li> <li>v0.43.0 - 2025-01-27</li> <li>v0.42.4 - 2025-03-28</li> <li>v0.42.3 - 2025-03-25</li> <li>v0.42.2 - 2025-03-12</li> <li>v0.42.1 - 2025-01-02</li> <li>v0.42.0 - 2024-11-14</li> <li>v0.41.0 - 2024-10-02</li> <li>v0.40.1 - 2024-09-18</li> <li>v0.40.0 - 2024-08-21</li> <li>v0.39.2 - 2024-09-04</li> <li>v0.39.1 - 2024-07-15</li> <li>v0.39.0 - 2024-06-19</li> <li>v0.38.2 - 2024-06-19</li> <li>v0.38.1 - 2024-05-24</li> <li>v0.38.0 - 2024-05-17</li> <li>v0.37.0 - 2024-04-12</li> <li>v0.36.0 - 2024-02-12</li> <li>v0.34.2 - 2024-01-16</li> <li>v0.35.1 - 2024-01-16</li> <li>v0.34.1 - 2023-12-22</li> <li>v0.35.0 - 2023-12-20</li> <li>v0.34.0 - 2023-11-21</li> <li>v0.33.1 - 2023-10-20</li> <li>v0.32.2 - 2023-10-20</li> <li>v0.33.0 - 2023-09-28</li> <li>v0.32.0 - 2023-08-07</li> <li>v0.31.0 - 2023-07-17</li> <li>v0.30.1 - 2023-06-05</li> <li>v0.30.0 - 2023-05-16</li> <li>v0.29.0 - 2023-03-16</li> <li>v0.28.1 - 2023-03-02</li> <li>v0.28.0 - 2023-01-30</li> <li>v0.27.0 - 2022-11-17</li> <li>v0.26.0 - 2022-09-19</li> <li>v0.25.0 - 2022-08-25</li> <li>v0.24.1 - 2022-08-01</li> <li>v0.24.0 - 2022-07-25</li> <li>v0.23.0 - 2022-07-06</li> <li>v0.22.0 - 2022-06-01</li> <li>v0.21.0 - 2022-05-04</li> <li>v0.20.0 - 2022-03-21</li> <li>v0.19.1 - 2022-03-01</li> <li>v0.19.0 - 2022-02-01</li> <li>v0.18.2 - 2021-12-16</li> <li>v0.17.2 - 2021-12-16</li> <li>v0.18.1 - 2021-12-08</li> <li>v0.17.1 - 2021-12-08</li> <li>v0.18.0 - 2021-11-04</li> <li>v0.17.0 - 2021-06-29</li> <li>v0.16.0 - 2021-05-27</li> </ul> <p>Note</p> <p>For a more detailed look check out the full changelog.</p>"},{"location":"release-notes/welkin/#v0490","title":"v0.49.0","text":"<p>Released 2025-10-07</p> <p>Application Developer Notice(s)</p> <ul> <li>Prometheus by default overwrites the namespace label for metrics that already has a namespace label, to the namespace of the pod that was scraped for metrics. This is now changed for the instances that were currently affected. The namespace label will now keep the original information, usually the namespace of the object it is referring to, e.g. for cert-manager this is usually the namespace where a certificate exists instead of the cert-manager namespace.Prometheus added a exported_namespace label when it was overwriting the namespace label, with the original value of the namespace label. This is kept for now, but it is deprecated and will be removed in a later version, likely v0.52.</li> </ul>"},{"location":"release-notes/welkin/#features","title":"Feature(s)","text":"<ul> <li>Added a Fluentd metric and alert which catches rejections due to mapping conflicts in OpenSearch.</li> </ul>"},{"location":"release-notes/welkin/#improvements","title":"Improvement(s)","text":"<ul> <li>Upgraded OpenSearch to v2.19.3.</li> <li>Make namespace label in metric refer to resource, not exporter.</li> <li>Added logging for failing DNS requests.</li> <li>OpenSearch namespace is now PSS restricted.</li> </ul>"},{"location":"release-notes/welkin/#v0481","title":"v0.48.1","text":"<p>Released 2025-10-15</p>"},{"location":"release-notes/welkin/#improvements_1","title":"Improvement(s)","text":"<ul> <li>Mirror kubectl bitnami image</li> </ul>"},{"location":"release-notes/welkin/#v0480","title":"v0.48.0","text":"<p>Released 2025-08-05</p> <p>Application Developer Notice(s)</p> <ul> <li>Grafana was upgraded to new major version 12, this comes some new features such as the \"Drilldown\" page. Please refer to the upstream release notes to see all changes.</li> </ul>"},{"location":"release-notes/welkin/#features_1","title":"Feature(s)","text":"<ul> <li>Added a safeguard to enforce signed image verification. This safeguard is disabled by default and can be enabled by your Platform Administrator. You can read more about this safeguard in our documentation.</li> <li>Added Cilium support.</li> <li>Added dual-stack support.</li> </ul>"},{"location":"release-notes/welkin/#improvements_2","title":"Improvement(s)","text":"<ul> <li>Upgraded Harbor to v2.13.1.</li> <li>Upgraded Falco chart to v6.0.2.</li> <li>Upgraded Grafana to v12.0.3.</li> </ul>"},{"location":"release-notes/welkin/#v0473","title":"v0.47.3","text":"<p>Released 2025-09-30</p>"},{"location":"release-notes/welkin/#improvements_3","title":"Improvement(s)","text":"<ul> <li>Patched calico-accountant.</li> </ul>"},{"location":"release-notes/welkin/#v0472","title":"v0.47.2","text":"<p>Released 2025-07-31</p>"},{"location":"release-notes/welkin/#changes-by-kind","title":"Changes by kind","text":""},{"location":"release-notes/welkin/#improvements_4","title":"Improvement(s)","text":"<ul> <li>Upgrade Falco chart to v6.0.2</li> </ul>"},{"location":"release-notes/welkin/#others","title":"Other(s)","text":"<ul> <li>Fix indexpernamespace for OpenSearch alerting role</li> <li>Bump Grafana image version to 11.5.7</li> </ul>"},{"location":"release-notes/welkin/#v0471","title":"v0.47.1","text":"<p>Released 2025-07-17</p>"},{"location":"release-notes/welkin/#others_1","title":"Other(s)","text":"<ul> <li>Fix NetworkPolicy for NGINX to allow uptime Ingress</li> </ul>"},{"location":"release-notes/welkin/#v0470","title":"v0.47.0","text":"<p>Released 2025-06-25</p> <p>Application Developer Notice(s)</p> <ul> <li>Alertmanager has been reworked.</li> <li>The configuration of the user alertmanager secret has been updated. It is now managed via the <code>alertmanager-kube-prometheus-stack-alertmanager</code> secret.</li> </ul>"},{"location":"release-notes/welkin/#features_2","title":"Feature(s)","text":"<ul> <li>Added configuration options to control the user session timeout for OpenSearch Dashboards. These can be adjusted by the administrator.</li> <li>Centralized container image configuration</li> </ul>"},{"location":"release-notes/welkin/#improvements_5","title":"Improvement(s)","text":"<ul> <li>Improved monitoring stability by inreasing Thanos distributor replicas</li> </ul>"},{"location":"release-notes/welkin/#v0461","title":"v0.46.1","text":"<p>Released 2025-07-17</p>"},{"location":"release-notes/welkin/#v0460","title":"v0.46.0","text":"<p>Released 2025-05-15</p> <p>Security Notice(s)</p> <ul> <li>Ingress-nginx upgraded to address the following CVEs:     -CVE-2025-24513     -CVE-2025-24514     -CVE-2025-1097     -CVE-2025-1098     -CVE-2025-1974</li> </ul> <p>Application Developer Notice(s)</p> <ul> <li>A new GPU access rule is now configured, you need to explicitly request an NVIDIA GPU in your application's resource requests specification to access the GPU drivers.</li> <li>A new gatekeeper policy has been added that will deny any PodDisruptionBudget and connected Pod controller if the PodDisruptionBudget does not allow at least 1 Pod disruption.</li> </ul>"},{"location":"release-notes/welkin/#features_3","title":"Feature(s)","text":"<ul> <li>Add OPA policy to restrict PDBs, always allow at least 1 disruption</li> <li>Add support for Kyverno</li> <li>Allow exposing internal Ingress-NGINX LoadBalancer in workload Cluster</li> </ul>"},{"location":"release-notes/welkin/#improvements_6","title":"Improvement(s)","text":"<ul> <li>Upgrade Grafana to 11.5.1 (chart 8.9.1)</li> <li>Upgrade GPU operator to 24.9.2, allow additional configuration options</li> <li>Upgrade Ingress-NGINX chart to v4.12.1</li> <li>Upgrade OPA/gatekeeper to v3.18.2</li> <li>Upgrade kube-prometheus-stack to 70.6.0</li> </ul>"},{"location":"release-notes/welkin/#others_2","title":"Other(s)","text":"<ul> <li>Fix GPU driver insufficient for CUDA runtime issue in operator v24.9.2</li> <li>Bump kubectl version to v1.31.7 in the requirements</li> </ul>"},{"location":"release-notes/welkin/#v0451","title":"v0.45.1","text":"<p>Released 2025-03-25</p> <p>Security Notice(s)</p> <ul> <li>Ingress-NGINX was upgraded to address the following vulnerabilities:<ul> <li>CVE-2025-24513</li> <li>CVE-2025-24514</li> <li>CVE-2025-1097</li> <li>CVE-2025-1098</li> <li>CVE-2025-1974</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#others_3","title":"Other(s)","text":"<ul> <li>Upgraded Ingress-NGINX chart to v4.12.1</li> </ul>"},{"location":"release-notes/welkin/#v0450","title":"v0.45.0","text":"<p>Released 2025-03-21</p> <p>Security Notice(s)</p> <ul> <li>Upgraded cert-manager to v1.17.1 which addresses critical CVE-2024-45337</li> </ul> <p>Application Developer Notice(s)</p> <ul> <li>A new guardrail is added that is enabled by default on ClusterAPI environments. It will by default warn but not deny usage of emptyDir storage, since this can stop cluster autoscaler from scaling down nodes. Read more on this page.</li> <li>A new guardrail is added that is enabled by default on ClusterAPI environments. It will by default warn but not deny usage of Pods without backing controllers, since this can stop cluster autoscaler from scaling down nodes. Read more on this page.</li> <li>Cert-manager was upgraded to v1.17.1. This comes with some potentially breaking changes for Venafi Issuer</li> <li>RBAC to modify the configmaps <code>fluentd-extra-config</code> and <code>fluentd-extra-plugins</code>, and to delete any fluentd pod in the <code>fluentd</code> namespace has been removed.Reach out to a platform administrator if any additional config or plugins are needed!</li> </ul>"},{"location":"release-notes/welkin/#features_4","title":"Feature(s)","text":"<ul> <li>Add gatekeeper policies to reject local storage emptydir and reject Pods without controller</li> <li>Add cert-manager mixin dashboard</li> </ul>"},{"location":"release-notes/welkin/#improvements_7","title":"Improvement(s)","text":"<ul> <li>Update helm/trivy-operator to 0.26.0 and trivy-operator to 0.24.0</li> <li>Upgrade cert-manager helm chart to v1.17.1</li> <li>Upgrade Thanos chart to v15.13.1</li> </ul>"},{"location":"release-notes/welkin/#deprecations","title":"Deprecation(s)","text":"<ul> <li>Remove rbac for additional Fluentd config and plugins configmaps</li> </ul>"},{"location":"release-notes/welkin/#others_4","title":"Other(s)","text":"<ul> <li>Change query expression for LessKubeletsThanNodes alerts</li> <li>Fixed S3 size alert to not double count postgres buckets</li> </ul>"},{"location":"release-notes/welkin/#v0442","title":"v0.44.2","text":"<p>Released 2025-03-25</p> <p>Security Notice(s)</p> <ul> <li>Ingress-NGINX was upgraded to address the following vulnerabilities:<ul> <li>CVE-2025-24513</li> <li>CVE-2025-24514</li> <li>CVE-2025-1097</li> <li>CVE-2025-1098</li> <li>CVE-2025-1974</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#others_5","title":"Other(s)","text":"<ul> <li>Upgraded Ingress-NGINX chart to v4.12.1</li> </ul>"},{"location":"release-notes/welkin/#v0441","title":"v0.44.1","text":"<p>Released 2025-03-12</p> <p>Application Developer Notice(s)</p> <ul> <li>Permissions to modify the additional Fluentd ConfigMaps have been removed.Reach out to a Platform Administrator if any changes to the Fluentd config or plugins are needed!</li> </ul>"},{"location":"release-notes/welkin/#deprecations_1","title":"Deprecation(s)","text":"<ul> <li>Remove permissions for Application Developers to modify the additional Fluentd ConfigMaps</li> </ul>"},{"location":"release-notes/welkin/#v0440","title":"v0.44.0","text":"<p>Released 2025-02-21</p> <p>Application Developer Notice(s)</p> <ul> <li>Prometheus has been upgraded to version 3.0. This includes changes to the Prometheus UI. Prometheus V3 comes with some changes that may affect existing PromQL expressions in alerts or dashboards. Please have a look at the Prometheus V3 migration guide.</li> <li>The NVIDIA GPU Operator was added to Welkin. The Operator can be installed by a Platform Administrator and facilitates provisioning GPU on infrastructure providers where this is supported.</li> </ul>"},{"location":"release-notes/welkin/#features_5","title":"Feature(s)","text":"<ul> <li>Added the NVIDIA GPU Operator.</li> </ul>"},{"location":"release-notes/welkin/#improvements_8","title":"Improvement(s)","text":"<ul> <li>Upgraded kube-prometheus-stack to 67.11.0.</li> <li>Upgraded Falco to v0.40.0.</li> <li>Upgraded Fluentd to v1.18.0.</li> <li>Upgraded OpenSearch and OpenSearch Dashboards to v2.18.0.</li> </ul>"},{"location":"release-notes/welkin/#others_6","title":"Other(s)","text":"<ul> <li>Changed some messages and documentation to Welkin, as part of our rebranding.</li> <li>Fixed KubeContainerOOMKilled alert for newly created Pods.</li> </ul>"},{"location":"release-notes/welkin/#v0433","title":"v0.43.3","text":"<p>Released 2025-03-28</p>"},{"location":"release-notes/welkin/#others_7","title":"Other(s)","text":"<ul> <li>Made Fluentd-forwarder image configurable</li> </ul>"},{"location":"release-notes/welkin/#v0432","title":"v0.43.2","text":"<p>Released 2025-03-25</p> <p>Security Notice(s)</p> <ul> <li>Ingress-NGINX was upgraded to address the following vulnerabilities:<ul> <li>CVE-2025-24513</li> <li>CVE-2025-24514</li> <li>CVE-2025-1097</li> <li>CVE-2025-1098</li> <li>CVE-2025-1974</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#others_8","title":"Other(s)","text":"<ul> <li>Upgraded Ingress-NGINX chart to v4.12.1</li> </ul>"},{"location":"release-notes/welkin/#v0431","title":"v0.43.1","text":"<p>Released 2025-03-12</p> <p>Application Developer Notice(s)</p> <ul> <li>Permissions to modify the additional Fluentd ConfigMaps have been removed.Reach out to a Platform Administrator if any changes to the Fluentd config or plugins are needed!</li> </ul>"},{"location":"release-notes/welkin/#deprecations_2","title":"Deprecation(s)","text":"<ul> <li>Remove permissions for Application Developers to modify the additional Fluentd ConfigMaps</li> </ul>"},{"location":"release-notes/welkin/#v0430","title":"v0.43.0","text":"<p>Released 2025-01-27</p> <p>Security Notice(s)</p> <ul> <li>Upgrades Grafana to <code>11.3.0</code> to fix CVE-2024-9264</li> <li>OpenSearch Dashboards was upgraded to <code>2.17.1</code> which mitigates CVE-2024-45801</li> </ul>"},{"location":"release-notes/welkin/#improvements_9","title":"Improvement(s)","text":"<ul> <li>Upgraded Grafana to 11.3.0 and chart to 8.5.9</li> <li>Upgraded OpenSearch to v2.17.1</li> <li>Upgraded Harbor to v2.12.1</li> </ul>"},{"location":"release-notes/welkin/#others_9","title":"Other(s)","text":"<ul> <li>Added JSON schema contribution guide</li> <li>Rebrand to Welkin in configuration and documentation</li> </ul>"},{"location":"release-notes/welkin/#v0424","title":"v0.42.4","text":"<p>Released 2025-03-28</p>"},{"location":"release-notes/welkin/#others_10","title":"Other(s)","text":"<ul> <li>Made Fluentd-forwarder image configurable</li> </ul>"},{"location":"release-notes/welkin/#v0423","title":"v0.42.3","text":"<p>Released 2025-03-25</p> <p>Security Notice(s)</p> <ul> <li>Ingress-NGINX was upgraded to address the following vulnerabilities:<ul> <li>CVE-2025-24513</li> <li>CVE-2025-24514</li> <li>CVE-2025-1097</li> <li>CVE-2025-1098</li> <li>CVE-2025-1974</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#others_11","title":"Other(s)","text":"<ul> <li>Upgraded Ingress-NGINX chart to v4.12.1</li> </ul>"},{"location":"release-notes/welkin/#v0422","title":"v0.42.2","text":"<p>Released 2025-03-12</p> <p>Application Developer Notice(s)</p> <ul> <li>Permissions to modify the additional Fluentd ConfigMaps have been removed.Reach out to a Platform Administrator if any changes to the Fluentd config or plugins are needed!</li> </ul>"},{"location":"release-notes/welkin/#deprecations_3","title":"Deprecation(s)","text":"<ul> <li>Remove permissions for Application Developers to modify the additional Fluentd ConfigMaps</li> </ul>"},{"location":"release-notes/welkin/#v0421","title":"v0.42.1","text":"<p>Released 2025-01-02</p>"},{"location":"release-notes/welkin/#others_12","title":"Other(s)","text":"<ul> <li>Use Mirror for Bitnami images</li> </ul>"},{"location":"release-notes/welkin/#v0420","title":"v0.42.0","text":"<p>Released 2024-11-14</p>"},{"location":"release-notes/welkin/#features_6","title":"Feature(s)","text":"<ul> <li>Added Prometheus metrics to the diagnostics script</li> </ul>"},{"location":"release-notes/welkin/#improvements_10","title":"Improvement(s)","text":"<ul> <li>Upgraded Grafana to v11.2.3.<ul> <li>Drops support for dashboards using Angular plugins. See here for more information.</li> </ul> </li> <li>Upgraded Falco to v0.38.2.</li> <li>Fix deprecated Grafana dashboards</li> </ul>"},{"location":"release-notes/welkin/#v0410","title":"v0.41.0","text":"<p>Released 2024-10-02</p>"},{"location":"release-notes/welkin/#features_7","title":"Feature(s)","text":"<ul> <li>Added option to use Velero CSI volume snapshot.</li> </ul>"},{"location":"release-notes/welkin/#improvements_11","title":"Improvement(s)","text":"<ul> <li>Added NetworkPolicies for Tekton to reduce blast radius.</li> <li>Enabled matchConditions for HNC webhook by default.<ul> <li>This is used to allow Velero to restore certain resources managed by HNC.</li> </ul> </li> <li>Fixed deprecated Grafana dashboards.</li> <li>Improved support for logging stack on Azure.</li> <li>Fixed a bug with Harbor restore on Azure.</li> <li>Improvements to automated testing.</li> <li>Node-local-dns was upgraded to v1.23.1.</li> <li>Added a flag to include config files for diagnostics script.</li> </ul>"},{"location":"release-notes/welkin/#others_13","title":"Other(s)","text":"<ul> <li>Calico-accountant image and chart was upgraded.</li> </ul>"},{"location":"release-notes/welkin/#v0401","title":"v0.40.1","text":"<p>Released 2024-09-18</p> <p>Security Notice(s)</p> <ul> <li>This patch release mitigates the following high and medium severity vulnerabilities:<ul> <li>Ingress-NGINX CVE-2024-7646</li> <li>Grafana CVE-2024-6837</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#improvements_12","title":"Improvement(s)","text":"<ul> <li>Ingress-NGINX was upgraded to v1.11.2 and NGINX itself to v1.25.5.</li> <li>Grafana was upgraded to v10.4.7.</li> </ul>"},{"location":"release-notes/welkin/#others_14","title":"Other(s)","text":"<ul> <li>Fix for install-requirements script.</li> <li>Fixed a broken Cluster status dashboard.</li> </ul>"},{"location":"release-notes/welkin/#v0400","title":"v0.40.0","text":"<p>Released 2024-08-21</p> <p>Application Developer Notice(s)</p> <ul> <li>Alerts for kured in workload clusters have been removed.</li> </ul>"},{"location":"release-notes/welkin/#improvements_13","title":"Improvement(s)","text":"<ul> <li>OpenSearch and OpenSearch Dashboards has been upgraded to v2.15.0.</li> <li>Harbor has been upgraded to v2.11.0.</li> <li>Dex has been upgraded to v2.40.0.</li> <li>Added Node filter to more graphs in the Kubernetes status dashboard.</li> <li>Increased default Grafana timeout.</li> <li>Improved diagnostics script.</li> </ul>"},{"location":"release-notes/welkin/#others_15","title":"Other(s)","text":"<ul> <li>Various fixes for install-requirements script.</li> <li>Replaced deprecated Angular panels in Daily and Backup Dashboards.</li> </ul>"},{"location":"release-notes/welkin/#v0392","title":"v0.39.2","text":"<p>Released 2024-09-04</p> <p>Security Notice(s)</p> <ul> <li>This patch release mitigates the following high and medium severity vulnerabilities:<ul> <li>Ingress-NGINX CVE-2024-7646</li> <li>Grafana CVE-2024-6837</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#improvements_14","title":"Improvement(s)","text":"<ul> <li>Ingress-NGINX was upgraded to v1.11.2 and NGINX itself to v1.25.5.</li> <li>Grafana was upgraded to v10.4.7.</li> </ul>"},{"location":"release-notes/welkin/#v0391","title":"v0.39.1","text":"<p>Released 2024-07-15</p>"},{"location":"release-notes/welkin/#improvements_15","title":"Improvement(s)","text":"<ul> <li>Increased rclone default job deadline to 14400 seconds allowing for rclone to complete its operations.</li> </ul>"},{"location":"release-notes/welkin/#v0390","title":"v0.39.0","text":"<p>Released 2024-06-19</p> <p>Application Developer Notice(s)</p> <ul> <li>There is a new feature to allow certain Pods access to Prometheus, e.g. for remote-read or federation from another Prometheus instance. This feature has to be enabled by a Platform Administrator, who will require a list of namespaces to allow access from. You must also label each Pod that should have access with <code>elastisys.io/prometheus-access: allow</code>.</li> </ul>"},{"location":"release-notes/welkin/#features_8","title":"Feature(s)","text":"<ul> <li>Add config option to allow internal traffic to Prometheus.</li> </ul>"},{"location":"release-notes/welkin/#improvements_16","title":"Improvement(s)","text":"<ul> <li>Trivy Operator was upgraded to v0.20.1.</li> <li>Velero was upgraded to v1.13.0.</li> </ul>"},{"location":"release-notes/welkin/#others_16","title":"Other(s)","text":"<ul> <li>Fixed an error that would occur when applying with rclone enabled.</li> </ul>"},{"location":"release-notes/welkin/#v0382","title":"v0.38.2","text":"<p>Release 2024-06-19</p>"},{"location":"release-notes/welkin/#others_17","title":"Other(s)","text":"<ul> <li>Fixed various platform administrator bugs.</li> </ul>"},{"location":"release-notes/welkin/#v0381","title":"v0.38.1","text":"<p>Release 2024-05-24</p>"},{"location":"release-notes/welkin/#others_18","title":"Other(s)","text":"<ul> <li>Fixed an error that would occur when applying with rclone enabled.</li> </ul>"},{"location":"release-notes/welkin/#v0380","title":"v0.38.0","text":"<p>Released 2024-05-17</p>"},{"location":"release-notes/welkin/#features_9","title":"Feature(s)","text":"<ul> <li>A new Gatekeeper constraint was added. It will warn if the user tries to deploy a Deployment or StatefulSet with less than 2 replicas.</li> </ul>"},{"location":"release-notes/welkin/#improvements_17","title":"Improvement(s)","text":"<ul> <li>Thanos was upgraded to v0.34.1.</li> <li>Gatekeeper was upgraded to v3.15.1.</li> </ul>"},{"location":"release-notes/welkin/#v0370","title":"v0.37.0","text":"<p>Released 2024-04-12</p>"},{"location":"release-notes/welkin/#improvements_18","title":"Improvement(s)","text":"<ul> <li>OpenSearch and OpenSearch Dashboards were upgraded to v2.12</li> <li>Grafana was upgraded to v10.4</li> <li>Falco was upgraded to v0.37.1</li> <li>A new capacity management Grafana dashboard is now available. This will give better visibility over resource usage per Node groups</li> <li>We recommend using the ingressClassName field over the <code>class</code> field for cert-manager issuers</li> </ul>"},{"location":"release-notes/welkin/#v0360","title":"v0.36.0","text":"<p>Released 2024-02-12</p>"},{"location":"release-notes/welkin/#features_10","title":"Feature(s)","text":"<ul> <li>Added some initial disk performance alerts</li> <li>Added probe Ingress to monitor services</li> </ul>"},{"location":"release-notes/welkin/#improvements_19","title":"Improvement(s)","text":"<ul> <li>Upgrade Velero to v1.11.1</li> </ul>"},{"location":"release-notes/welkin/#others_19","title":"Other(s)","text":"<ul> <li>bug - Fixed issue where large Harbor backups would fail</li> <li>clean-up - Removed the ciskubernetesbenchmark dashboard from Grafana</li> </ul>"},{"location":"release-notes/welkin/#v0342","title":"v0.34.2","text":"<p>Released 2024-01-16</p>"},{"location":"release-notes/welkin/#improvements_20","title":"Improvement(s)","text":"<ul> <li>Added more configuration options for Trivy-Operator</li> </ul>"},{"location":"release-notes/welkin/#v0351","title":"v0.35.1","text":"<p>Released 2024-01-16</p>"},{"location":"release-notes/welkin/#improvements_21","title":"Improvement(s)","text":"<ul> <li>Added more configuration options for Trivy-Operator</li> </ul>"},{"location":"release-notes/welkin/#v0341","title":"v0.34.1","text":"<p>Released 2023-12-22</p>"},{"location":"release-notes/welkin/#improvements_22","title":"Improvement(s)","text":"<ul> <li>Updated trivy-operator helm chart to v0.19.1 and application to v0.17.1</li> </ul>"},{"location":"release-notes/welkin/#others_20","title":"Other(s)","text":"<ul> <li>bug - Fixed issue where large Harbor backups would fail</li> </ul>"},{"location":"release-notes/welkin/#v0350","title":"v0.35.0","text":"<p>Released 2023-12-20</p> <p>Security Notice(s)</p> <ul> <li>Enabling \"chroot\" for the ingress-nginx controller is one way to limit nginx inside the ingress-nginx controller container from having access to list secrets cluster-wide.   Note that this also allows the controller to use the <code>unshare</code> and <code>clone</code> syscalls which are not normally allowed when using the default seccompProfile.</li> </ul> <p>Application Developer Notice(s)</p> <ul> <li>As of Harbor v2.9, Notary V1 is removed. If you rely on this for artifact signing, you will need to migrate to one of the alternatives. You can read more about this here.</li> </ul>"},{"location":"release-notes/welkin/#features_11","title":"Feature(s)","text":"<ul> <li>Added option to run NGINX in chroot</li> <li>Added support for self-managed Kafka</li> </ul>"},{"location":"release-notes/welkin/#improvements_23","title":"Improvement(s)","text":"<ul> <li>Upgrade Harbor to v2.9.1</li> </ul>"},{"location":"release-notes/welkin/#v0340","title":"v0.34.0","text":"<p>Released 2023-11-23</p> <p>Security Notice(s)</p> <ul> <li>New curl release (CVE-2023-38545 and CVE-2023-38546)</li> <li>New Go release (https://github.com/advisories/GHSA-qppj-fm5r-hxr3 and https://github.com/advisories/GHSA-4374-p667-p6c8)</li> <li>Fix for HTTP/2 Rapid Reset Attack CVE-2023-44487</li> </ul>"},{"location":"release-notes/welkin/#features_12","title":"Feature(s)","text":"<ul> <li>Dashboard for visualizing how spread-out Pods are across Nodes</li> <li>Application developers can now self manage CRDs for MongoDB, SealedSecrets and Flux</li> <li>Upgrade HNC and expose opt-in propagation</li> </ul>"},{"location":"release-notes/welkin/#improvements_24","title":"Improvement(s)","text":"<ul> <li>Update Gatekeeper violation messages</li> <li>Add Network Policies for hnc</li> <li>Upgrade Ingress-NGINX controller to 1.8.4 and chart to 4.7.3</li> <li>Upgrade Falco chart and rework exceptions</li> </ul>"},{"location":"release-notes/welkin/#v0331","title":"v0.33.1","text":"<p>Released 2023-10-20</p>"},{"location":"release-notes/welkin/#updated","title":"Updated","text":"<ul> <li>Ingress-NGINX controller to 1.8.4 and chart to 4.7.3 (HTTP/2 fix for CVE-2023-44487)<ul> <li>a limit of no more than 2 * max_concurrent_streams new streams per one event loop iteration was introduced</li> <li>refused streams are now limited to maximum of max_concurrent_streams and 100</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#v0322","title":"v0.32.2","text":"<p>Released 2023-10-20</p>"},{"location":"release-notes/welkin/#updated_1","title":"Updated","text":"<ul> <li>Ingress-NGINX controller to 1.8.4 and chart to 4.7.3 (HTTP/2 fix for CVE-2023-44487)<ul> <li>a limit of no more than 2 * max_concurrent_streams new streams per one event loop iteration was introduced</li> <li>refused streams are now limited to maximum of max_concurrent_streams and 100</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#v0330","title":"v0.33.0","text":"<p>Released 2023-09-28</p>"},{"location":"release-notes/welkin/#changed","title":"Changed","text":"<ul> <li>Increased the default <code>proxy-buffer-size</code> setting in Ingress-NGINX to <code>8k</code>.</li> </ul>"},{"location":"release-notes/welkin/#fixed","title":"Fixed","text":"<ul> <li>Refer to Grafana, OpenSearch and Harbor as Web Portals in Grafana and OpenSearch welcome dashboards</li> </ul>"},{"location":"release-notes/welkin/#removed","title":"Removed","text":"<ul> <li>Removed the deprecated Grafana dashboard Image vulnerabilities.</li> </ul>"},{"location":"release-notes/welkin/#v0320","title":"v0.32.0","text":"<p>Released 2023-08-07</p>"},{"location":"release-notes/welkin/#updated_2","title":"Updated","text":"<ul> <li>Upgraded Falco chart version to <code>3.3.0</code> and app version to <code>0.35.1</code>.</li> </ul>"},{"location":"release-notes/welkin/#added","title":"Added","text":"<ul> <li>Added support to turn off trailing dots for Grafana.<ul> <li>This fixes an issue with the certificate for Grafana appearing not to be valid on some browsers.</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#changed_1","title":"Changed","text":"<ul> <li>Increased window for <code>FrequentPacketsDroppedFromWorkload</code> and <code>FrequentPacketsDroppedToWorkload</code> alerts.<ul> <li>To make it less sensitive to semi-consistent blocked network traffic.</li> </ul> </li> <li>Reduced CPU requests for some components in the service Cluster.</li> </ul>"},{"location":"release-notes/welkin/#fixed_1","title":"Fixed","text":"<ul> <li>Added some default annotations for Harbor that will fix issues with not being able to upload larger images.</li> <li>Fixed the Gatekeeper Grafana dashboard.<ul> <li>Updated queries to produce correct numbers</li> <li>Removed broken/duplicate panels</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#v0310","title":"v0.31.0","text":"<p>Released 2023-07-17</p>"},{"location":"release-notes/welkin/#updated_3","title":"Updated","text":"<ul> <li>Harbor is upgraded to <code>v2.8.2</code>.<ul> <li>This version drops the support for chartmuseum and replaces it with a OCI compatible chart storage. You can find the documentation for how to use OCI compatible chart storage here.</li> <li>They are also replacing the Notary image signer with Cosign image signer. You can find the documentation for how to use Cosign to sign images here.</li> <li>Dex is now the default login page.</li> </ul> </li> <li>Ingess-NGINX is upgraded to <code>v1.8.0</code>.</li> <li>Grafana is upgraded to <code>v9.5.5</code>.</li> <li>OpenSearch and OpenSearch Dashboard are upgraded to <code>v2.8.0</code>.</li> </ul>"},{"location":"release-notes/welkin/#added_1","title":"Added","text":"<ul> <li>Added RBAC for admin users to view events and logs.</li> <li>Possibility to add custom config for Node-local-dns.</li> <li>Harbor GC is enabled by default and will run every Sunday at midnight UTC.</li> </ul>"},{"location":"release-notes/welkin/#v0301","title":"v0.30.1","text":"<p>Released 2023-06-05</p>"},{"location":"release-notes/welkin/#updated_4","title":"Updated","text":"<ul> <li>Update Trivy Operator Dashboard to improve the user experience.</li> <li>Another Network Policy fix for Harbor to allow garbage collection.</li> <li>Fixed duplicate exception for Falco alerts.</li> <li>Update Falco rules and Falco alert exceptions.</li> </ul>"},{"location":"release-notes/welkin/#changed_2","title":"Changed","text":"<ul> <li>Change Trivy Operator Dashboard to only count image states once per image instead for each namespace and resource.</li> </ul>"},{"location":"release-notes/welkin/#v0300","title":"v0.30.0","text":"<p>Released 2023-05-16</p>"},{"location":"release-notes/welkin/#added_2","title":"Added","text":"<ul> <li>Kubernetes Jobs will now have a default TTL of 7 days if unset to ensure resources are cleaned up.</li> </ul>"},{"location":"release-notes/welkin/#updated_5","title":"Updated","text":"<ul> <li>kube-prometheus-stack chart to <code>v45.2.0</code>.<ul> <li>the portName for Alertmanager and Prometheus have been renamed from web to http-web. If this port names are used by you application or to port-forward to Prometheus/Alertmanager, you will need to update them to http-web or use the port numbers instead (e.g 9090 for Prometheus and 9093 for Alertmanager);</li> <li>added default metric relabeling for cAdvisor and apiserver metrics to reduce cardinality;</li> <li>Alertmanager, using regex field from the Matcher type is deprecated and it will be removed in a future version.</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#changed_3","title":"Changed","text":"<ul> <li>Kubernetes PodSecurityPolcies have been replaced with Kubernetes Pod Security Standards and additional Gatekeeper Constraints and Mutations.<ul> <li>This should not affect user applications as the default behavior is kept, and the new default restricted Pod Security Standard is slightly less restricted than the previous restricted PodSecurityPolicy following the upstream changes;</li> <li>You might see <code>warnings</code> generated by PodSecurity while deploying manifests into your Kubernetes Cluster, if fields are unset or do not follow the Restricted policy for the Pod Security Standards. If fields are unset, the new Gatekeeper mutations will set defaults, that follow the restricted Pod Security Standards, as the Pods get scheduled.</li> </ul> </li> <li>Trivy Operator has replaced Starboard Operator as the online security scanning tool.<ul> <li>This includes a new Trivy Operator dashboard and the deprecation of the old Image vulnerabilities dashboard.</li> </ul> </li> <li>Both <code>responseObject</code> and <code>requestObject</code> are no longer dropped in Fluentd from Kubernetes audit events.</li> <li>Changed timekey to stageTimestamp for Kubernetes audit logs. Use auditID to correlate stages of the same request.</li> </ul>"},{"location":"release-notes/welkin/#removed_1","title":"Removed","text":"<ul> <li>Remove HNC admin-rbac from admin (attached to user admins).<ul> <li>User admins will now only have the HNC user-rbac instead.</li> </ul> </li> <li>Removed the ability to edit HierarchyConfiguration for users.<ul> <li>HierarchyConfiguration controls the Pod Security Standard level, and as such should not be allowed to be changed by a user.</li> </ul> </li> <li>Disable Non sudo setuid Falco rule.</li> </ul>"},{"location":"release-notes/welkin/#v0290","title":"v0.29.0","text":"<p>Released 2023-03-16</p>"},{"location":"release-notes/welkin/#added_3","title":"Added","text":"<ul> <li>Static users can now be added in OpenSearch.</li> </ul>"},{"location":"release-notes/welkin/#changed_4","title":"Changed","text":"<ul> <li>The Fluentd Deployment has changed considerably and users must ensure that their custom filters continue to work as expected.</li> </ul>"},{"location":"release-notes/welkin/#updated_6","title":"Updated","text":"<ul> <li>cert-manager updated to <code>v1.11.0</code>.<ul> <li>The containers in Pods created by cert-manager have been renamed to better reflect what they do. This can be breaking for automation that relies on these names being static.</li> <li>The cert-manager Gateway API integration now uses the v1beta1 API version. ExperimentalGatewayAPISupport alpha feature users must upgrade to v1beta of Gateway API.</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#v0281","title":"v0.28.1","text":"<p>Released 2023-03-02</p>"},{"location":"release-notes/welkin/#added_4","title":"Added","text":"<ul> <li>Added Falco rules to ignore Redis operator related alerts.</li> </ul>"},{"location":"release-notes/welkin/#v0280","title":"v0.28.0","text":"<p>Released 2023-01-30</p>"},{"location":"release-notes/welkin/#changed_5","title":"Changed","text":"<ul> <li>Updated Rook alerts to <code>v1.10.5</code>.</li> <li>NGINX Ingress Controller service can now have multiple annotations instead of just one.</li> <li>Synced all Grafana dashboards to use the default organization timezone.</li> <li>Several default resource requests and limits have changed for the included services.</li> </ul>"},{"location":"release-notes/welkin/#fixed_2","title":"Fixed","text":"<ul> <li>Use FQDN for services connecting from the Workload Cluster to the service Cluster to prevent resolve timeouts.</li> <li>Fixed <code>KubeletDown</code> alert rule not alerting if a kubelet was missing.</li> <li>Added permissions to the <code>alerting_full_access</code> role in OpenSearch to be able to view notification channels.</li> <li>Added <code>fluent-plugin-record-modifier</code> to the Fluentd image to prevent mapping errors.</li> <li>Various fixes to Network Policies.</li> </ul>"},{"location":"release-notes/welkin/#added_5","title":"Added","text":"<ul> <li>Improved security posture by adding Network Policies for some of the networking and storage components.</li> <li>Added alert for less kubelets than Nodes in the Cluster.</li> <li>Added alert for object limits in buckets.</li> </ul>"},{"location":"release-notes/welkin/#v0270","title":"v0.27.0","text":"<p>Released 2022-11-17</p>"},{"location":"release-notes/welkin/#updated_7","title":"Updated","text":"<ul> <li>Updated Dex helm chart to <code>v0.12.0</code>, which also upgrades Dex to <code>v2.35.1</code>.</li> <li>Updated Falco helm chart to <code>2.2.0</code>, which also upgrades Falco to <code>0.33.0</code> and Falco Sidekick to <code>2.26.0</code>.</li> <li>Updated Falco Exporter helm chart to <code>0.9.0</code>, which also upgrades Falco Exporter to <code>0.8.0</code>.</li> <li>Updated Velero helm chart to <code>v2.31.8</code>, which also upgrades Velero to <code>v1.9.2</code>.</li> <li>Updated Grafana helm chart to <code>v6.43.4</code>, which also upgrades Grafana to <code>v9.2.4</code>.</li> </ul>"},{"location":"release-notes/welkin/#changed_6","title":"Changed","text":"<ul> <li>Improved Network security by adding Network Policies to a lot of the included services.</li> <li>NetworkPolicies are now automatically propagated from a parent namespace to its subnamespaces in HNC.</li> <li>Several default resource requests and limits have changed for the included services.</li> <li>Lowered the default retention age for Kubernetes logs in the prod flavor down to 30 days.</li> <li>Made Dex ID Token expiration time configurable.</li> <li>User Alertmanager is now enabled by default.</li> </ul>"},{"location":"release-notes/welkin/#fixed_3","title":"Fixed","text":"<ul> <li>Fixed an issue with the \"Kubernetes Cluster status\" Grafana dashboard not loading data for some panels</li> <li>Rclone can now be configured to run every x minutes/hours/days/week/month/year.</li> </ul>"},{"location":"release-notes/welkin/#added_6","title":"Added","text":"<ul> <li>Added RBAC for admin users to view Gatekeeper constraints.</li> <li>New section in the welcoming dashboards, displaying the most relevant features and changes for the user added in the last two releases.</li> <li>Added an option to configure alerts for growing indices in OpenSearch.<ul> <li>The settings for this might need to be tweaked to better suit the environment.</li> </ul> </li> <li>Added an alert for failed evicted Pods (KubeFailedEvictedPods).</li> </ul>"},{"location":"release-notes/welkin/#v0260","title":"v0.26.0","text":"<p>Released 2022-09-19</p>"},{"location":"release-notes/welkin/#updated_8","title":"Updated","text":"<ul> <li>Harbor upgraded to <code>v2.6.0</code></li> <li>Upgraded OpenSearch helm chart to <code>2.6.0</code>, this upgrades OpenSearch to <code>2.3.0</code>. For more information about the upgrade, check out their 2.3 Launch Announcement.</li> </ul>"},{"location":"release-notes/welkin/#fixed_4","title":"Fixed","text":"<ul> <li>Fixed the welcome dashboard template for OpenSearch Dashboards</li> </ul>"},{"location":"release-notes/welkin/#added_7","title":"Added","text":"<ul> <li>Option to create custom solvers for Let's Encrypt issuers, including a simple way to add secrets</li> <li>Kube-bench runs on every Node   Automated CIS tests are performed on each Node using kube-bench   Added a CIS kube-bench Grafana dashboard</li> <li>Added option for kured to notify to slack when draning and rebooting Nodes</li> <li>Allow users to proxy and port-forward to Prometheus running in the Workload Cluster</li> </ul>"},{"location":"release-notes/welkin/#v0250","title":"v0.25.0","text":"<p>Released 2022-08-25</p>"},{"location":"release-notes/welkin/#added_8","title":"Added","text":"<ul> <li>Added Hierarchical Namespace Controller Allowing users to create and manage subnamespaces, namespaces within namespaces. You can read more about this in our FAQ.</li> <li>Added support for custom solvers in Cluster issuers  Allowing DNS01 challenges for certificate requests.</li> <li>Added support for running Harbor in High Availability</li> </ul>"},{"location":"release-notes/welkin/#updated_9","title":"Updated","text":"<ul> <li> <p>Updated cert-manager from v1.6.1 to v1.8.2  API versions <code>v1alpha2</code>, <code>v1alpha3</code>, and <code>v1beta1</code> have been removed from the custom resource definitions (CRDs), certificate rotation policy will now be validated. See their changelog for more details.</p> </li> <li> <p>Updated OpenSearch with new usability improvements and features  Checkout their launch announcement.</p> </li> </ul>"},{"location":"release-notes/welkin/#changed_7","title":"Changed","text":"<ul> <li>New additions to the Kubernetes Cluster status Grafana dashboard  It now shows information about resource requests and limits per Node, and resource usage vs request per Pod.</li> </ul>"},{"location":"release-notes/welkin/#v0241","title":"v0.24.1","text":"<p>Released 2022-08-01</p> <ul> <li>Required patch to be able to use release <code>v0.24.0</code></li> </ul>"},{"location":"release-notes/welkin/#fixed_5","title":"Fixed","text":"<ul> <li>Fixed a formatting issue with Harbor S3 configuration.</li> </ul>"},{"location":"release-notes/welkin/#v0240","title":"v0.24.0","text":"<p>Released 2022-07-25</p>"},{"location":"release-notes/welkin/#updated_10","title":"Updated","text":"<ul> <li> <p>Upgraded Helm stack   Upgrades for Helm, Helmfile and Helm-secrets.</p> </li> <li> <p>Image upgrade to Node-local-dns</p> </li> </ul>"},{"location":"release-notes/welkin/#changed_8","title":"Changed","text":"<ul> <li>Improved stability to automatic Node reboots</li> </ul>"},{"location":"release-notes/welkin/#added_9","title":"Added","text":"<ul> <li>Further configurability to Ingress-NGINX</li> </ul>"},{"location":"release-notes/welkin/#v0230","title":"v0.23.0","text":"<p>Released 2022-07-06</p>"},{"location":"release-notes/welkin/#updated_11","title":"Updated","text":"<ul> <li>Updated the Ingress Controller <code>ingress-nginx</code> to image version v1.2.1<ul> <li>You can find the changelog here.</li> </ul> </li> </ul>"},{"location":"release-notes/welkin/#changed_9","title":"Changed","text":"<ul> <li>Added support for accessing Alertmanager via port-forward</li> </ul>"},{"location":"release-notes/welkin/#added_10","title":"Added","text":"<ul> <li>Backups can now be encrypted before they are replicated to an off-site S3 service.</li> <li>Improved metrics and alerting for OpenSearch.</li> </ul>"},{"location":"release-notes/welkin/#fixed_6","title":"Fixed","text":"<ul> <li>The Deployment of Dex is now properly configured to be HA, ensuring that the Dex instances are placed on different Kubernetes worker Nodes.</li> </ul>"},{"location":"release-notes/welkin/#v0220","title":"v0.22.0","text":"<p>Released 2022-06-01</p>"},{"location":"release-notes/welkin/#added_11","title":"Added","text":"<ul> <li> <p>Added support for Elastx and UpCloud!</p> </li> <li> <p>New 'Welcoming' dashboard in OpenSearch and Grafana.   Users can now access public docs and different urls to the services provided by Welkin.</p> </li> <li> <p>Improved availability of metrics and alerting.   Alertmanager now runs with two replicas by default, Prometheus can now be run in HA mode.</p> </li> <li> <p>Added Falco rules to reduce alerts for services in Welkin.   Falco now alerts less on operations that are expected out of these services.</p> </li> </ul>"},{"location":"release-notes/welkin/#fixed_7","title":"Fixed","text":"<ul> <li> <p>Fixed a bug where users couldn't silence alerts when portforwarding to Alertmanager.</p> </li> <li> <p>Improved logging stack and fixed a number of issues to ensure reliability.</p> </li> </ul>"},{"location":"release-notes/welkin/#v0210","title":"v0.21.0","text":"<p>Released 2022-05-04</p>"},{"location":"release-notes/welkin/#changed_10","title":"Changed","text":"<ul> <li> <p>Users can now view ClusterIssuers.</p> </li> <li> <p>User admins can now add users to the ClusterRole user-view.   This is done by adding users to the ClusterRoleBinding <code>extra-user-view</code>.</p> </li> <li> <p>User can now get ClusterIssuers.</p> </li> <li> <p>Ensured all CISO dashboards are available to users.   All the Grafana dashboards in our CISO docs are now available.</p> </li> <li> <p>Better stability for Dex   Dex now runs with two replicas and has been updated.</p> </li> </ul>"},{"location":"release-notes/welkin/#updated_12","title":"Updated","text":"<ul> <li>Image upgrades to reduce number of vulnerabilities   Upgrades for Fluentd, Grafana, and Harbor chartmuseum.</li> </ul>"},{"location":"release-notes/welkin/#v0200","title":"v0.20.0","text":"<p>Released 2022-03-21</p>"},{"location":"release-notes/welkin/#added_12","title":"Added","text":"<ul> <li> <p>Added kured - Kubernetes Reboot Daemon.   This enables automatic Node reboots and security patching of the underlying base Operating System image, container runtime and Kubernetes Cluster components.</p> </li> <li> <p>Added Fluentd Grafana dashboard and alerts.</p> </li> <li> <p>Added RBAC for admin users.   Admin users can now list Pods Cluster wide and run the kubectl top command.</p> </li> <li> <p>Added containerd support for Fluentd.</p> </li> </ul>"},{"location":"release-notes/welkin/#changed_11","title":"Changed","text":"<ul> <li> <p>Added the new OPA policy.   To disallow the latest image tag.</p> </li> <li> <p>Persist Dex state in Kubernetes.   This ensure the JWT token received from an OpenID provider is valid even after security patching of Kubernetes Cluster components.</p> </li> <li> <p>Add ingressClassName in ingresses where that configuration option is available.</p> </li> <li> <p>Thanos is now enabled by default.</p> </li> </ul>"},{"location":"release-notes/welkin/#updated_13","title":"Updated","text":"<ul> <li> <p>Upgraded Ingress-NGINX helm chart to v4.0.17   This upgrades Ingress-NGINX to v1.1.1. When upgrading an ingressClass object called NGINX will be installed, this class has been set as the default class in Kubernetes. Ingress-NGINX has been configured to still handle existing Ingress objects that do not specify any ingressClassName.</p> </li> <li> <p>Upgraded starboard-operator helm chart to v0.9.1   This is upgrading starboard-operator to v0.14.1</p> </li> </ul>"},{"location":"release-notes/welkin/#removed_2","title":"Removed","text":"<ul> <li>Removed influxDB and dependent helm charts.</li> </ul>"},{"location":"release-notes/welkin/#v0191","title":"v0.19.1","text":"<p>Released 2022-03-01</p>"},{"location":"release-notes/welkin/#fixed_8","title":"Fixed","text":"<ul> <li>Fixed critical stability issue related to Prometheus rules being evaluated without metrics.</li> </ul>"},{"location":"release-notes/welkin/#v0190","title":"v0.19.0","text":"<p>Released 2022-02-01</p>"},{"location":"release-notes/welkin/#added_13","title":"Added","text":"<ul> <li> <p>Added Thanos as a new metrics backend.   Provides a much more efficient and reliable platform for long-term metrics, with the capabilities to keep metrics for much longer time periods than previously possible.   InfluxDB will still be supported in this release.</p> </li> <li> <p>Added a new feature to enable off-site replication of backups.   Synchronizes S3 buckets across regions or clouds to keep an off-site backup.</p> </li> <li> <p>Added a new feature to create and log into separate indices per namespace. Currently considered to be an alpha feature.</p> </li> </ul>"},{"location":"release-notes/welkin/#changed_12","title":"Changed","text":"<ul> <li> <p>Replacing Open Distro for Elasticsearch with OpenSearch.   In this release, since the Open Distro project has reached end of life, Elasticsearch is replaced with OpenSearch and Kibana with OpenSearch Dashboards.   OpenSearch is a fully open source fork of Elasticsearch with a compatible API and familiar User Experience. Note that recent versions of official Elasticsearch clients and tools will not work with OpenSearch as they employ a product check, compatible versions can be found here.</p> </li> <li> <p>Enforcing OPA policies by default.   Provides strict safeguards by default.</p> </li> <li> <p>Allowing viewers to inspect and temporarily edit panels in Grafana.   Gives more insight to the metrics and data shown.</p> </li> <li> <p>Setting Fluentd to log the reason why when it can't push logs to OpenSearch.</p> </li> </ul>"},{"location":"release-notes/welkin/#updated_14","title":"Updated","text":"<ul> <li>Large number of application and service updates, keeping up to date with new security fixes and changes.</li> </ul>"},{"location":"release-notes/welkin/#v0182","title":"v0.18.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/welkin/#v0172","title":"v0.17.2","text":"<p>Released 2021-12-16.</p> <p>Changes:</p> <ul> <li>Updated Open Distro for Elasticsearch to 1.13.3 to mitigate CVE-2021-44228 &amp; CVE-2021-45046</li> </ul>"},{"location":"release-notes/welkin/#v0181","title":"v0.18.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/welkin/#v0171","title":"v0.17.1","text":"<p>Released 2021-12-08.</p> <p>Changes:</p> <ul> <li>updated Grafana to 8.0.7 in order to fix CVE-2021-43798</li> </ul>"},{"location":"release-notes/welkin/#v0180","title":"v0.18.0","text":"<p>Released 2021-11-04.</p> <p>Changes:</p> <ul> <li>Ingress-NGINX-controller has been updated from v0.28.0 to v0.49.3, bringing various updates.<ul> <li>Additionally, the configuration option <code>allow-snippet-annotations</code> has been set to <code>false</code> to mitigate known security issue CVE-2021-25742</li> </ul> </li> <li>Fixes, minor version upgrades, improvements to resource requests and limits for applications, improvements to stability.</li> </ul>"},{"location":"release-notes/welkin/#v0170","title":"v0.17.0","text":"<p>Released 2021-06-29.</p> <p>Changes:</p> <ul> <li>The dashboard tool Grafana has been updated to a new major version of 8.x.x. This introduces new features and fixes, as well as some possibly breaking changes. See their release notes for more information.</li> <li>The single-sign-on service Dex has been updated, bringing small changes and better consistency to the UI.</li> <li>Fixes, improvements to resource limits, resource usage, and stability.</li> </ul>"},{"location":"release-notes/welkin/#v0160","title":"v0.16.0","text":"<p>Released 2021-05-27.</p> <p>Changes:</p> <ul> <li>The default retention values have been changed and streamlined for <code>authlog*</code> and <code>other*</code>. The former will be kept for a longer period of time while the latter for shorter, both have reduced sized according to their actual usage.</li> <li>Updates, fixes, and features to improve the security of the platform.</li> </ul>"},{"location":"user-guide/","title":"Application Developer Overview","text":"<p>We know software developers are busy people that want to get up and running as soon as possible!</p> <p>Use the navigational bar to the side to jump to the section that interests you the most.</p>","boost":2},{"location":"user-guide/#orientation-the-devsecops-loop","title":"Orientation: The DevSecOps Loop","text":"<p>As an application platform, the main job of Welkin is to reduce your cognitive load. The picture above helps you get a good grasp of how Welkin can support your everyday work. It consists of a DevSecOps loop. As you can see, Welkin integrates open-source projects, which help you do application Ops (operations) and application Sec (security).</p> <p>Specifically:</p> <ul> <li>Harbor: Is a container registry. You can use it to store container images produced by your Continuous Integration (CI) solution. Welkin does not require a particular CI solution. It facilitates security by having fine grained access control and built-in container image vulnerability scanning.</li> <li>Argo CD (Additional Managed Service): Is a Continuous Delivery (CD) solution. It helps you deploy your application -- usually represented by a Helm Chart -- into a Welkin environment. Argo CD pulls changes from a Git repository, hence, it allows you practice GitOps, which improves security by reducing the number of people who need Kubernetes access.</li> <li>Kubernetes: Is the \"engine\" of the platform, the \"spider in the net\" if you will. Welkin security-hardens Kubernetes, e.g., with restrictive access control, Pod Security Standards and OpenID authentication.</li> <li>Grafana: Allows you to observe application metrics. It also hosts several dashboards which allow you to demonstrate compliance with common security controls.</li> <li>OpenSearch: Allows you to observe application and platform logs. It is also home to platform audit logs, which allows you to determine who did what and when. This improves security both by reducing incentives to act carelessly and by facilitating after-the-fact investigations.</li> <li>Jaeger (Additional Managed Service): Allows you to observe application traces. Jaeger can further simplify incident and performance management.</li> <li>Falco: Observes your application and alerts in case of behavior which is suspecious security-wise. This improves security by watching for \"unknown unknowns\".</li> <li>cert-manager: Automates provisioning of TLS certificates. This makes it easy for you to implement encryption-at-rest over untrusted networks.</li> <li>Velero: Handles backups and disaster recovery. This enables the platform administrator to help you recover even from the worst incident.</li> <li>Open Policy Agent: Enforces guardrails to avoid trivial security mistakes, which may lead to compromising information confidentiality, integrity or availability. Guardrails instill a culture of security by making it easier to use Welkin the right way.</li> <li>Dex: Integrates Welkin with your Identity Provider (IdP). This improves security by making sure that each application developer accesses the platform with an individual account. Said individual account makes its way into platform audit logs, which store who did what and when.</li> <li>Trivy: Scans containers for known security vulnerabilities. This helps you deliver code which is free from vulnerabilities, which is an essential security requirement.</li> <li>Rclone: Copies the primary backup to a secondary backup infrastructure provider. This improves resilience against ransomware attacks by making it harder for an attacker to compromise backups.</li> <li>Kured: Automates application of kernel and base Operating System (OS) patches. This essentially it does automated vulnerability management \"below\" the container runtime.</li> </ul>","boost":2},{"location":"user-guide/#getting-started-quickly","title":"Getting started quickly","text":"<p>Welkin is a Kubernetes distribution that consists of the best (community-driven) open source components in the cloud native space, configured for security and platform stability. It does not contain any proprietary technology, and no vendor-specific tooling, such as command-line tools or abstractions that only work in this distribution. To the greatest extent possible, all technology contained within the distribution is community-driven open source, as in, not under a single vendor's control or governance. The distribution is itself open source, and is also designed and developed in a transparent manner (see our Architectural Decision Records).</p> <p>Your administrator has already set up the platform for you. You will therefore have received:</p> <ul> <li>URLs for the Service Endpoints: OpenSearch Dashboards, Grafana, and Harbor;</li> <li>a kubeconfig file for configuring <code>kubectl</code> or Lens access to the Workload Cluster; and</li> <li>(optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Azure Active Directory, or Google Identity.</li> </ul>","boost":2},{"location":"user-guide/#install-prerequisite-software","title":"Install Prerequisite Software","text":"<p>Required software:</p> <ul> <li>oidc-login, which helps you log into your Kubernetes Cluster via OpenID Connect integration with your Identity Provider of choice</li> </ul> <p>Your Cluster management software of choice, of which you can choose either or both:</p> <ul> <li>kubectl, a command-line tool to help manage your Kubernetes resources</li> <li>Kubernetes VS Code extension, a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Kubernetes UIs)</li> </ul> <p>Optional, but very useful, tools for developers and DevOps engineers:</p> <ul> <li>docker, if you want to build (Docker) container images locally</li> <li>Helm, if you want to manage your application with the Helm package manager</li> </ul> You can verify that configuration is correct by issuing the following simple commands <p>Make sure you have configured your tools properly:</p> <pre><code>export KUBECONFIG=path/of/kubeconfig.yaml  # leave empty if you use the default of ~/.kube/config\nexport DOMAIN=  # the domain you received from the administrator\n</code></pre> <p>To verify if the required tools are installed and work as expected, type:</p> <pre><code>docker version\nkubectl version  --client\nhelm version\n# You should see the version number of installed tools and no errors.\n</code></pre> <p>To verify the received KUBECONFIG, type:</p> <pre><code># Notice that you will be asked to complete browser-based single sign-on\nkubectl get nodes\n# You should see the Nodes of your Kubernetes cluster\n</code></pre> <p>To verify the received URLs, type:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --head https://harbor.$DOMAIN/healthz\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://opensearch.$DOMAIN/api/status\ncurl --insecure --head https://app.$DOMAIN/healthz  # Ingress Controller\n# All commands above should return 'HTTP/2 200'\n</code></pre>","boost":2},{"location":"user-guide/#endpoint-access","title":"Endpoint access","text":"<p>Those URLs that your Welkin administrator gave you all have a <code>$DOMAIN</code>, which will typically include your company name and perhaps the environment name.</p> <p>Your web portals are available at:</p> <ul> <li><code>harbor.$DOMAIN</code> -- the Harbor container image registry, which will be the home to all your container images</li> <li><code>opensearch.$DOMAIN</code> -- the OpenSearch Dashboards portal, where you will view your application and audit logs</li> <li><code>grafana.$DOMAIN</code> -- the Grafana portal, where you will view your monitoring metrics for both the platform, as such, and your application-specific metrics</li> </ul> <p>Additional endpoints are also available, depending on if your platform has these additional managed services (AMS) or not:</p> <ul> <li><code>jaeger.$DOMAIN</code> -- the Jaeger distributed tracing observability tool</li> <li><code>argocd.$DOMAIN</code> -- the Argo CD continuous Deployment GitOps tool</li> </ul>","boost":2},{"location":"user-guide/#finding-more-information","title":"Finding more information","text":"<p>If you are not familiar with Kubernetes since before, following our three-step process is a good idea, which includes a demo application for you to deploy and understand the entire process of containerizing an application and how to deploy it.</p> <ol> <li>The first step is about making necessary preparations such as installing prerequisite software on your laptop.</li> <li>The second step is about deploying your software.</li> <li>The third step is about how you continuously operate the software.</li> </ol> <p>It may be a good idea to follow along in all of these, even if you have worked with similar systems before.</p> <p>If you are familiar with similar systems, a common next step for Application Developers that are already used to Kubernetes is to read up on the guardrails that Welkin ships with. You may also wish to use the \"Go Deeper\" link in the site's navigational bar to find more information about specific topics, such as:</p> <ul> <li>how to set up log-based or metric-based alerts,</li> <li>configure long-term retention of logs, or</li> <li>how to use a user-friendly Kubernetes UI as an alternative or complement to the <code>kubectl</code> command line tool.</li> </ul>","boost":2},{"location":"user-guide/alerts/","title":"Alerts via Alertmanager","text":"<p>Welkin includes alerts via Alertmanager.</p> <p>Important</p> <p>By default, you will get some platform alerts. This may benefit you, by giving you improved \"situational awareness\". Please decide if these alerts are of interest to you or not. Feel free to silence them, as the Welkin administrator will take responsibility for them.</p> <p>Your focus should be on user alerts or application-level alerts, i.e., alerts under the control and responsibility of the Welkin user. We will focus on user alerts in this document.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#compliance-needs","title":"Compliance needs","text":"<p>Many regulations require you to have an incident management process. Alerts help you discover abnormal application behavior that need attention. This maps to ISO 27001 \u2013 Annex A.16: Information Security Incident Management.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#configuring-user-alerts","title":"Configuring user alerts","text":"<p>User alerts are configured via the Secret <code>alertmanager-kube-prometheus-stack-alertmanager</code> located in the <code>alertmanager</code> namespace. This configuration file is specified here.</p> <pre><code># retrieve the old configuration:\nkubectl get -n alertmanager secret alertmanager-kube-prometheus-stack-alertmanager -o jsonpath='{.data.alertmanager\\.yaml}' | base64 -d &gt; alertmanager.yaml\n\n# edit alertmanager.yaml as needed\n\n# patch the new configuration:\nkubectl patch -n alertmanager secret alertmanager-kube-prometheus-stack-alertmanager -p \"{\\\"data\\\":{\\\"alertmanager.yaml\\\":\\\"$(base64 -w 0 &lt; alertmanager.yaml)\\\"}}\"\n\n# mac users may need to omit -w 0 arguments to base64:\nkubectl patch -n alertmanager secret alertmanager-kube-prometheus-stack-alertmanager -p \"{\\\"data\\\":{\\\"alertmanager.yaml\\\":\\\"$(base64 &lt; alertmanager.yaml)\\\"}}\"\n</code></pre> <p>Make sure to configure and test a receiver for you alerts, e.g., Slack or OpsGenie.</p> <p>Note</p> <p>If you get an access denied error, check with your Welkin administrator.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#silencing-alerts","title":"Silencing alerts","text":"<p>Welkin comes with a lot of predefined alerts. As a user you might not find all of them relevant and would want to silence/ignore some of them. You can do this by adding new routes in the secret and set <code>receiver: 'null'</code>. Here is an example that would drop all alerts from the kube-system namespace (alerts with the label <code>namespace=kube-system</code>):</p> <pre><code>routes:\n  - receiver: \"null\"\n    matchers:\n      - namespace = kube-system\n</code></pre> <p>You can match any label in the alerts, read more about how the <code>matcher</code> configuration works in the upstream documentation.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#accessing-user-alertmanager","title":"Accessing user Alertmanager","text":"<p>If you want to access Alertmanager, for example to confirm that its configuration was picked up correctly, proceed as follows:</p> <ol> <li>Type: <code>kubectl proxy</code>.</li> <li>Open this link in your browser.</li> </ol> <p>You can configure silences in the UI, but they will not be persisted if Alertmanager is restarted. Use the secret mentioned above instead to create silences that persist.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#configuring-alerts","title":"Configuring alerts","text":"<p>Before setting up an alert, you must first collect metrics from your application by setting up either ServiceMonitors or PodMonitors. In general ServiceMonitors are recommended over PodMonitors, and it is the most common way to configure metrics collection.</p> <p>Then create a <code>PrometheusRule</code> following the examples below, or the upstream documentation, with an expression that evaluates to the condition to alert on. Prometheus will pick them up, evaluate them, and then send notifications to Alertmanager.</p> <p>The API reference for the Prometheus Operator describes how the Kubernetes resource is configured, and the configuration reference for Prometheus describes the rules themselves.</p> <p>In Welkin the Prometheus Operator in the Workload Cluster is configured to pick up all PrometheusRules, regardless in which namespace they are or which labels they have.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#running-example","title":"Running Example","text":"<p>The user demo already includes a PrometheusRule, to configure an alert:</p> <pre><code>{{- if .Values.prometheusRule.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: {{ include \"welkin-user-demo.fullname\" . }}\n  labels:\n    {{- include \"welkin-user-demo.labels\" . | nindent 4 }}\nspec:\n  groups:\n  - name: ./example.rules\n    rules:\n    - alert: ApplicationIsActuallyUsed\n      expr: rate(http_request_duration_seconds_count[1m])&gt;1\n{{- end }}\n</code></pre> <p>The screenshot below gives an example of the application alert, as seen in Alertmanager.</p> <p></p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/alerts/#detailed-example","title":"Detailed example","text":"<p>PrometheusRules have two features, either the rules alert based on an expression, or the rules <code>record</code> based on an expression. The former is the way to create alerting rules and the latter is a way to pre-compute complex queries that will be stored as separate metrics:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  labels:\n    prometheus: example\n    role: alert-rules\n  name: prometheus-example-rules\nspec:\n  groups:\n    - name: ./example.rules\n      # interval: 30s # optional parameter to configure how often groups of rules are evaluated\n      rules:\n        - alert: ExampleAlert\n          expr: vector(1)\n          # for: 1m # optional parameter to configure how long an alert must be triggered to be fired\n          labels:\n            severity: high\n          annotations:\n            summary: \"Example Alert has been fired!\"\n            description: \"The Example Alert has been fired! It shows the value {{ $value }}.\"\n        - record: example_record_metric\n          expr: vector(1)\n          labels:\n            record: example\n</code></pre> <p>For alert rules, labels and annotations can be added or overridden, which will then be included in the resulting alert notifications. Furthermore, the annotations support Go Templating, allowing access to the evaluated value via the <code>$value</code> variable, and all labels from the expression using the <code>$labels</code> variable.</p> <p>For recording rules, labels can be added or overridden, which will then be included in the resulting metric.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/backup/","title":"Backups","text":"<p>Important</p> <p>Welkin comes with a default backup retention time of 30 days, which was assessed suitable for most use-cases.</p> <p>You should be aware that some data protection regulations put a minimum requirement on backup retention time, while some data protection regulations put a maximum requirement on backup retention time.</p> <p>For example, Swedish Patient Data Laws (HSLF-FS 2016:40 3 kap. 13 \u00a7) says the following:</p> <p>The healthcare provider must decide how long the backup copies are to be saved [...].</p> <p>This can be interpreted as setting a minimum backup retention time.</p> <p>At the other end, too long backup retention time clash with GDPR Art. 17 \"Right to erasure (\u2018right to be forgotten\u2019)\". For more details, see How do I comply with GDPR Art. 17.</p> <p>Make sure you research regulations applicable to your organization to determine if the default backup retention time is suitable for your organization.</p> <p>Welkin includes backup functionality through Velero, a backup tool for Kubernetes Resources and Persistent Volumes. For backup of container images, Harbor is used instead.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/backup/#what-is-velero","title":"What is Velero?","text":"<p>Velero is an open source, cloud native tool for backing up and migrating Kubernetes Resources and Persistent Volumes. It has been developed by VMware since 2017. It allows for both manual and scheduled backups, and also allows for subsets of Resources in a Cluster to be backed up rather than necessarily backing up everything.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/backup/#usage","title":"Usage","text":"<p>The following are instructions for backing up and restoring resources.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/backup/#backing-up","title":"Backing up","text":"<p>Welkin takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label <code>compliantkubernetes.io/nobackup</code> can be added to opt-out of the daily backups.</p> <p>Application metrics (Grafana) and application log (OpenSearch) dashboards are also backed up by default.</p> <p>By default, backups are stored for 720 hours (30 days).</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/backup/#restoring","title":"Restoring","text":"<p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>To restore a backup on demand, contact your Welkin administrator.</p>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/backup/#protection-of-backups","title":"Protection of Backups","text":"<p>The Welkin administrator will take the following measure to ensure backups are protected:</p> <ol> <li> <p>Backups are encrypted at rest, if the underlying infrastructure provider supports it.</p> <p>Why? This ensures backups remain confidential, even if, e.g., hard drives are not safely disposed.</p> </li> <li> <p>Backups are replicated to an off-site location, if requested. This process is performed from outside the Cluster, hence the users -- or attackers gaining access to their application -- cannot access the off-site replicas.</p> <p>Why? This ensures backups are available even if the primary location is subject to a disaster, such as extreme weather. The backups also remain available -- though unlikely confidential -- in case an attacker manages to gain access to the Cluster.</p> </li> <li> <p>The buckets holding the backups are configured with object lock, if the underlying cloud provider supports it. This means that backups cannot be modified or erase until a given retention time, even with privileged credentials.</p> <p>Why? This ensures backups are available -- though unlikely confidential -- even if the whole Welkin environment is compromised.</p> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A5","HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","MSBFS 2020:7 4 kap. 14 \u00a7","MSBFS 2020:7 4 kap. 15 \u00a7","HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","NIS2 Minimum Requirement (c) Backup Management","ISO 27001 Annex A 8.13 Information Backup"],"boost":2},{"location":"user-guide/ci-cd/","title":"External CI/CD Integration","text":"<p>Tip</p> <p>Welkin comes with Argo CD as an Additional Service. Integration with an external CI/CD is non-trivial and time-consuming. Therefore, we only recommend to read this page if you have an existing CI/CD solution in place and determined that migrating to Argo CD is impractical.</p> <p>This page discusses integration between Welkin and external CI/CD solutions.</p> <p>Important</p> <p>Access control is an extremely important topic for passing an audit for compliance with data privacy and data security regulations. For example, Swedish patient data law requires all persons to be identified with individual credentials and that logs should capture who did what.</p> <p>Therefore, Welkin has put significant thought into how to do proper access control. As a consequence, CI/CD solutions that require cluster-wide permissions and/or introduce their own notion of access control are highly discouraged. Make sure you thoroughly evaluate your CI/CD solution with your CISO before investing in it.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#background","title":"Background","text":"<p>For the purpose of Welkin, one can distinguish between two \"styles\" of CI/CD: push-style and pull-style.</p> <p>Push-style or external CI/CD -- like GitLab CI or GitHub Actions -- means that a commit will trigger some commands on a CI/CD worker, which will push changes into the Welkin Cluster. The CI/CD worker generally runs outside the Kubernetes Cluster. Push-style CI/CD solutions should work out-of-the-box and require no special considerations for Welkin.</p> <p>Pull-styles or in-Cluster CI/CD -- like ArgoCD or Flux -- means that a special controller is installed inside the Cluster, which monitors a Git repository. When a change is detected the controller \"pulls\" changes into the Cluster from the Git repository. The special controller often requires considerable permissions and introduces a new notion of access control, which is problematic from a compliance perspective.</p> <p>Below we show how to use external CI/CD solutions.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#external-cicd","title":"External CI/CD","text":"<p>External CI/CD works pretty much as if you would access Welkin from your laptop, running <code>kubectl</code> or <code>helm</code> against the Cluster, as required to deploy your application. However, for improved access control, the <code>KUBECONFIG</code> provided to your CI/CD pipeline should employ a ServiceAccount which is used only by your CI/CD pipeline. This ServiceAccount should be bound to a Role which gets the least permissions possible. For example, if your application only consists of a Deployment, Service and Ingress, those should be the only resources available to the Role.</p> <p>To create a <code>KUBECONFIG</code> for your CI/CD pipeline, proceed as shown below.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#pre-verification","title":"Pre-verification","text":"<p>First, make sure you are in the right namespace on the right Cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>You can only create a Role which is as powerful as you (see Privilege escalation prevention). Therefore, check what permissions you have and ensure they are sufficient for your CI/CD:</p> <pre><code>kubectl auth can-i --list\n</code></pre> <p>Note</p> <p>What permissions you need depends on your application. For example, the user demo creates Deployments, HorizontalPodAutoscalers, Ingresses, PrometheusRules, Services and ServiceMonitors. If unsure, simply continue. RBAC permissions errors are fairly actionable.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#create-a-role","title":"Create a Role","text":"<p>Next, create a Role for you CI/CD pipeline. If unsure, start from the example Role that the user demo's CI/CD pipeline needs.</p> <pre><code>kubectl apply -f ci-cd-role.yaml\n</code></pre> <p>Dealing with Forbidden or RBAC permissions errors</p> <p>Error from server (Forbidden): error when creating \"STDIN\": roles.rbac.authorization.k8s.io \"ci-cd\" is forbidden: user \"demo@example.com\" (groups=[\"system:authenticated\"]) is attempting to grant RBAC permissions not currently held:</p> <p>If you get an error like the one above, then it means you have insufficient permissions on the Welkin cluster. Contact your administrator.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#create-a-serviceaccount","title":"Create a ServiceAccount","text":"<p>User accounts are for humans, service accounts for robots. See User accounts versus service accounts. Hence, you should employ a ServiceAccount for your CI/CD pipeline.</p> <p>The following command creates a ServiceAccount for your CI/CD pipeline:</p> <pre><code>kubectl create serviceaccount ci-cd\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#create-a-rolebinding","title":"Create a RoleBinding","text":"<p>Now create a RoleBinding to bind the CI/CD ServiceAccount to the Role, so as to grant it associated permissions:</p> <pre><code>NAMESPACE=$(kubectl config view --minify --output 'jsonpath={..namespace}')\nkubectl create rolebinding ci-cd --role ci-cd --serviceaccount=$NAMESPACE:ci-cd\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#create-a-secret-with-a-token","title":"Create a Secret with a token","text":"<p>Now create a secret for the ServiceAccount that Kubernetes will populate with a token:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: ci-cd\n  annotations:\n    kubernetes.io/service-account.name: ci-cd\ntype: kubernetes.io/service-account-token\nEOF\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#extract-the-kubeconfig","title":"Extract the KUBECONFIG","text":"<p>You can now extract the <code>KUBECONFIG</code> of the ServiceAccount:</p> <pre><code>SECRET_NAME=ci-cd\n\nserver=$(kubectl config view --minify --output 'jsonpath={..cluster.server}')\ncluster=$(kubectl config view --minify --output 'jsonpath={..context.cluster}')\n\nca=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.ca\\.crt}')\ntoken=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.token}' | base64 --decode)\nnamespace=$(kubectl get secret $SECRET_NAME -o jsonpath='{.data.namespace}' | base64 --decode)\n\necho \"\\\napiVersion: v1\nkind: Config\nclusters:\n- name: ${cluster}\n  cluster:\n    certificate-authority-data: ${ca}\n    server: ${server}\ncontexts:\n- name: default-context\n  context:\n    cluster: ${cluster}\n    namespace: ${namespace}\n    user: default-user\ncurrent-context: default-context\nusers:\n- name: default-user\n  user:\n    token: ${token}\n\" &gt; kubeconfig_ci_cd.yaml\n</code></pre> <p>The generated <code>kubeconfig_ci_cd.yaml</code> can then be used in your CI/CD pipeline. Note that, <code>KUBECONFIG</code>s -- especially the token -- must be treated as a secret and injected into the CI/CD pipeline via a proper secrets handing feature, such as GitLab CI's protected variable and GitHub Action's secrets.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/ci-cd/#example-github-actions","title":"Example: GitHub Actions","text":"<p>Please find a concrete example for GitHub Actions here. Below is the produced output:</p> <p></p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","NIST SP 800-171 3.5.1","NIST SP 800-171 3.5.2"],"boost":2},{"location":"user-guide/cluster-api/","title":"Cluster API","text":"<p>Note</p> <p>Our packaging of Cluster API is being rolled out and is not available in all environments. Most Elastisys Managed Service customers are using a version of this platform that is based on Kubespray. Elastisys will eventually migrate everyone to Cluster API. There is no exact schedule for this migration yet, more information will be provided later. If you have any questions then contact our support.</p> <p>This document aims to show what changes to expect as an Application Developer using Welkin when running on Cluster API instead of Kubespray.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/cluster-api/#what-is-cluster-api","title":"What is Cluster API","text":"<p>Cluster API is a project that provides declarative APIs and tools to provision, upgrade and operate Kubernetes Clusters. It works using Kubernetes objects that describe the state (Cluster) you want and operators that try to reconcile the actual state to the desired state. This means it's very similar to how one normally works with <code>Pods</code> and other resources in Kubernetes, e.g. one can use a <code>MachineDeployment</code> to provision Kubernetes Nodes (<code>Machines</code>) in a similar way that one can use <code>Deployments</code> to provision <code>Pods</code>.</p> <p>Application Developers don't need a full understanding of Cluster API. In fact, Application Developers will not interact directly with Cluster API at all. However, Welkin running with Cluster API has a few implications, which Application Developers need to be aware of. These are described below.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/cluster-api/#node-replacement","title":"Node replacement","text":"<p>Cluster API will often replace Nodes instead of modifying them as part of different operations, such as upgrading the Kubernetes version or resizing a Node. This means that you should not expect individual Kubernetes Nodes to stay in the Cluster.</p> <p>Node names and IP addresses will change as the Nodes are replaced.</p> <p>However, you can rely on the new replacement Nodes to be functionally equivalent to the old ones:</p> <ul> <li>Any attached storage to application Pods will (as per usual Kubernetes behavior) move to the new Nodes.</li> <li>Replacement Nodes will have the same size and any predefined labels.</li> </ul> <p>For Clusters that are spread out across zones you can also rely on the new Nodes being spread out across zones in the same way.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/cluster-api/#egress-traffic-source-ip","title":"Egress traffic source IP","text":"<p>Kubernetes Nodes provisioned with Cluster API will by default only have a private IP address. This means that egress traffic will use Network Address Translation (NAT) via the Infrastructure Provider, similar to how home networks behind a router work. In turn, this removes the possibility to allowlist traffic from specific Kubernetes Nodes in external services based on the source IP that the external service will see. Depending on the underlying Cloud Infrastructure Provider and their features, we cannot guarantee that the IP the external services sees is stable over time. Also depending on underlying cloud infrastructure, the IP might not be exclusive for your Welkin environment, so traffic from that IP could originate from other servers that are not related to your environment.</p> <p>Note</p> <p>Elastisys will look into the possibility of getting a stable egress IP for Application Developers that really need it.</p> <p>All Ingress traffic will go through load balancers and there will not be any major differences for the Ingress traffic.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/cluster-api/#cluster-autoscaling","title":"Cluster autoscaling","text":"<p>Warning</p> <p>For Elastisys Managed Services customers that have or want to use the Cluster autoscaling feature, please ensure your workload does not prevent the autoscaler from removing Nodes, e.g. running Pods that configure non-memory <code>emptyDir</code> volumes. Please go through the list in the Cluster autoscaling documentation.</p> <p>The Cluster Autoscaler will scale up a Cluster if there are Pods in the Pending state (refer to the Kubernetes documentation on Pod Lifecycle) that cannot currently be scheduled because of a lack of resources with the current set of Nodes. This means that the scaling is based on resource requests on Pods, not the actual current CPU/memory utilization. In turn, this means the autoscaler cannot prevent Nodes from running out of CPU or memory if the resource requests are not close to the actual usage: to benefit the most from autoscaling, you need to set resource requests as correctly as possible. The autoscaler will scale down a Cluster if there are unneeded Nodes for more than 10 minutes. A Node is unneeded if it has less than cpu and memory requests less that 50% of its capacity and all Pods running there can be moved to other Nodes (refer to this documentation for more information)</p> <p>Cluster autoscaling will not be needed for everyone and might not be available on all Infrastructure Providers, contact your Platform Administrator to see if it could be possible to enable for you.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/continous-development/","title":"Continuous Development","text":"<p>When developing on Kubernetes, it can be time-consuming to manually run commands to build new images, push images to a container registry, update Kubernetes manifests, and deploy new manifests to the Cluster with each source-code change.</p> <p>Skaffold is a tool that can be used to ease this process. Skaffold will automate the process of building, pushing and deploying new images based on changes to the source-code.</p> <p>Skaffold requires minimal setup as the tool has no Cluster-side components, and will automatically detect the configuration to use.</p>","boost":2},{"location":"user-guide/continous-development/#installing-skaffold","title":"Installing Skaffold","text":"<p>The Skaffold CLI can be installed by downloading and installing the latest release. Instructions can be found in the Skaffold documentation:</p> <pre><code>curl -Lo skaffold https://storage.googleapis.com/skaffold/releases/v2.0.1/skaffold-linux-amd64 &amp;&amp; \\\nsudo install skaffold /usr/local/bin/\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#getting-started-with-skaffold","title":"Getting started with Skaffold","text":"<p>Note</p> <p>Skaffold will use the active <code>KUBECONFIG</code> to authenticate to the Kubernetes cluster.</p> <p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#initialize-skaffold","title":"Initialize Skaffold","text":"<pre><code>skaffold init\n</code></pre> <p>This command will scan the current project for images to build and Kubernetes manifests to deploy. For each image that Skaffold finds you will be prompted to specify how to build them, or if they are not built by the current project.</p> <p>The first image Skaffold finds is <code>busybox</code>, however this image is not built from this project, so choose: <code>None (image not built from these sources)</code></p> <p></p> <p>The second image Skaffold finds is the user-demo image and this image is built from the Dockerfile.</p> <p></p> <p>Skaffold then asks for which resources we want to create Kubernetes resources, but as the image already has a Helm Chart this can be skipped by pressing enter.</p> <p>Skaffold will then create the <code>skaffold.yaml</code> file containing our configuration. Skaffold will also automatically detect the Helm Chart that deploys the <code>user-demo</code> image.</p> <p>The <code>skaffold.yaml</code> must then be configured to use the correct domain and project for the image (More info). You need push access to this repository (Optionally add a hostname to access the application).</p> <pre><code>build:\n  artifacts:\n- - image: i-didnt-read-the-docs/welkin-user-demo\n+ - image: &lt;DOMAIN&gt;/&lt;REGISTRY_PROJECT&gt;/welkin-user-demo\n    docker:\n      dockerfile: Dockerfile\n...\n  valuesFiles:\n  - deploy/welkin-user-demo/values.yaml\n  version: 0.1.0\n+ setValues:\n+   image.repository: &lt;DOMAIN&gt;/&lt;REGISTRY_PROJECT&gt;/welkin-user-demo\n+   ingress.hostname: demo.&lt;DOMAIN&gt;     # (Optional)\n</code></pre> <p>If the repository is private, a pull secret must be created to use it in Kubernetes, see Configure an Image Pull Secret.</p>","boost":2},{"location":"user-guide/continous-development/#developing","title":"Developing","text":"<p>To start developing using Skaffold run:</p> <pre><code>skaffold dev\n</code></pre> <p>When you run <code>skaffold dev</code>, Skaffold will first build, and deploy all of the artifacts specified in <code>skaffold.yaml</code>. Skaffold will then begin monitoring all source file dependencies for all artifacts specified in the project and rebuild the associated artifacts and redeploy the new changes to your Cluster as changes are made to these source files. So any changes made to the source-files will automatically be updated in the Cluster.</p> <p>When starting <code>skaffold dev</code>, the logs of the deployed artifacts will automatically be directed to the console, which makes it easy to debug the application in the Cluster.</p> <p>If <code>ingress.hostname</code> was configured previously the application can be accessed from there directly. Otherwise the flag <code>--port-forward</code> can be added to the command, and Skaffold will automatically forward the ports on the application to the local workstation:</p> <pre><code>skaffold dev --port-forward\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#application-updates","title":"Application updates","text":"<p>When the application has been built and deployed to the Cluster Skaffold shows which URL to access, shows the logs of the application, and starts listening for changes in the source-files.</p> <p></p> <p>When visiting the URL to the application or the port-forwarded URL the following output can be seen:</p> <pre><code>{ \"hostname\": \"welkin-user-demo-dd9c58979-rm9rv\", \"version\": \"0.0.1\" }\n</code></pre> <p>If you inside the <code>routes/index.js</code> file add the following:</p> <pre><code>...\nres.send({\n  hostname: os.hostname(),\n  version: process.env.npm_package_version,\n+ hello: \"world\"\n});\n...\n</code></pre> <p>And then save the file, Skaffold will automatically detect the change, build a new image, and deploy the new image to the Cluster. After the Deployment has stabilized, when visiting the same URL, the output is now:</p> <pre><code>{ \"hostname\": \"welkin-user-demo-54bbdcf6fc-gthsc\", \"version\": \"0.0.1\", \"hello\": \"world\" }\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#configuration-updates","title":"Configuration updates","text":"<p>To see the amount of Pods, run (inside another terminal):</p> <pre><code>$ kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\nwelkin-user-demo-7645db4f5c-h4xks   1/1     Running   0          45s\nwelkin-user-demo-7645db4f5c-svqfs   1/1     Running   0          35s\n</code></pre> <p>There are two Pods running. To change this, edit the file <code>deploy/welkin-user-demo/values.yaml</code>:</p> <pre><code>- replicaCount: 2\n+ replicaCount: 1\n</code></pre> <p>And save the file, this will also trigger Skaffold to update the Deployment. Because the modification only impacts the Kubernetes configuration, the application image does not need to be rebuilt, and a new Helm revision may be deployed right away.</p> <p>Once the deployments have stabilized the amount of Pods can be inspected again:</p> <pre><code>$ kubectl get pods\nNAME                              READY   STATUS    RESTARTS   AGE\nwelkin-user-demo-7645db4f5c-svqfs   1/1     Running   0          4m45s\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#clean-up","title":"Clean-up","text":"<p>To stop Skaffold <code>ctrl + c</code> can be used and will trigger Skaffold to stop listening for changes and clean-up the deployed artifacts from the Cluster.</p> <p>The clean-up can also be triggered by running:</p> <pre><code>skaffold delete\n</code></pre>","boost":2},{"location":"user-guide/continous-development/#advanced","title":"Advanced","text":"<ul> <li> <p>Skaffold supports multiple different builder, such as Dockerfile, Bazel, Buildpacks or others   (More Info).</p> </li> <li> <p>Skaffold supports copying files to the running containers which will avoid rebuilding of   containers when not needed (More Info).</p> </li> </ul>","boost":2},{"location":"user-guide/continous-development/#further-reading","title":"Further Reading","text":"<ul> <li>Skaffold Documentation</li> </ul>","boost":2},{"location":"user-guide/debug/","title":"Step 4: Debug","text":"<p>Welcome to the fourth and final step, Application Developer!</p> <p>Sometimes, your application might be end up with issue - for instance, it might be stuck, fail silently. In such cases, you may want to debug the Pod.</p>","boost":2},{"location":"user-guide/debug/#attach-a-debug-container","title":"Attach a Debug Container","text":"<p>Welkin supports Kubernetes <code>kubectl debug</code> command, which allows you to temporarily attach an ephemeral container to a running Pod for troubleshooting purposes. This container can share the Pod\u2019s process and network space and can include useful tools not available in the original container image.</p> <p>Note</p> <p>Welkin enforces security guardrails:</p> <ul> <li>The image must come from an allowed registry</li> <li>The container must run as a non-root user</li> <li>Privilege escalation is not allowed</li> </ul>","boost":2},{"location":"user-guide/debug/#example-usage","title":"Example Usage","text":"<pre><code>kubectl debug -n staging my-app-6c9f75f457-abcde \\\n  --image=harbor.com/tools/nonroot-debug:latest \\\n  --target=my-app \\\n  --share-processes \\\n  --tty --stdin\n</code></pre>","boost":2},{"location":"user-guide/debug/#next-step-going-deeper","title":"Next step? Going deeper!","text":"<p>By now, you're fully up and running! You have an application. The next step is to open the \"Go Deeper\" section of this documentation and read up on more topics that interest you.</p> <p>Thank you for starting your journey beyond the clouds with Welkin!</p>","boost":2},{"location":"user-guide/delegation/","title":"How to Delegate?","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.4.1 Information Access Restriction</li> </ul> <p>Now that you are almost ready to go live, you will certainly want to delegate some permissions to other team members or IT systems in your organization. This page shows you how to do that.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#authentication-vs-access-control","title":"Authentication vs. Access Control","text":"<p>Authentication is the act of proving your identity. Welkin is usually configured to use your organization's Identity Provider (IdP). Examples of supported IdPs include Google, Microsoft Entra ID, and Jump Cloud. The email and group provided by your IdP are used for access control in various components.</p> <p>Next sections will explain how to handle access control in each user-facing Welkin component.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#container-registry-harbor","title":"Container registry (Harbor)","text":"<p>Welkin uses Harbor as container registry. For access control, Harbor defines the concepts of:</p> <ul> <li>user and group -- for human access;</li> <li>robot account -- for IT system access.</li> </ul> <p>You don't need to create Harbor users or groups. Welkin configures Harbor in \"OIDC authentication mode\", which means that Harbor will automatically onboard users logging in via your IdP and will automatically get the group from your IdP. In contrast, you need to create robot accounts, as these only exist within Harbor.</p> <p>Your administrator will have configured one of your IdP groups as the \"Harbor system administrator\" group. Please read the upstream documentation linked below to learn how a Harbor admin can:</p> <ul> <li>manage user permissions by role and</li> <li>create robot accounts.</li> </ul> <p>Note</p> <p>You can either add users or groups to a project with various roles. To simplify access control, consider only using groups and assigning users to groups from your IdP.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#kubernetes-api","title":"Kubernetes API","text":"<p>Kubernetes uses the following concepts for access control:</p> <ul> <li>users and groups -- these are provided by your IdP;</li> <li>ServiceAccounts -- these are configured within Kubernetes and are used by IT systems;</li> <li>Roles (and ClusterRoles) -- these define a set of permissions, i.e., allowed API operations;</li> <li>RoleBindings (and ClusterRoleBindings) -- these associate Roles, i.e., a set of permissions, with users, groups or ServiceAccounts.</li> </ul> <p>For delegating permissions to ServiceAccounts, follow the example on the CI/CD page.</p> <p>The next section covers delegation to users and groups.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#pre-verification","title":"Pre-verification","text":"<p>First, make sure you are in the right namespace on the right Cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>You can only delegate as much permission as you have (see Privilege escalation prevention). Therefore, check what permissions you have:</p> <pre><code>kubectl auth can-i --list\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#create-a-role","title":"Create a Role","text":"<p>Next, create a Role capturing the set of permissions you want to delegate. If unsure, start from the example Role that the user demo's CI/CD pipeline needs.</p> <pre><code>kubectl apply -f ci-cd-role.yaml\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#delegate-to-a-group","title":"Delegate to a Group","text":"<p>Prefer delegating to a group, so that access control is centralized in your IdP.</p> <pre><code>ROLE=my-role     # Role created above\nGROUP=my-group   # As set in your IdP\n\nkubectl create rolebinding $ROLE --role $ROLE --group=$GROUP --dry-run=client -o yaml &gt; my-role-binding.yaml\n# review my-role-binding.yaml\nkubectl apply -f my-role-binding.yaml\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#add-a-kubernetes-admin","title":"Add a Kubernetes admin","text":"<p>In Welkin, Application Developers who are Kubernetes admins have the ability to add more <code>Kubernetes admins</code> themselves.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#steps","title":"Steps","text":"<ol> <li> <p>Edit the ClusterRoleBinding <code>extra-user-view</code> and add the desired users or groups under <code>subjects</code>. If unsure, look at an example subject from the official Kubernetes documentation.</p> <pre><code>kubectl edit clusterrolebinding extra-user-view\n</code></pre> </li> <li> <p>In each of your user namespaces that you want the users or groups to be admin in, edit the RoleBinding <code>extra-workload-admins</code> and add the desired users or groups under <code>subjects</code>. If you have a root HNC namespace and you want the users or groups to be admin in all of your namespaces, you only need to edit the RoleBinding in this root namespace and it will propagate.</p> <pre><code>kubectl edit rolebinding extra-workload-admins -n user-namespace\n</code></pre> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#application-metrics-grafana","title":"Application Metrics (Grafana)","text":"<p>Your administrator will have mapped your IdP groups to the Grafana viewer, editor and admin roles. Access can be limited to specific email domains. Please read the upstream documentation to learn more.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/delegation/#application-logs-opensearch-dashboards","title":"Application Logs (OpenSearch Dashboards)","text":"<p>For improved security and due to technical limitations, OpenSearch permissions can only be configured by the Platform Administrator. Contact them to change access control to application logs.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","NIST SP 800-171 3.1.2","NIST SP 800-171 3.1.4","NIST SP 800-171 3.1.5","NIST SP 800-171 3.1.6","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.15 Access Control"],"boost":2},{"location":"user-guide/demarcation/","title":"Demarcation","text":"<p>TL;DR: You cannot install:</p> <ul> <li>ClusterRoles, ClusterRoleBindings</li> <li>Roles and RoleBindings that would escalate your privileges</li> <li>CustomResourceDefinitions (CRDs)</li> <li>PodSecurityPolicies</li> <li>ValidatingWebhookConfiguration, MutatingWebhookConfiguration</li> </ul> <p>This means that generally you cannot deploy Operators.</p> <p>TL;DR: You cannot:</p> <ul> <li>Run containers as root (<code>uid=0</code>)</li> <li>SSH into any Node</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/demarcation/#can-i","title":"Can I?","text":"<p>Welkin comes with a lot of guardrails to ensure you protect your business reputation and earn the trust of your Application Developers. Furthermore, it is a good idea to keep regulators happy, since they bring public trust into digitalization. Public trust is necessary to shift Application Developers away from pen-and-paper to drive usage of your amazing application.</p> <p>If you used Kubernetes before, especially if you acted as a Platform Administrator, then being a Welkin user might feel a bit limiting. For example, you might not be able to run containers with root (<code>uid=0</code>) as you were used to. Again, these are not limitations, rather guardrails.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/demarcation/#why","title":"Why?","text":"<p>As previously reported, Kubernetes is not secure by default, nor by itself. This is due to the fact that Kubernetes prefers to keep its \"wow, it just works\" experience. This might be fine for a company that does not process personal data. However, if you are in a regulated industry, for example, because you process personal data or health information, your regulators will be extremely unhappy to learn that your platform does not conform to security best practices.</p> <p>In case of Welkin this implies a clear separation of roles and responsibilities between Welkin users and administrators. The mission of administrators is to make you, the Welkin user, succeed. Besides allowing you to develop features as fast as possible, the administrator also needs to ensure that you build on top of a platform that lives up to regulatory requirements, specifically data privacy and data security regulations.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/demarcation/#general-principle","title":"General Principle","text":"<p>Welkin does not allow users to make any changes which may compromise the security of the platform. This includes compromising or working around access control, logging, monitoring, backups, alerting, etc. For example, accidental deletion of the CustomResourceDefinitions of Prometheus would prevent administrators from getting alerts and fixing Cluster issues before your application is impacted. Similarly, accidentally deleting Fluentd Pods would make it impossible to capture the Kubernetes audit log and investigate data breaches.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/demarcation/#specifics","title":"Specifics","text":"<p>To stick to the general principles above, Welkin comes with some technical guardrails. These are implemented through Pod Security Admission enforcing that user namespaces adhere to the <code>restricted</code> Pod Security Standard, in combination with OPA Policies and RBAC. This list may be updated in the future to take into account the fast evolving risk and technological landscape.</p> <p>More technically, Welkin does not allow users to:</p> <ul> <li>change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks;</li> <li>run container images as root or mount <code>hostPath</code>s;</li> <li>mutate ClusterRoles or Roles so as to escalate privileges;</li> <li>mutate Kubernetes resources in administrator-owned namespaces, such as <code>monitoring</code> or <code>kube-system</code>;</li> <li>re-configure system Pods, such as Prometheus or Fluentd;</li> <li>access the hosts directly.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/demarcation/#but-what-if-i-really-need-to","title":"But what if I really need to?","text":"<p>Unfortunately, many application asks for more permissions than Welkin allows by default. When looking at the Kubernetes resources, the following are problematic:</p> <ul> <li>ClusterRoles, ClusterRoleBindings</li> <li>Too permissive Roles and RoleBindings</li> <li>Namespaces that do not have the <code>restricted</code> Pod Security Standard enforced</li> <li>CustomResourceDefinitions</li> <li>WebhookConfiguration</li> </ul> <p>In such a case, ask your administrator to make a risk-reward analysis. As long as they stick to the general principles, this should be fine. However, as much as they want to help, they might not be allowed to say \"yes\". Remember, administrators are there to help you focus on application development, but at the same time they have a responsibility to protect your application against security risks.</p>","tags":["BSI IT-Grundschutz APP.4.4.A3","HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","MSBFS 2020:7 4 kap. 3 \u00a7","MSBFS 2020:7 4 kap. 4 \u00a7","HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.15","NIST SP 800-171 3.13.3","NIS2 Minimum Requirement (i) Access Control","ISO 27001 Annex A 5.3 Segregation of Duties","ISO 27001 Annex A 8.2 Privileged Access Rights"],"boost":2},{"location":"user-guide/deploy/","title":"Step 2: Deploy","text":"<p>Hello again, Application Developer! In this step, we will walk you through what is needed to deploy your application on Welkin.</p>","boost":2},{"location":"user-guide/deploy/#demo-application-available","title":"Demo Application Available","text":"<p>In case you are just reading along, or do not already have a containerized application prepared, we have developed a demo application which allows you to quickly explore the benefits of Welkin.</p> <p>The provided artifacts, including Dockerfile and Helm Chart, allow you to quickly get started on your journey to become an agile organization with zero compromise on compliance with data protection regulations.</p> <p>We have versions of it for Node JS and .NET available. You will note that once built and containerized, they deploy exactly the same.</p>","boost":2},{"location":"user-guide/deploy/#push-your-container-images","title":"Push Your Container Images","text":"","boost":2},{"location":"user-guide/deploy/#configure-container-registry-credentials","title":"Configure container registry credentials","text":"<p>First, retrieve your Harbor CLI secret and configure your local Docker client.</p> <ol> <li>In your browser, type <code>harbor.$DOMAIN</code> where <code>$DOMAIN</code> is the information you retrieved from your administrator.</li> <li>Log into Harbor using Single Sign-On (SSO) via OpenID.</li> <li>In the right-top corner, click on your username, then \"User Profile\".</li> <li>Copy your CLI secret.</li> <li>Now log into the container registry: <code>docker login harbor.$DOMAIN</code>.</li> <li>You should see <code>Login Succeeded</code>.</li> </ol>","boost":2},{"location":"user-guide/deploy/#create-a-registry-project","title":"Create a registry project","text":"<p>Example</p> <p>Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root.</p> <p>If you haven't already done so, create a project called <code>demo</code> via the Harbor UI, which you have accessed in the previous step.</p>","boost":2},{"location":"user-guide/deploy/#clone-the-user-demo","title":"Clone the user demo","text":"<p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre>","boost":2},{"location":"user-guide/deploy/#build-and-push-the-image","title":"Build and push the image","text":"<pre><code>REGISTRY_PROJECT=demo  # Name of the project, created above\nTAG=v1                 # Container image tag\n\ndocker build -t harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo:$TAG .\ndocker push harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo:$TAG\n</code></pre> <p>You should see no error message. Note down the <code>sha256</code> of the image.</p>","boost":2},{"location":"user-guide/deploy/#verification","title":"Verification","text":"<ol> <li>Go to <code>harbor.$DOMAIN</code>.</li> <li>Choose the <code>demo</code> project.</li> <li>Check if the image was uploaded successfully, by comparing the tag's <code>sha256</code> with the one returned by the <code>docker push</code> command above.</li> <li>(Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed.</li> </ol>","boost":2},{"location":"user-guide/deploy/#deploy-your-application","title":"Deploy Your Application","text":"","boost":2},{"location":"user-guide/deploy/#pre-verification","title":"Pre-verification","text":"<p>Make sure you are in the right namespace on the right Cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre>","boost":2},{"location":"user-guide/deploy/#configure-an-image-pull-secret","title":"Configure an Image Pull Secret","text":"<p>To start, make sure you configure the Kubernetes Cluster with an image pull secret. Ideally, you should create a container registry Robot Account, which only has pull permissions and use its token.</p> <p>Important</p> <p>Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations.</p> <p>Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and only be allowed to pull images.</p> <pre><code>DOCKER_USER='robot$name'       # enter robot account name\nDOCKER_PASSWORD=               # enter robot secret\n</code></pre> <p>Now create a pull secret and (optionally) use it by default in the current namespace.</p> <pre><code># Create a pull secret\nkubectl create secret docker-registry pull-secret \\\n    --docker-server=harbor.$DOMAIN \\\n    --docker-username=$DOCKER_USER \\\n    --docker-password=$DOCKER_PASSWORD\n\n# Set default pull secret in current namespace\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}'\n</code></pre> <p>Note</p> <p>For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts.</p>","boost":2},{"location":"user-guide/deploy/#deploy-user-demo","title":"Deploy user demo","text":"<p>Example</p> <p>Here is an example Helm Chart to get you started.</p> <p>If you haven't done so already, clone the user demo and ensure you are in the right folder:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre> <p>Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example:</p> <pre><code>REGISTRY_PROJECT=demo\nTAG=v1\n</code></pre> <p>You are ready to deploy the application.</p> <pre><code>helm upgrade \\\n    --install \\\n    myapp \\\n    deploy/welkin-user-demo/ \\\n    --set image.repository=harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo \\\n    --set image.tag=$TAG \\\n    --set ingress.hostname=demo.$DOMAIN\n</code></pre>","boost":2},{"location":"user-guide/deploy/#verification_1","title":"Verification","text":"<p>Verify that the application was deployed successfully:</p> <pre><code>kubectl get pods\n# Wait until the status of your Pod is Running.\n</code></pre> <p>Verify that the certificate was issued successfully:</p> <pre><code>kubectl get certificate\n# Wait until your certificate shows READY True.\n</code></pre> <p>Verify that your application is online. You may use your browser or <code>curl</code>:</p> <pre><code>curl --include https://demo.$DOMAIN\n# First line should be HTTP/2 200\n</code></pre> <p>Do not expose <code>$DOMAIN</code> to your users.</p> <p>Although your administrator will set <code>*.$DOMAIN</code> to point to your applications, prefer to buy a branded domain. For example, register the domain <code>myapp.com</code> and point it via a CNAME or ALIAS record to <code>myapp.$DOMAIN</code>.</p>","boost":2},{"location":"user-guide/deploy/#view-application-logs","title":"View Application Logs","text":"<p>The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Welkin further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry.</p> <p>The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern <code>kubernetes*</code> and the filter <code>kubernetes.labels.app_kubernetes_io/instance:myapp</code>.</p> <p></p> <p>Note</p> <p>You may want to save frequently used searches as dashboards. Welkin saves and backs these up for you.</p>","boost":2},{"location":"user-guide/deploy/#next-step-operating","title":"Next step? Operating!","text":"<p>Now that you have deployed your containerized application and know how to look at its logs, what's next? Head over to the next step, where you learn how to operate and monitor it!</p>","boost":2},{"location":"user-guide/faq/","title":"Application Developer FAQ","text":"","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#why-cant-i-kubectl-run","title":"Why can't I <code>kubectl run</code>?","text":"<p>To increase security, Compliance Kubernetes does not allow by default to run containers as root. Additionally, the container image is not allowed to be pulled from a public docker hub registry and all Pods are required to be selected by some NetworkPolicy. This ensures that an active decision has been made for what network access the Pod should have and helps avoid running \"obscure things found on the internet\".</p> <p>Considering the above, you should start by pushing the container image you want to use to Harbor and make sure it doesn't run as root. See this document for how to use OIDC with docker. With that in place, you will need to create a NetworkPolicy for the Pod you want to run. Here is an example of how to create a NetworkPolicy that allows all TCP traffic (in and out) for Pods with the label <code>run: blah</code>.</p> <p>Note</p> <p>This is just an example, not a good idea! You should limit the policy to whatever your application really needs.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: networking.k8s.io/v1\nkind: NetworkPolicy\nmetadata:\n  name: blah\nspec:\n  podSelector:\n    matchLabels:\n      run: blah\n  policyTypes:\n  - Ingress\n  - Egress\n  ingress:\n  # Allow all incoming traffic\n  - {}\n  egress:\n  # Allow all outgoing traffic\n  - {}\nEOF\n</code></pre> <p>Now you are ready to run a Pod! Make sure you match the name with the label you used for the NetworkPolicy. Kubectl will automatically set the label <code>run: &lt;name-of-pod&gt;</code> when you create a Pod with <code>kubectl run &lt;name-of-pod&gt;</code>. Here is an example command (please replace the <code>$MY_HARBOR_IMAGE</code>):</p> <pre><code>kubectl run blah --rm -ti --image=$MY_HARBOR_IMAGE\n</code></pre> <p>If your image runs as root by defaults, but can handle running as another user, you may override the user by adding a flag like this to the above command:</p> <pre><code>--overrides='{ \"spec\": { \"securityContext\": \"runAsUser\": 1000, \"runAsGroup\": 1000 } }'\n</code></pre>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#how-do-i-give-access-to-a-new-application-developer-to-a-welkin-environment","title":"How do I give access to a new Application Developer to a Welkin environment?","text":"<p>Add the new user to the correct group via your Identity Provider (IdP), and Welkin will automatically pick it up.</p> <p>Feeling lost? To find out what users and groups currently have access to your Welkin environment, type:</p> <pre><code>kubectl get rolebindings.rbac.authorization.k8s.io workload-admin -o yaml\n# look at the 'subjects' field\n</code></pre> <p>If you are not using groups, contact your administrator.</p> <p>After adding a user to a group in your IdP, the group membership may not be immediately visible because of credential caching. To make the new membership visible right away, you can clear the cache. This will trigger a new login to Dex, which will fetch the correct groups. Any of the following commands will clear the credential cache for <code>kubectl</code>:</p> <pre><code># For modern kubelogin versions &gt;= 1.32.0\nkubectl oidc-login clean\n\n# For older kubelogin versions:\nrm ~/.kube/cache/oidc-login/*\n</code></pre>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#how-do-i-add-a-new-namespace","title":"How do I add a new namespace?","text":"<p>See Namespaces.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#connection-reset-by-peer-when-port-forwarding-to-postgresql","title":"\"Connection reset by peer\" when port-forwarding to PostgreSQL?","text":"<p>You may have seen this error when port-forwarding to PostgreSQL:</p> <pre><code>Forwarding from 127.0.0.1:5432 -&gt; 5432\nForwarding from [::1]:5432 -&gt; 5432\nHandling connection for 5432\nHandling connection for 5432\nportforward.go:406] an error occurred forwarding 5432 -&gt; 5432: error forwarding port 5432 to pod, uid : failed to execute portforward in network namespace \"\": read tcp4 127.0.0.1:5432-&gt;127.0.0.1:5432: read: connection reset by peer\nportforward.go:234] lost connection to pod\n</code></pre> <p>You have two options to resolve this issue:</p> <ol> <li> <p>Send a request to your administrator to disable TLS in the PostgreSQL Cluster. Although it sounds \"bad\", it does not compromise security, since;</p> <ul> <li>Traffic between kubectl and the Kubernetes API is encrypted.</li> <li>In-Cluster network is trusted.</li> </ul> </li> <li> <p>A workaround for the issue is to use an older version of <code>kubectl</code> when making this request, specifically <code>v1.21.14</code> or lower.</p> <p>To avoid always using an old <code>kubectl</code> version, you can give the binary another name when downloading the <code>v1.21.14</code> version, e.g. <code>kubectl-1.21</code>. This way your normal <code>kubectl</code> binary can be kept up to date.</p> <p>Then use that specific binary when making the port-forward request:</p> <pre><code>kubectl-1.21 -n $NAMESPACE port-forward svc/$USER_ACCESS 5432\n</code></pre> </li> </ol> <p>You can read more about this issue here.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#what-is-encrypted-at-rest","title":"What is encrypted at rest?","text":"<p>Welkin encrypts everything at rest, including Kubernetes resources, PersistentVolumeClaims, logs, metrics and backups, if the underlying Infrastructure Provider supports it.</p> <p>Get in touch with your administrator to check the status. They are responsible for performing a provider audit.</p> <p>Why does Welkin not offer encryption-at-rest at the platform level?</p> <p>TL;DR: operational scalability and to avoid security theatre.</p> <p>We are frequently asked why we don't simply do full-disk encryption at the VM level, using something like cryptsetup. Let us explain our rationale.</p> <p>The reason why people want encryption-at-rest is to add another safeguard to data confidentiality. Encryption-at-rest is a must-have for laptops, as they can easily be stolen or get lost. However, it is a nice-to-have addition for servers, which are supposed to be in a physically protected data-center, with disks being safely disposed. This is verified during a provider audit.</p> <p>At any rate, if encryption-at-rest is deployed it must: (a) actually safeguard data confidentiality; (b) without prohibitive costs in terms of administration.</p> <p>A Welkin environment may comprise as many as 10 Nodes, i.e., VMs. These Nodes need to be frequently rebooted, to ensure Operating System (OS) security patches are applied. This is especially important for Linux kernel, container runtime (Docker) and Kubernetes security patches. Thanks to the power of Kubernetes, a carefully engineered and deployed application can tolerate such reboots with zero downtime. (See the go-live checklist.)</p> <p>The challenge is how to deliver the disk encryption key to the VM when they are booting. Let us explore a few options:</p> <ul> <li> <p>Non-option 1: Store the encryption key on the VM's <code>/boot</code> disk. This is obvious security theatre. For example, if server disks are stolen, the VM's data is in the hands of the thiefs.</p> </li> <li> <p>Non-option 2: Let admins type the encryption key on the VM's console. Asking admins to do this is time-consuming, error-prone, effectivly jeopardizing uptime. Instead, Welkin recommends automatic VM reboots during application \"quiet times\", such as at night, to ensure the OS is patched without sacrificing uptime.</p> </li> <li> <p>Non-option 3: Let the VM pull the encryption key via instance metadata or instance configuration. This would imply storing the encryption key on the Infrastructure Provider. If the Infrastructure Provider doesn't have encryption-at-rest, then the encryption key is also stored unencrypted, likely on the same server as the VM is running. Hence, this quickly ends up being security theatre.</p> </li> <li> <p>Non-option 4: Let the VM pull the encryption key from an external location which features encryption-at-rest. This would imply that the VM needs some kind of credentials to authenticate to the external location. Again these credentials are stored unencrypted on the Infrastructure Provider, so we are back to non-option 3.</p> </li> </ul> <p>Okay, so what is the real option, then?</p> <p>The only real option is to rely on support from the Infrastructure Provider. The latest generation (physical) servers feature a TPM to store the disk encryption key. This can be securely release to the Linux kernel thanks to pre-boot authentication. This process is performance-neutral and fully transparent to the VMs running on top of the servers.</p> <p>And that is why Welkin encrypts everything at rest, only if the underlying Infrastructure Provider supports it.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#what-are-preview-features","title":"What are preview features?","text":"<p>Preview features are assessed to have a higher residual risk than commonly accepted by Customers. Residual risks include, but are not limited to:</p> <ul> <li>risk of downtime;</li> <li>risk of the feature becoming unavailable in the future;</li> <li>risk of data loss.</li> </ul> <p>The risks are usually due to novelty of the feature or uncertainties in the open-source ecosystem. By using Preview Features, the Customer accepts these additional risks.</p>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/faq/#why-are-my-grafana-dashboards-not-working","title":"Why are my Grafana dashboards not working?","text":"<p>Sometimes when increasing the amount of metrics emitted to Prometheus, or selecting a larger time frame for your dashboards, they might stop working and show errors similar to <code>\"Status: 504. Timeout exceeded while awaiting headers\"</code>. This is likely due to the Grafana Pods running OOM (Out Of Memory). Fetching a larger volume of data requires more memory for the Grafana Pods to temporarily store that data for processing and visualization.</p> <p>If you are facing this issue, you can:</p> <ul> <li>See if you can optimize your dashboard queries, making sure you aren't fetching big volumes of data that doesn't necessarily need to be monitored.</li> <li>Lower the time frame for the dashboard, visualizing smaller intervals of the metrics.</li> <li>Contact your Platform Administrators and get them to increase the memory limits of Grafana or if your a Managed services customer, send us a ticket to increase the limit. As Grafana doesn't live in the workload Cluster, it won\u2019t use any resources associated with your applications.</li> </ul>","tags":["HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","MSBFS 2020:7 4 kap. 7 \u00a7"],"boost":2},{"location":"user-guide/go-live/","title":"Go-live Checklist","text":"<p>The administrator set up a shiny new Welkin environment. You containerized your application, deployed it, configured a working CI/CD pipeline, configured application alerts, etc. etc. All seems fine, but somehow you feel anxious about going into production 24/7.</p> <p>To move from production anxiety to production karma, here is a checklist to go through before going live 24/7. Make sure to perform this checklist in a shared session with the administrator.</p> <ul> <li> Load testing was performed.<ul> <li>Why? This ensures that enough capacity was allocated for the environment.</li> <li>How? Set up a synthetic workload generator or replay a relevant workload. Ask the administrator to monitor your environment's capacity usage, including that related to components necessary for application logs and application metrics.</li> <li>Desired outcome: Allocated capacity is sufficient.</li> <li>Possible resolution: Ensure the application has proper resource requests and limits (see our user demo as an example).</li> </ul> </li> <li> Load testing was performed while updating the application.<ul> <li>Why? This ensures that the application can be updated without downtime.</li> <li>How? Make a trivial change to your application, e.g., add <code>Lorem ipsum</code> in the output of some API, and redeploy.</li> <li>Desired outcome: Measured downtime is acceptable.</li> <li>Possible resolutions: Make sure you have the right Deployment strategy. Prefer <code>RollingUpdate</code> over <code>Recreate</code>. Ensure other parameters of the Deployment strategy are tuned as needed.</li> </ul> </li> <li> Load testing was performed while doing a rolling reboot of Nodes:<ul> <li>Why? Node failure may cause application downtime. Said downtime can be large if it happens at night, when administrators need to wake up before they can respond. Also, administrators need some extra capacity for performing critical security updates on the base operating system of the Nodes.</li> <li>How? As above, but now ask the administrator to perform a rolling reboot of Nodes.</li> <li>Desired outcome: The measured downtime (due to Pod migration) during Node failure or drain is acceptable. Capacity is sufficient to tolerate one Node failure or drain.</li> <li>Possible resolution:<ul> <li>Ensure the application has proper resource requests and limits (see our user demo for an example).</li> <li>Ensure the application has at least two replicas (see our user demo for an example).</li> <li>Ensure the application has <code>topologySpreadConstraints</code> to ensure Pods do not end up on the same Node (see our user demo for an example).</li> </ul> </li> </ul> </li> <li> [For multi-Zone environments] Load testing was performed while failing an entire Zone:<ul> <li>Why? If a multi-Zone environment was requested, then the additional resilience must be tested. Otherwise, Zone failure may cause application downtime.</li> <li>How? As above, but now ask the administrator to fail an entire Zone.</li> <li>Desired outcome: The measured downtime (due to Pod migration) during Zone failure is acceptable. Capacity is sufficient to tolerate one Zone failure.</li> <li>Possible resolution:<ul> <li>Ensure the application has proper resource requests and limits (see our user demo for an example).</li> <li>Ensure the application has at least two replicas (see our user demo for an example).</li> <li>Ensure the application has <code>topologySpreadConstraints</code> to ensure Pods do not end up on the same Zone (see our user demo for an example).</li> </ul> </li> </ul> </li> <li> Disaster recovery testing was performed:<ul> <li>Why? This ensures that the application and platform team agreed on who backs up what, instead of ending up thinking that \"backing up this thingy\" is the other team's problem.</li> <li>How? Ask the administrator to destroy the environment and restore from off-site backups. Check if your application is back up and its data is restored as expected.</li> <li>Desired outcome: Measured recovery point and recovery time is acceptable.</li> <li>Possible resolution: Ensure you store application either in PersistentVolumes -- these are backed up by default in Welkin -- or a managed database hosted inside Welkin.</li> </ul> </li> <li> Redeployment of the application from scratch works.<ul> <li>Why? This ensures that no tribal knowledge exists and your Git repository is truly the only source of truth.</li> <li>How? Ask your administrator to \"reset\" the environment, i.e., remove all container images, remove all cached container images, remove all Kubernetes resources, etc. Redeploy your application.</li> <li>Desired outcome: Measured setup time is acceptable.</li> <li>Possible resolutions: Make sure to add all code and Kubernetes manifests to your Git repository. Make sure that relevant documentation exists.</li> </ul> </li> </ul>","tags":["HIPAA S26 - Contingency Plan - Testing and Revision Procedure - \u00a7 164.308(a)(7)(ii)(D)","ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity"],"boost":2},{"location":"user-guide/gpu/","title":"Using GPU Workload in Welkin","text":"<p>For Welkin Managed Customers</p> <p>You can order a new Environment with GPU support by filing a service ticket. Make sure to specify the need for GPU Nodes in \"Additional information or comments\". If you are unsure, get in touch with your account manager.</p> <p>As the demand for AI, machine learning, and data science workloads grows, Kubernetes provides a flexible and scalable platform to manage these applications. In this guide, we'll focus on how to use GPU in the Welkin platform.</p> <p>Note</p> <p>Not all infrastructure providers have support for GPU. Check with the platform administrator to find out if your environment has support for GPU workload.</p>","boost":2},{"location":"user-guide/gpu/#deployment","title":"Deployment","text":"<p>To use GPU resources in your Cluster, you need to create a Deployment that is using the resource <code>nvidia.com/gpu</code>. Here's an example of how to configure GPU resources for a Pod:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  name: cuda-vectoradd\nspec:\n  restartPolicy: OnFailure\n  containers:\n  - name: cuda-vectoradd\n    image: \"nvcr.io/nvidia/k8s/cuda-sample:vectoradd-cuda11.7.1-ubuntu20.04\"\n    resources:\n      limits:\n        nvidia.com/gpu: 1\n</code></pre>","boost":2},{"location":"user-guide/gpu/#managing-node-types-for-gpu-workloads","title":"Managing Node Types for GPU Workloads","text":"<p>GPU Nodes are essential for certain high-performance workloads, such as machine learning or data processing. Clusters often host a variety of workloads, some of which require GPU Nodes, while others can run on standard CPU Nodes. To optimize resource usage and prevent unnecessary costs\u2014since GPU Nodes are significantly more expensive than CPU Nodes\u2014we have introduced multiple Node types with corresponding labels and taints. This ensures workloads are deployed on appropriate Node types based on their requirements.</p>","boost":2},{"location":"user-guide/gpu/#node-labels-and-taints","title":"Node Labels and Taints","text":"<p>To enforce resource-specific scheduling, Nodes have been configured with the following labels and taints:</p> <ul> <li> <p>CPU Nodes:</p> <ul> <li>Label: <code>elastisys.io/node-group=worker</code></li> <li>Taint: None</li> <li>These Nodes serve as the default option for general workloads and require no additional configuration in Pod specifications.</li> </ul> </li> <li> <p>GPU Nodes:</p> <ul> <li>Labels:<ul> <li><code>elastisys.io/node-type=gpu</code></li> <li><code>elastisys.io/node-group=gpu-worker</code></li> </ul> </li> <li>Taint: <code>elastisys.io/node-type=gpu:NoSchedule</code></li> <li>These Nodes are reserved exclusively for workloads requiring GPU resources. To schedule Pods on GPU Nodes, specific Node affinity and toleration must be configured in the Pod definition.</li> </ul> </li> </ul> <p>By using these labels and taints, we ensure that:</p> <ol> <li>General workloads remain on CPU Nodes by default.</li> <li>Only workloads explicitly configured to use <code>GPUs</code> are scheduled on GPU Nodes.</li> </ol> <p>For Welkin Enterprise Customers</p> <p>Elastisys can help you ensure that your GPU Nodes are labeled and tainted as described in this page. This prevents workloads from unintentionally consuming GPU resources, thereby avoiding unnecessary expenses.</p>","boost":2},{"location":"user-guide/gpu/#how-to-configure-gpu-workloads","title":"How to Configure GPU Workloads","text":"<p>Application developers who need to deploy workloads on GPU Nodes must include the appropriate affinity and toleration settings in the Pod specification. Below is an example configuration:</p> <pre><code>spec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n            - key: elastisys.io/node-type\n              operator: In\n              values:\n              - gpu\n  tolerations:\n    - effect: NoSchedule\n      key: elastisys.io/node-type\n      operator: Equal\n      value: gpu\n</code></pre> <p>This configuration ensures the workload is scheduled only on GPU Nodes.</p>","boost":2},{"location":"user-guide/gpu/#advanced-example-scenario","title":"Advanced example Scenario","text":"<p>Let\u2019s imagine you have a workload that specifically requires a GPU flavor called <code>Standard_B2s</code>. You want to ensure this workload is scheduled exclusively on Nodes with this GPU type. Here\u2019s how you can achieve this:</p> <ol> <li>Ask your platform administrator, or if you're an Elastisys Managed Service Customer you can file a service ticket here and ask to add a new GPU flavor <code>Standard_B2s</code> and a label to that GPU Node like <code>node.kubernetes.io/instance-type=Standard_B2s</code></li> <li>Modify the workload manifest file to include the additional label. The updated manifest would look like this:</li> </ol> <pre><code>spec:\n  affinity:\n    nodeAffinity:\n      requiredDuringSchedulingIgnoredDuringExecution:\n        nodeSelectorTerms:\n        - matchExpressions:\n            - key: elastisys.io/node-type\n              operator: In\n              values:\n              - gpu\n            - key: node.kubernetes.io/instance-type\n              operator: In\n              values:\n              - Standard_B2s\n  tolerations:\n    - effect: NoSchedule\n      key: elastisys.io/node-type\n      operator: Equal\n      value: gpu\n</code></pre> <p>This configuration ensures that the workload is only scheduled on GPU Nodes labeled with both <code>elastisys.io/node-type=gpu</code> and <code>node.Kubernetes.io/instance-type=Standard_B2s</code>.</p> <p>Note</p> <p>If your Cluster is using the Cluster autoscaling feature and there's currently not enough resources, the autoscaler will create one for you. It might take a couple of minutes for the new Node to join the Cluster and to install all the pre-requisites.</p>","boost":2},{"location":"user-guide/gpu/#further-reading","title":"Further Reading","text":"<ul> <li>Kubernetes Schedule GPU Documentation</li> <li>Kubernetes Cluster Autoscaler Documentation</li> <li>Cluster Autoscaler FAQ</li> </ul>","boost":2},{"location":"user-guide/how-many-environments/","title":"How many environments?","text":"<p>For Welkin Managed Customers</p> <p>You can order a new Environment by filing a service ticket.</p> <p>If you have multiple Environments, and one or more have been clearly designated to be non-production Environments, Elastisys will apply major and minor updates to your non-production Environment(s) at least five working days before applying said update to your production Environment(s).</p> <p>For more information, please read ToS 3.5 Updates and Upgrades.</p> <p>Welkin recommends setting up at least two separate environments: one for non-production use (e.g. development and testing) and one for production use.</p>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#how-many-environments","title":"How Many Environments?","text":"","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#definitions","title":"Definitions","text":"<p>For the purpose of this document we use the following distinction:</p> <ul> <li>Application Deployment - One instance of a customer's application. Commonly, multiple application deployments are used in the software development life cycles, such as: local, development, integration, testing, staging, and production.</li> <li>Environment - One instance of a Welkin Deployment. One Environment is composed of two Kubernetes Clusters, the Management Cluster and Workload Cluster.</li> </ul>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#levels-of-isolation","title":"Levels of Isolation","text":"<p>Various levels of isolation between Application Deployments can be achieved while using Kubernetes:</p> <ul> <li>Labels: in which Application Deployments reside in the same Namespace and are separated \"only\" by e.g. Helm Releases or Argo CD Application labels. Network Policies can apply here, but e.g. Secrets will be accessible throughout the entire Namespace.</li> <li>Namespace isolation: in which Application Deployments share a Workload Cluster, but are separated logically using Namespaces. Network Policies work here, too, and Namespaces are a Kubernetes trust boundary when it comes to access to Secrets. Namespaces can also be used for resource quotas, for soft capacity allocation limitations.</li> <li>Node isolation: in which Application Deployments share an environment, but are deployed on different Nodes. On this level, both performance and security isolation is greater than in the previous, since the underlying virtual machines are separate. This is what Welkin does with additional services, such as PostgreSQL and RabbitMQ. Node isolation can be done both with and without Namespace isolation.</li> <li>Cluster isolation: in which Application Deployments share an Environment, but are deployed on separate Workload Clusters. This helps improve isolation in terms of access control, but does no such thing in the services that Welkin provides for logging, metrics, and image registry as they all are deployed in a shared Management Cluster.</li> <li>Separate Environments: in which Application Deployments share nothing. This level of isolation is the highest, which implies total isolation for access control, credentials, network traffic, performance, and there are no shared platform components.</li> </ul>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#relevant-regulations","title":"Relevant Regulations","text":"<p>Many regulations require strict separation between testing and production Application Deployments. In particular, production data should not be compromised, no matter what happens in testing Application Deployments.</p> <p>Similarly, some regulations -- such as Medical Devices Regulation (MDR) -- require you to take a risk-based approach to changing the tech stack. Depending on your risk assessment, this implies verifying changes in a non-production Application Deployment before going into production.</p> <p>Some regulations, such as NIS2, require that the organization takes measures related to \"security in network and information systems acquisition, development and maintenance, including vulnerability handling and disclosure\". This is commonly implemented using a concept called Security Zones. If two applications are in different Security Zones, then Cluster isolation might be required.</p>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#recommendations","title":"Recommendations","text":"<p>Taking into account the relevant regulations, Welkin recommends setting up at least two Environments:</p> <ul> <li>non-production Environment hosting Application Deployments from development up to staging;</li> <li>production Environment hosting the production Application Deployment.</li> </ul> <p>However, the exact number of Application Deployments and Environments will depend on your needs. Please use the two figures below to reason about environments, trading developer productivity and data security:</p> <p></p> <p></p>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/how-many-environments/#multi-tenancy-in-welkin","title":"Multi-tenancy in Welkin","text":"<p>Welkin does not support multi-tenancy in a way where multiple Application Teams share an environment but do not have access to each others data. Even with Cluster isolation with multiple Workload Clusters there is a shared Management Cluster within a Welkin Environment where common platform services run.</p> <p>Some services such as OpenSearch and Grafana with Thanos technically support multi-tenancy even if it is not implemented in Welkin as of yet. Harbor however, with its system administrator role, has no concept of multi-tenancy and can not be implemented without decreasing the permissions of an Application Developer in Welkin.</p> <p>This means that if multi-tenancy was implemented wherever possible, it would leave the responsibility of maintaining the trust boundary in the hands of the developers of the major application developer-facing Endpoints (i.e. Grafana, Harbor, OpenSearch, Thanos). Some of these projects are maintained by companies that may make choices to remove certain functionality they deem enterprise-only. Welkin would have to trust that they maintain the trust boundary as their software offering evolves, but this is not a given. We have already decided in ADR-0050 that Cluster isolation between the Workload and Management Cluster is necessary to achieve regulatory compliance, and in this page that multiple environments are recommended for further security between production and non-production data.</p> <p>We therefore deem that implementing software-level multi-tenancy to not be sufficient in securing necessary levels of isolation between tenants and that separate environments are needed if data isolation is required.</p>","tags":["HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","MSBFS 2020:7 3 kap. 1 \u00a7","MSBFS 2020:7 3 kap. 2 \u00a7","HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","BSI IT-Grundschutz APP.4.4.A1","BSI IT-Grundschutz APP.4.4.A15","MDR Annex VI UDI-related","NIST SP 800-171 3.4.4","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/kubernetes-api/","title":"Kubernetes API","text":"<p>The Kubernetes API is the entrypoint to managing your Kubernetes resources. Your Welkin administrator will provide you with a kubeconfig file upon onboarding, which is required to access the API.</p> <p>The following sections describe how to access the Cluster in order to manage your Kubernetes resources.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#authentication-and-access-control-in-welkin","title":"Authentication and Access Control in Welkin","text":"<p>In order to facilitate access control and audit logging, Welkin imposes a certain way to access the Kubernetes API. The kubeconfig file provides individual access to the Kubernetes API through Dex. Normally, you should authenticate using your organizations identity provider connected to Dex, but it is also possible for your administrator to configure static usernames and passwords.</p> <p>The authorization is done by the Kubernetes API based on Kubernetes role-based access controls. Your Cluster administrator will grant you permissions as part of onboarding. You have administrator access to the user workload Kubernetes Namespaces by default. In order to follow the principle of least privilege, you as a user should only have sufficient access to manage resources required by your application. User access to the Kubernetes API may need to be restricted from case to case to follow the principle of least privilege.</p> <p>Note</p> <p>Regardless of your privilege, you will not be able to see components such as Harbor and OpenSearch via the Kubernetes API. This is in order to comply with common logging policies, which requires logging to be sent to a tamper-proof environment. The tamper-proof environment needs to be separated from the production cluster.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#usage-guide","title":"Usage guide","text":"<p>This section focuses on using the kubeconfig.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#using-the-kubeconfig-file","title":"Using the kubeconfig file","text":"<p>The kubeconfig file can be used with <code>kubectl</code> by:</p> <ul> <li>Setting and exporting the <code>KUBECONFIG</code> environment variable:</li> </ul> <p></p> <ul> <li>Merging the configuration with your existing kubeconfig file, see Kubernetes documentation on merging kubeconfig files.</li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#authenticating-to-the-kubernetes-api","title":"Authenticating to the Kubernetes API","text":"<p>To authenticate to the Kubernetes API, run a <code>kubectl</code> command. The <code>oidc-login</code> plugin will launch a browser where you log in to the Cluster:</p> <p></p> <p>This page contains the authentication options provided by your administrator. Select your log in method and log in:</p> <p></p> <p>Once you have logged in through the browser, you are authenticated to the Cluster:</p> <p></p> <p>Your credentials will then be used by the Kubernetes API to make sure you are authorized. You are now logged in and can use <code>kubectl</code> to manage your Kubernetes resources!</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#running-example","title":"Running Example","text":"","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#pre-verification","title":"Pre-verification","text":"<p>Make sure you are in the right namespace on the right Cluster:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#configure-an-image-pull-secret","title":"Configure an Image Pull Secret","text":"<p>To start, make sure you configure the Kubernetes Cluster with an image pull secret. Ideally, you should create a container registry Robot Account, which only has pull permissions and use its token.</p> <p>Important</p> <p>Using your own registry credentials as an image pull secret, instead of creating a robot account, is against best practices and may violate data privacy regulations.</p> <p>Your registry credentials identify you and allow you to both push and pull images. A robot account should identify the Kubernetes cluster and only be allowed to pull images.</p> <pre><code>DOCKER_USER='robot$name'       # enter robot account name\nDOCKER_PASSWORD=               # enter robot secret\n</code></pre> <p>Now create a pull secret and (optionally) use it by default in the current namespace.</p> <pre><code># Create a pull secret\nkubectl create secret docker-registry pull-secret \\\n    --docker-server=harbor.$DOMAIN \\\n    --docker-username=$DOCKER_USER \\\n    --docker-password=$DOCKER_PASSWORD\n\n# Set default pull secret in current namespace\nkubectl patch serviceaccount default -p '{\"imagePullSecrets\": [{\"name\": \"pull-secret\"}]}'\n</code></pre> <p>Note</p> <p>For each Kubernetes namespace, you will have to create an image pull secret and configure it to be default. Aim to have a one-to-one-to-one mapping between Kubernetes namespaces, container registry projects and robot accounts.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#deploy-user-demo","title":"Deploy user demo","text":"<p>Example</p> <p>Here is an example Helm Chart to get you started.</p> <p>If you haven't done so already, clone the user demo and ensure you are in the right folder:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre> <p>Ensure you use the right registry project and image tag, i.e., those that you pushed in the previous example:</p> <pre><code>REGISTRY_PROJECT=demo\nTAG=v1\n</code></pre> <p>You are ready to deploy the application.</p> <pre><code>helm upgrade \\\n    --install \\\n    myapp \\\n    deploy/welkin-user-demo/ \\\n    --set image.repository=harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo \\\n    --set image.tag=$TAG \\\n    --set ingress.hostname=demo.$DOMAIN\n</code></pre>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#verification","title":"Verification","text":"<p>Verify that the application was deployed successfully:</p> <pre><code>kubectl get pods\n# Wait until the status of your Pod is Running.\n</code></pre> <p>Verify that the certificate was issued successfully:</p> <pre><code>kubectl get certificate\n# Wait until your certificate shows READY True.\n</code></pre> <p>Verify that your application is online. You may use your browser or <code>curl</code>:</p> <pre><code>curl --include https://demo.$DOMAIN\n# First line should be HTTP/2 200\n</code></pre> <p>Do not expose <code>$DOMAIN</code> to your users.</p> <p>Although your administrator will set <code>*.$DOMAIN</code> to point to your applications, prefer to buy a branded domain. For example, register the domain <code>myapp.com</code> and point it via a CNAME or ALIAS record to <code>myapp.$DOMAIN</code>.</p> <p>Use <code>topologySpreadConstraints</code> if you want cross-data-center resilience</p> <p>If you want your application to tolerate a whole zone (data-center) to go down, you need to add <code>topologySpreadConstraints</code> by uncommenting the relevant section in values.yaml.</p> <p>In order for this to work, your administrator must configure the Nodes with zone labels. You can verify if this was performed correctly typing <code>kubectl get nodes --show-labels</code> and checking if Nodes feature the <code>topology.kubernetes.io/zone</code> label.</p>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-api/#further-reading","title":"Further reading","text":"<ul> <li>Dex on GitHub</li> <li>oidc-login/kubelogin on GitHub</li> <li>Organizing Cluster Access Using kubeconfig Files</li> </ul>","tags":["HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","NIST SP 800-171 3.1.13","NIST SP 800-171 3.5.4","NIST SP 800-171 3.5.5","NIST SP 800-171 3.5.6"],"boost":2},{"location":"user-guide/kubernetes-ui/","title":"Kubernetes UI","text":"<p>Welkin recommends operations to be done via script -- if possible the GitOps way -- to improve repeatability and application developer productivity, as well as reduce human errors. That being said, User Interfaces (UIs) have their advantages. In particular, they have a reduced learning curve and make features more discoverable.</p> <p>Welkin does not come with a UI for the sake of resource efficiency. However, you can run a locally-installed Kubernetes UI yourself. We have tested:</p> <ul> <li>Monokle</li> <li>Kubernetes Extension for Visual Studio Code</li> </ul> <p>Both of these projects are installed locally, and have no Cluster-side component. Both use the exact same permissions as your Welkin user has. This makes it a perfectly safe and secure user interface to use and does not compromise your Cluster's stability or security posture.</p>","boost":2},{"location":"user-guide/kubernetes-ui/#note-for-macos-and-linux-users","title":"Note for macOS and Linux users","text":"<p>If you followed the Install Prerequisites steps of this documentation, you have probably installed the <code>oidc-login</code> plugin to <code>kubectl</code> via <code>krew</code>. If so, a locally-installed Kubernetes UI might not be able to find it. That makes it fail to authenticate via Dex, the OpenID Connect provider in Welkin.</p> <p>You have two options for making the <code>oidc-login</code> plugin findable by locally-installed Kubernetes UIs:</p> <ol> <li> <p>Edit <code>~/.profile</code> and add:</p> <pre><code>if [ -d \"$HOME/.krew/bin\" ] ; then\n  PATH=\"$HOME/.krew/bin:$PATH\"\nfi\n</code></pre> </li> <li> <p>Run the following command:</p> <pre><code>sudo ln -s ~/.krew/bin/kubectl-oidc_login /usr/local/bin\n</code></pre> </li> </ol>","boost":2},{"location":"user-guide/kubernetes-ui/#monokle","title":"Monokle","text":"<p>To get started with Monokle:</p> <ol> <li>Make sure you performing the above changes.</li> <li>Install Monokle as instructed in the upstream documentation.</li> <li>Perform OpenID authentication via your browser.</li> <li>Make sure to select a valid namespace, since Welkin does not allow access to the <code>default</code> namespace.</li> </ol> <p>Monokle should work out-of-the-box with Welkin.</p> <p></p>","boost":2},{"location":"user-guide/kubernetes-ui/#kubernetes-extension-for-visual-studio-code","title":"Kubernetes Extension for Visual Studio Code","text":"<p>To get started:</p> <ol> <li>Make sure you performing the above changes.</li> <li>Install the Kubernetes extension as instructed in the upstream documentation.</li> <li>Perform OpenID authentication via your browser.</li> </ol> <p>The Kubernetes extension should work out-of-the-box with Welkin.</p> <p></p>","boost":2},{"location":"user-guide/log-based-alerts/","title":"OpenSearch Alert","text":"<p>The alerting feature notifies you when data from one or more OpenSearch indices meets certain conditions. For example, you might want to notify a Slack channel if your application logs more than five HTTP 404 errors in one hour/minute, or you might want to page a developer if no new documents have been indexed in the past 20 minutes.</p> <p>Alerting features have been enabled by default in Welkin as of version 0.19.X</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/log-based-alerts/#opensearch-alert-demo","title":"OpenSearch Alert Demo","text":"<p>To use OpenSearch alerting feature, it involves two steps described below.</p> <ol> <li>Create Notifications Channel - A reusable location for the information that you want the monitor to send out after being triggered. Supported locations are Amazon Chime, Email, Slack, or custom webhook.</li> <li>Create Monitor - A job that runs on a defined schedule and queries OpenSearch indices. The results of these queries are then used as input for one or more triggers (Conditions that, if met, generate alerts).</li> </ol> <p>When you log into OpenSearch Dashboards, you will start at the home page as shown below.</p> <p></p> <p>From here click Visualize &amp; analyze to continue and you will be greeted with the options to go forward to either Dashboard or Discover. Opening the sidebar in the top left will also provide navigation to Plugins and Management, and here Alerting and Notifications can be found in the page shown below.</p> <p></p> <p>Step 1 - Create Notification Channel:</p> <p>We start with creating a notification channel, which enables sending messages directly to a designated Slack channel.</p> <ul> <li>Go to Notifications page, then Channels tab as shown below.</li> </ul> <p></p> <ul> <li> <p>Click on Create channel</p> </li> <li> <p>Fill in Channel details</p> </li> </ul> <p></p> <pre><code>- **Name** - Name of the destination - for example `user-demo-404-slack-notify`\n- **Channel type** - choose **Slack**\n- **Slack webhook URL** - Create a Slack Webhook following [Slack documentation](https://api.slack.com/messaging/webhooks). Paste the webhook URL\n</code></pre> <ul> <li>Test that the Slack integration works by clicking Send test message button and check if you receive a test message in your Slack Channel.</li> </ul> <p></p> <ul> <li>Finally, save the Notification channel by clicking Create button.</li> </ul> <p></p> <p>Next, we can proceed with creating a monitor that will use our newly created channel.</p> <p>Step 2 - Create Monitors:</p> <ul> <li>Go to Alerting page, then Monitors tab as shown below.</li> </ul> <p></p> <ul> <li> <p>Click on Create monitor button</p> </li> <li> <p>Fill in Monitor details</p> </li> </ul> <p></p> <pre><code>- **Monitor name** - Name of the monitor, for example `user-demo-404-error`\n- **Monitor type** - Select **Per query monitor** - For more information check OpenSearch documentation on [Monitor types](https://opensearch.org/docs/latest/observing-your-data/alerting/monitors/#monitor-types)\n- **Schedule** - How often to monitor, for example, to check every 1 minute, set:\n    - **Frequency** - **By interval**\n    - **Run every** - **1 Minutes**\n- **Data source**\n\n    - **Index** where your logs are stored, for instance, `kubernetes*` (per default, Welkin will store all application logs indices that match the `kubernetes*` index pattern)\n\n    - **Time field** should be set to `@timestamp`\n</code></pre> <ul> <li>Continue with Query details</li> </ul> <p></p> <pre><code>- **Metrics** - optional\n- **Time range for the last** - Time frame of data the plugin should monitor - **1 minute(s)**\n- **Data filter** - `status-code is 404`\n</code></pre> <ul> <li>Continue with Triggers details</li> </ul> <p></p> <pre><code>- **Trigger name** - Name of the trigger - **404-error occurred &gt;5 times in last 1 minute**\n- **Severity level** - Select the severity level **1(Highest)**\n- **Trigger condition** - Select the condition according to your applications - **IS ABOVE 5**\n</code></pre> <ul> <li>Continue with Actions details</li> </ul> <p></p> <pre><code>- Create an action with name, destination and customized message notification accordingly.\n- Test the action by clicking **Send test message** and check if you receive a test message in your Slack Channel.\n\n![OpenSearch Test Trigger](../img/trigger-notification-slack-test.png)\n</code></pre> <ul> <li>Finally click Create button to complete the creation of the monitor.</li> </ul> <p></p> <ul> <li>You can see the status of the monitor under Alerting &gt; Monitors &gt; user-demo-404-error as shown below.</li> </ul> <p></p> <p>Test alert notification to Slack:</p> <ul> <li>Demo application deployed and users get 404 errors many times (5 is the condition set before) as shown below.</li> </ul> <p></p> <ul> <li>We get the Slack notifications as shown below.</li> </ul> <p></p> <ul> <li>Users can view the alert status under the Alerting tab as shown below and accordingly take the required action.</li> </ul> <p></p> <ul> <li>Users can acknowledge the alerts under the Alerting tab as shown below.</li> </ul> <p></p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/log-based-alerts/#alert-state","title":"Alert state","text":"<ul> <li>Active - The alert is ongoing and unacknowledged. Alerts remain in this state until you acknowledge them, delete the trigger associated with the alert, or delete the monitor entirely.</li> <li>Acknowledged - Someone has acknowledged the alert, but not fixed the root cause.</li> <li>Completed - The alert is no longer ongoing. Alerts enter this state after the corresponding trigger evaluates to false.</li> <li>Error - An error occurred while executing the trigger\u2014usually the result of a bad trigger or destination.</li> <li>Deleted - Someone deleted the monitor or trigger associated with this alert while the alert was ongoing.</li> </ul> <p>You can find the more information about OpenSearch alerting by following the link.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/logs/","title":"Logging","text":"<p>Welkin provides the mechanism to manage your Cluster as well as the lifecycle of thousands of containerized applications deployed in the Cluster. The resources managed by Welkin are expected to be highly distributed with dynamic behaviors. A Welkin environment involves several components with Nodes that host hundreds of containers that are constantly being spun up and destroyed based on workloads.</p> <p>When dealing with a large pool of containerized applications and workloads in Welkin, it is imperative to be proactive with continuous monitoring and debugging information in order to observe what is going on in the Cluster. This information can be seen at the container, Node, or Cluster level. Logging as one of the three pillars of observability is a crucial element to manage and monitor services and infrastructure. It allows you to track debugging information at different levels of granularity.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#compliance-needs","title":"Compliance needs","text":"<p>The requirements to comply with ISO 27001 are stated in ISO 27001:2013. The annexes that mostly concerns logging are:</p> <ul> <li>Annex 12, article A.12.4.1 \"Event Logging\" and A.12.4.3 \"Administrator and Operator Logs\".</li> <li>Annex 16 which deals with incident management.</li> </ul> <p>In Welkin, OpenSearch is separate from the production workload, hence it complies with A.12.4.2 \"Protection of Log Information\". The Infrastructure Provider should ensure that the clock of Kubernetes Nodes is synchronized, hence complying with A.12.4.4 \"Clock Synchronisation\".</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#opensearch","title":"OpenSearch","text":"<p>Raw logs in Welkin are normalized, filtered, and processed by Fluentd and shipped to OpenSearch for storage and analysis.</p> <p>OpenSearch provides a powerful, easy-to-use event monitoring and alerting system, enabling you to monitor, search and visualize your data among other things. OpenSearch Dashboards is used as a visualization and analysis interface for OpenSearch for all your logs.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#visualization-using-opensearch-dashboards","title":"Visualization using OpenSearch Dashboards","text":"<p>OpenSearch Dashboards is used as a data visualization and exploration tool for log time-series and aggregate analytics. It offers powerful and easy-to-use features such as histograms, line graphs, pie charts, heat maps, and built-in geospatial support.</p> <p>When you log into OpenSearch Dashboards, you will start at the home page as shown below.</p> <p></p> <p>From here click \"Visualize &amp; analyze\" to continue and you will be greeted with the options to go forward to either Dashboard or Discover. Opening the sidebar in the top left will also provide navigation to OpenSearch Dashboards features, and here Visualize can be found in addition to the former two, as outlined in the page shown below.</p> <p></p> <p>Since we are concerned with searching logs and their visualization, we will focus on these three features indicated by the red rectangle in the figure above. If you are interested to know more about the rest please visit the official OpenSearch Dashboards documentation.</p> <p>Before we dive in further, let us discuss the type of logs ingested into OpenSearch. Logs in Welkin Cluster are filtered and indexed by Fluentd into four categories.</p> <p>Application level logs:</p> <ul> <li> <p>Kubernetes audit logs (index pattern: <code>kubeaudit*</code>) relate to Kubernetes audits to provide a security-relevant chronological set of records documenting the sequence of activities that have affected the system by individual users, administrators or other components of the system. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> <li> <p>Kubernetes logs (index patterns: <code>kubernetes*</code>) that provide insight into Welkin resources such as Nodes, Pods, Containers, Deployments and ReplicaSets. This allows you to observe the interactions between those resources and see the effects that one action has on another. Generally, logs in the Welkin ecosystem can be divided into the Cluster level (logs outputted by components such as the kubelet, the API server, the scheduler) and the application level (logs generated by Pods and containers). This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> </ul> <p>Platform level logs:</p> <ul> <li> <p>SSH authentication log (index pattern: <code>authlog*</code>) includes information about system authorization, along with user logins and the authentication mechanism that were used. Such as SSH access to the Nodes. This is mostly related to the ISO 27001 requirement A.12.4.3 \"Administrator and Operator Logs\".</p> </li> <li> <p>Other logs (index pattern: <code>other*</code>) are shipped to OpenSearch. These logs are collected from the Node's <code>journald</code> logging system.</p> </li> </ul> <p>Note</p> <p>Users can only view the logs of Kubernetes and kubeaudit. authlog and others are for Platform Administrators.</p> <p>Let us dive into it then.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#data-visualization-and-exploration","title":"Data Visualization and Exploration","text":"<p>As you can see in the figure above, data visualization and exploration in OpenSearch Dashboards has three components: Discover, Visualize and Dashboard. The following section describes each component using examples.</p> <p>Note</p> <p>These following examples were created for Open Distro for Elasticsearch and Kibana, however the user experience is the same when using OpenSearch Dashboards.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#discover","title":"Discover","text":"<p>The Discover component in OpenSearch Dashboards is used for exploring, searching and filtering logs.</p> <p>Navigate to Discover as shown previously to access the features provided by it. The figure below shows a partial view of the page that you will get under Discover.</p> <p></p> <p>As you can see in the above figure, the Kubernetes audit logs are loaded by default. If you want to explore logs from another index, please select the right index under the dropdown menu marked log index category.</p> <p>To appreciate the searching and filtering capability, let us get data for the following question:</p> <ul> <li>Get all logs that were collected for the past 20 hours in host 172.16.0.3 where the <code>responseStatus</code> reason is <code>notfound</code></li> </ul> <p>We can use different ways to find the answer for the question. Below is one possible solution.</p> <ol> <li> <p>Write <code>sourceIPs: 172.16.0.3</code> in the search text box.</p> </li> <li> <p>Click Add Filter and select <code>responseStatus.reason</code> and is under field and Operator dropdown menus respectively. Finally, enter     <code>notfound</code> under Value input box and click Save. The following figure shows the details.</p> <p></p> </li> <li> <p>Click the part that is labelled Time in the Discover figure from the beginning of this section, then enter 20 under the input box and select hours in the dropdown menu. Make sure that you are under the Relative tab. Finally, click update. The following figure shows how to set the hours. Note that the data will be automatically updated as time passes to reflect the past 20 hours from the current time.</p> <p></p> </li> </ol> <p>Once you are done, you will see a result similar to the following figure.</p> <p></p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#visualize","title":"Visualize","text":"<p>The Visualize component in OpenSearch Dashboards can be used to create different visualizations. Let us create a couple of visualizations.</p> <p>To create visualizations:</p> <ol> <li>Open the sidebar and click Visualize under OpenSearch Dashboards.</li> <li>Click the Create visualization button located on the top right side of the page.</li> <li>Select a visualization type, we will use Pie here.</li> <li>Choose an index pattern or saved search name under New Pie / Choose a source. You can utilize the search function. We will use the <code>kubernetes*</code> index here.</li> </ol> <p>By default a pie chart with the total number of logs will be provided by OpenSearch Dashboards. Let us divide the pie chart based on the number of logs contributed by each <code>namespace</code>. To do that perform the following steps:</p> <ol> <li> <p>Under Buckets click Add then Split slices. See the figure below.</p> <p></p> </li> <li> <p>Under Aggregation select Significant Terms. See the figure below.</p> <p></p> </li> <li> <p>Under Field select <code>kubernetes.namespace_name.keyword</code> and under Size input <code>10</code>. See the figure below.</p> <p></p> </li> <li> <p>Click the Update button located in the bottom right corner.</p> </li> </ol> <p>The final result will look like the following figure.</p> <p></p> <p>Please save the pie chart as we will use it later.</p> <p>Let us create a similar pie chart using <code>host</code> instead of <code>namespace</code>. The chart will look like the following figure.</p> <p></p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#dashboard","title":"Dashboard","text":"<p>The Dashboard component in OpenSearch Dashboards is used for organizing related visualizations together.</p> <p>Let us bring the two visualizations that we created above together in a single dashboard.</p> <p>To do that:</p> <ol> <li>Open the sidebar and click Dashboard under OpenSearch Dashboards.</li> <li>Click Create dashboard button located on the top right side of the page.</li> <li>Click Add an existing link located on the left side.</li> <li>Select the name of the two charts/visualizations that you created above.</li> </ol> <p>The figure below shows the dashboard generated from the above steps showing the two pie charts in a single page.</p> <p></p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#refresh-field-list-for-index-pattern","title":"Refresh field list for index pattern","text":"<p>When new fields are indexed in OpenSearch they are not immediately available for query in OpenSearch Dashboards, as the field list is created when the index pattern is created, and is not automatically updated when new fields are indexed. This is the most common reason for seeing the <code>Unindexed fields can not be searched</code> message. This unavailability can also happen to once queryable older fields, but with the reason being different e.g. conflicting mapping types caused by a race condition with dynamic mapping and applications pushing logs with different types, where one is unindexed.</p> <p>If you find fields missing from a selected index pattern, or if you are seeing the error message <code>Unindexed fields can not be searched</code>, you can try to refresh the field list for the particular index pattern by:</p> <ol> <li>Open the sidebar and click Dashboards Management under Management.</li> <li>Click Index patterns in the top left side.</li> <li>Click the index pattern you want to refresh e.g. <code>kubernetes*</code>.</li> <li>Click Refresh field list. which is the refresh icon located at the top right side of the page.</li> <li>Finally click Refresh and the fields would hopefully be queryable again.</li> </ol> <p>Note</p> <p>Refreshing the field list is a relatively low impact operation that usually only takes seconds. It will reset the non-persistent <code>popularity</code> score of fields on the Discover page.</p> <p>If this didn't help with the missing fields, you can contact your Platform Administrator for additional assistance.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#accessing-falco-and-opa-logs","title":"Accessing Falco and OPA Logs","text":"<p>To access Falco or OPA logs, go to the Discover panel and write Falco or OPA in the search text box. Make sure that the Kubernetes log index category is selected.</p> <p>The figure below shows the search result for Falco logs. </p> <p>The figure below shows the search result for OPA logs. </p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#opensearch-mappings","title":"OpenSearch Mappings","text":"<p>An index mapping specifies the structure of the data within that index, listing all the fields and their data types.</p> <p>Mappings can be created:</p> <ol> <li>Dynamically by OpenSearch</li> <li>Explicitly on index creation</li> <li>Using Templates</li> </ol> <p>For example, if you index an integer field without pre-defining the mapping, OpenSearch sets the mapping of that field as long.</p> <p>Importantly, a field can only be of one type, sending data of another type can result in a mapping conflict and data being rejected.</p> <p>In Welkin, index mappings are dynamically created from the data you send in. To set explicit mappings, reach out to your Platform Administrator. A mapping conflict occurs when you try to send data into a field that already has a mapping created but the data doesn't meet the same type (date, integer, string, etc.)</p> <p>A very short example of index mapping is displayed and commented below:</p> <pre><code>{\n  \"movies\": {                         # Index name we're looking at\n    \"mappings\": {\n      \"properties\": {\n        \"release_date\": {             # The release_date field is of\n          \"type\": \"date\"              # date format allowing for time based\n        },                            # searching, eg between 1970 and 2000\n        \"title\": {                    # The title field stores the title,\n          \"type\": \"text\",             # and is of type text,\n          \"fields\": {                 # and also a keyword\n            \"keyword\": {              # less than 256 bytes in length\n              \"type\": \"keyword\",      # the default limit is to avoid\n              \"ignore_above\": 256     # excessive disk/memory usage\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>To learn more, see Configuring OpenSearch Mappings.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#handling-mapping-conflicts","title":"Handling Mapping Conflicts","text":"<p>If you get the following error:</p> <pre><code>Mapping conflict! A field is defined as several types (string,\ninteger, etc) across the indices that match this pattern. You may still\nbe able to use these conflict fields in parts of Kibana, but they will\nbe unavailable for functions that require Kibana to know their type.\nCorrecting this issue will require re-indexing your data.\n</code></pre> <p>This means that your application has changed the type of a field in your structured logs. For example, say version A of your application logs the HTTP request path in <code>request</code>. Later, version B logs the HTTP request path in <code>request.path</code> and the HTTP verb in <code>request.verb</code>. Essentially, <code>request</code> has changed from string to dictionary.</p> <p>As a first step, review your application change management policy to reduce the chance of a log field changing type.</p> <p>Second, ask your administrator to re-index the affected indices.</p> <p>Note</p> <p>Re-indexing requires a lot of permissions, including creating and deleting indices, and changing Index templates. This may interfere with audit logs and compromise platform security. Therefore, to ensure platform security, re-indexing can only be performed by Platform Administrators.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#running-example","title":"Running Example","text":"<p>The user demo application already includes structured logging: For each HTTP request, it logs the URL, the user agent, etc. Welkin further adds the Pod name, Helm Chart name, Helm Release name, etc. to each log entry.</p> <p>The screenshot below gives an example of log entries produced by the user demo application. It was obtained by using the index pattern <code>kubernetes*</code> and the filter <code>kubernetes.labels.app_kubernetes_io/instance:myapp</code>.</p> <p></p> <p>Note</p> <p>You may want to save frequently used searches as dashboards. Welkin saves and backs these up for you.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#exporting-logs","title":"Exporting logs","text":"<p>OpenSearch has a reporting feature which facilitates creating different types of reports in PNG, PDF and CSV format. If you want an alternative to the reporting feature in OpenSearch we recommend you to use elasticsearch-dump.</p> <p>For Welkin Managed Customers</p> <p>Permissions for reporting are not added from the start and elasticsearch-dump is not enabled by default, but either one can be requested by filing a service ticket.</p> <p>Example of exporting the <code>kubernetes-*</code> index pattern to a folder <code>opensearch-dump</code>:</p> <pre><code>docker pull elasticdump/elasticsearch-dump\nmkdir opensearch-dump\n\n# OpenSearch username and password\n# This will be handed out from your Welkin administrator\nOPENSEARCH_USERNAME=\"your-username\"\nOPENSEARCH_PASSWORD=\"your-password\"\n\n# Your domain that is used for your cluster.\n# This is the same as the one you are using for your other services (grafana, harbor, etc.)\nDOMAIN=\"your-domain\"\n\ndocker run --rm -ti -v $(pwd)/opensearch-dump:/tmp elasticdump/elasticsearch-dump \\\n  --input=\"https://${OPENSEARCH_USERNAME}:${OPENSEARCH_PASSWORD}@opensearch.ops.${DOMAIN}/kubernetes-*\" \\\n  --type=data \\\n  --output=/tmp/opensearch-dump.json \\\n  --searchBody='{\"query\":{......}}'\n</code></pre> <p>For more examples and how to use the tool, read the documentation in the repository.</p>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#logs-over-http","title":"Logs over HTTP","text":"<p>Some applications might not directly output certain logs to stdout/stderr. For example, MinIO can be configured to send audit logs to an HTTP endpoint. In such cases, we recommend running your own instance of Fluentd as a sidecar container. Said sidecar container outputs received HTTP requests to stdout. This allows Welkin -- which happens to also use Fluentd -- to pick up, filter and buffer the logs.</p> <p>In the example below, we use \"Fluentd\" to refer to your instance of Fluentd. Below is an example of how to configure MinIO to send audit logs over HTTP to a Fluentd sidecar container:</p> <ol> <li> <p>Create a ConfigMap containing the following Fluentd configuration:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\n  namespace: minio\ndata:\n  fluent.conf: |\n    &lt;source&gt;\n      @type http\n    #  @log_level debug # uncomment this to get fluentd debug logs\n      port 9880\n      bind 127.0.0.1\n      &lt;parse&gt;\n        @type json\n      &lt;/parse&gt;\n    &lt;/source&gt;\n    &lt;match **&gt;\n      @type stdout\n      &lt;format&gt;\n        @type json\n      &lt;/format&gt;\n    &lt;/match&gt;\n</code></pre> </li> <li> <p>Add the following container and volume mounts to the MinIO workload to run Fluentd as a sidecar:</p> <pre><code>spec:\n  containers:\n  - name: fluentd\n    image: ghcr.io/elastisys/fluentd-forwarder:v4.7.5-ck8s1\n    imagePullPolicy: IfNotPresent\n    ports:\n    - name: http\n      containerPort: 9880\n    resources:\n    requests:\n      cpu: 10m\n      memory: 100M\n    securityContext:\n      runAsUser: 1000\n    volumeMounts:\n    - mountPath: /etc/fluent\n      name: fluentd-config\n  volumes:\n  - configMap:\n      defaultMode: 420\n      name: fluentd-config\n    name: fluentd-config\n</code></pre> </li> <li> <p>Configure the following environment variables for the MinIO container for it to send its audit logs to the Fluentd sidecar:</p> <pre><code>env:\n- name: MINIO_AUDIT_WEBHOOK_ENABLE\n  value: \"on\"\n- name: MINIO_AUDIT_WEBHOOK_ENDPOINT\n  value: http://localhost:9880/minioaudit.log\n</code></pre> </li> <li> <p>Verify by generating MinIO audit logs by running the MinIO client and then checking logs in OpenSearch:</p> <pre><code>mc admin info &lt;TARGET&gt;\n</code></pre> <p></p> </li> </ol>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#log-review-dashboard","title":"Log review dashboard","text":"<p>This dashboard can be viewed to get a quick overview of the Cluster's state.</p> <p></p> <pre><code>- number of authlog sessions = Authlog sessions\n</code></pre>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#kubernetes-audit-logs-index-pattern-kubeaudit","title":"Kubernetes audit logs (index pattern: <code>kubeaudit*</code>)","text":"<ul> <li>All API-Requests = Successful API requests</li> <li>Forbid Error = Forbidden API requests</li> <li>Client Error = Client error logs</li> <li>Server Error = Server error logs</li> </ul>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#kubernetes-logs-index-pattern-kubernetes","title":"Kubernetes logs (index pattern: <code>kubernetes*</code>)","text":"<ul> <li>error OR denied = Error &amp; denied logs</li> </ul>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#other-logs-index-pattern-other","title":"Other logs (index pattern: <code>other*</code>)","text":"<ul> <li>error OR critical OR alert OR warning = System logs of priority 1-4</li> </ul>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#authentication-logs-index-pattern-authlog","title":"Authentication logs (index pattern: <code>authlog*</code>)","text":"","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/logs/#further-reading","title":"Further Reading","text":"<ul> <li>OpenSearch</li> <li>OpenSearch Dashboards</li> <li>Fluentd</li> </ul>","tags":["NIST SP 800-171 3.1.13","NIST SP 800-171 3.12.3","ISO 27001 Annex A 8.15 Logging"],"boost":2},{"location":"user-guide/long-term-log-retention/","title":"Long-term log retention","text":"<p>Welkin by default sets an retention of 30 days for logs. Many regulators, including Swedish Healthcare, require a minimum of 5 year log retention.</p> <p>This is not provided at the platform level by Welkin as it runs the risk of GDPR non-compliance. Logs may include sensitive information like personal data, which requires that the retention scheme is designed together with application-specific knowledge to ensure compliance. Specifically, this includes that the retention scheme ensures that erased personal data can not be accidentally restored, as per Art. 17 GDPR Right to erasure (\u2018right to be forgotten\u2019).</p> <p>Using application-specific knowledge would also make it possible to reduce the amount of logs stored, by filtering out so only the required logs are kept. Minimising the kept data, storage costs and storage management.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/long-term-log-retention/#exporting-logs-for-long-term-storage","title":"Exporting logs for long-term storage","text":"<p>To enable long-term log retention we instead recommend using Elasticdump. This tool can export logs from OpenSearch within Welkin on a per document basis in either CSV or JSON format, allowing other tools to process the logs and ship them somewhere else. It can also perform transformations, compress using Gzip, and write them into a file or send them to S3 object storage.</p> <p>Using this tool, along with the REST API of OpenSearch, then it is possible to create scripts to export logs for long-term storage using a Kubernetes CronJob. Down below are some examples how to discover indices to export from OpenSearch, some commands to use with Elasticdump, and an example Dockerfile and some Kubernetes manifests.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/long-term-log-retention/#accessing-opensearch","title":"Accessing OpenSearch","text":"<p>Info</p> <p>To access OpenSearch contact your Welkin administrator and ask them to create a user with suitable permissions listed here below. For Elastisys managed services customers this can be done by filing a service ticket.</p> <p>This is the permissions required for the OpenSearch API snippets and the permissions required by Elasticdump:</p> <pre><code>cluster_permissions:\n  - cluster_monitor\n  - indices:data/read/scroll\n  - indices:data/read/scroll/clear\nindex_permissions:\n  - index_patterns:\n      - \"*\"\n    allowed_actions:\n      - indices:admin/aliases/get # Can be omitted when aliases is not used\n      - indices:monitor/*\n  - index_patterns:\n      - kubernetes*\n    allowed_actions:\n      - indices:admin/get\n      - indices:admin/mappings/get\n      - indices_monitor\n      - read\n      - search\n</code></pre> <p>With <code>${DOMAIN}</code> set to the domain of your environment, the variables then needed to connect becomes:</p> <pre><code>export OS_PROTOCOL=\"https\"\nexport OS_ENDPOINT=\"opensearch.ops.${DOMAIN}\"\nexport OS_USERNAME=\"&lt;provided-by-admin&gt;\"\nexport OS_PASSWORD=\"&lt;provided-by-admin&gt;\"\n\n# The index pattern we want to export, normally \"kubernetes*\"\nexport OS_PATTERN=\"kubernetes*\"\n</code></pre> <p>Important</p> <p>These variables will be used later on in the example snippets.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/long-term-log-retention/#discovering-aliases-and-indices","title":"Discovering aliases and indices","text":"<p>In OpenSearch logs are stored into indices. These indices are managed in a way that will limit them both in time and size, to make them more manageable. Each index typically represents a days worth of logs, but if the size of the index exceeds a set threshold a new one will be created to limit their maximum size.</p> <p>The indices are all grouped within index aliases, a sort of virtual index that behind the scenes links to other indices. This allows one to read from all indices and write to one designated write index, all using the same name.</p> <p>Since only the write index can change, one method to select indices for exporting into log-term storage is to only export the read indices. This way there is no need to check and update indices in case they've changed since the previous export run, simplifying the export logic.</p> <p>Example: List all indices using a pattern</p> <pre><code># call as: get_indices &lt;pattern&gt;\nget_indices() {\n  pattern=\"$1\"\n\n  res=\"$(curl -u \"${OS_USERNAME}:${OS_PASSWORD}\" -XGET \"${OS_PROTOCOL}://${OS_ENDPOINT}/_cat/indices?h=index\")\"\n\n  if echo \"${res}\" | grep \"error\" &gt;&amp;2; then\n    exit 1\n  elif echo \"${res}\" | grep \"fail\" &gt;&amp;2; then\n    exit 1\n  else\n    echo \"${res}\" | sed -n \"/${pattern}/p\" | sort\n  fi\n}\n</code></pre> <p>This will generate a list of all indices within OpenSearch for the specified pattern.</p> <p>The pattern accepts regex used by <code>sed</code> and should in most instances be <code>kubernetes*</code> to only include the application logs indices.</p> <p>Example: List all write indices using a pattern</p> <pre><code># call as: get_write_index &lt;pattern&gt;\nget_write_index() {\n  pattern=\"$1\"\n\n  res=\"$(curl -u \"${OS_USERNAME}:${OS_PASSWORD}\" -XGET \"${OS_PROTOCOL}://${OS_ENDPOINT}/_cat/aliases?h=alias,index,is_write_index\")\"\n\n  if echo \"${res}\" | grep \"error\" &gt;&amp;2; then\n    exit 1\n  elif echo \"${res}\" | grep \"fail\" &gt;&amp;2; then\n    exit 1\n  else\n    echo \"${res}\" | grep \"true\" | sed -n \"/${pattern}/p\" | awk '{print $2}' || true\n  fi\n}\n</code></pre> <p>This will generate a list of all write indices within OpenSearch for the specified pattern.</p> <p>The pattern accepts regex used by <code>sed</code> and should in most instances be <code>kubernetes*</code> to only include the application logs alias.</p> <p>Since the pattern should only match a single alias, and since each alias can only have a single write index, the output should be validated that it at most only contains one index.</p> <p>Using these two example functions we can now fetch the indices for a pattern and find the write index for any matching alias, meaning we can iterate over them, filter out the write index, and perform our backup:</p> <p>Example: Iterating over indices, filtering out the write index, and perform export action</p> <pre><code>indices=$(get_indices \"${OS_PATTERN}\")\nwrite_index=$(get_write_index \"${OS_PATTERN}\")\n\nfor index in ${indices}; do\n  if [ \"${index}\" = \"${write_index}\" ]; then\n    continue # skipping as it is the write index\n  fi\n\n  perform_export \"${index}\"\ndone\n</code></pre> <p>For more complex tasks checkout the OpenSearch REST API reference.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/long-term-log-retention/#exporting-indices","title":"Exporting indices","text":"<p>With Elasticdump it is possible to export logs out to the console, to file, or to S3. In these example snippets we will export them to S3. To be able to do some management functions we will also use s3cmd, most importantly to be able to check for existing exports.</p> <p>Using S3 both for Elasticdump and s3cmd will require the following variables:</p> <pre><code>export S3_BUCKET=\"&lt;bucket&gt;\"\nexport S3_REGION=\"&lt;region&gt;\"\nexport S3_ENDPOINT=\"&lt;region-endpoint&gt;\"\nexport S3_FORCE_PATH_STYLE=\"&lt;false|true&gt;\" # Generally \"false\" for AWS and Exoscale, else \"true\"\nexport AWS_ACCESS_KEY_ID=\"&lt;access-key&gt;\"\nexport AWS_SECRET_ACCESS_KEY=\"&lt;secret-key&gt;\"\n\nif [ \"$S3_FORCE_PATH_STYLE\" = \"true\" ]; then\n  export S3_BUCKET_ENDPOINT=\"${S3_ENDPOINT}\"\nelse\n  export S3_BUCKET_ENDPOINT=\"%(bucket)s.${S3_ENDPOINT}\"\nfi\n</code></pre> <p>Important</p> <p>These variables will be used later on in the example snippets.</p> <p>Example: Export entire index to S3</p> <pre><code># With ${index} set to the index to export.\nelasticdump \\\n  --input \"${OS_PROTOCOL}://${OS_USERNAME}:${OS_PASSWORD}@${OS_ENDPOINT}/${index}\" \\\n  --output \"s3://$S3_BUCKET/${index}.json.gz\" \\\n  --s3Region ${S3_REGION} \\\n  --s3Endpoint ${S3_ENDPOINT} \\\n  --s3ForcePathStyle ${S3_FORCE_PATH_STYLE} \\\n  --s3Compress \\\n  --concurrency 40 \\\n  --concurrencyInterval 1000 \\\n  --intervalCap 20 \\\n  --limit 1000\n</code></pre> <p>This process will take a while depending on the size of the index. By default it will not try to delete, replace or update any resources, so this must be enabled using the appropriate flags or it should be managed by other means like s3cmd.</p> <p>Caution</p> <p>If this process is aborted it will leave multipart uploads after itself that should be cleared, else they will still use storage on the S3 service!</p> <p>These can be listed and then removed with s3cmd: <pre><code>s3cmd \\\n  --host=${S3_ENDPOINT} \\\n  --host-bucket=${S3_BUCKET_ENDPOINT} \\\n  multipart \"s3://${S3_BUCKET}\"\n\ns3cmd \\\n  --host=${S3_ENDPOINT} \\\n  --host-bucket=${S3_BUCKET_ENDPOINT} \\\n  abortmp \"s3://${S3_BUCKET}/&lt;multipart-upload-path&gt;\" \"&lt;multipart-upload-id&gt;\"\n</code></pre></p> <p>The example above can be modified to only export certain logs, by adding a query using OpenSearch Query DSL with the <code>--searchBody '&lt;query&gt;'</code> flag. This way it is possible to filter on certain labels to only export logs for a particular namespace, Deployment, or even using identifier within structured logs. An example for a specific namespace would be:</p> <pre><code>{\n  \"query\": {\n    \"term\": {\n      \"kubernetes.namespace_name\": \"production\"\n    }\n  }\n}\n</code></pre> <p>Since the Query DSL is in JSON format it must be properly quoted or escaped to keep its format, preferably the whitespace should be stripped before sending it as an argument to Elasticdump.</p> <p>Example: Putting it all together</p> <pre><code># call as: perform_export &lt;index&gt;\nperform_export() {\n  index=\"$1\"\n\n  check=\"$(s3cmd \"--host=${S3_ENDPOINT}\" \"--host-bucket=${S3_BUCKET_ENDPOINT}\" ls \"s3://${S3_BUCKET}/${index}.json.gz\"\n  if [ -n \"${check}\" ]; then\n    return # skipping as it is already exported\n  fi\n\n  # Just as an example\n  query='{\"query\": {\"term\": {\"kubernetes.namespace_name\": \"production\"}}}'\n\n  elasticdump \\\n    --input \"${OS_PROTOCOL}://${OS_USERNAME}:${OS_PASSWORD}@${OS_ENDPOINT}/${index}\" \\\n    --output \"s3://$S3_BUCKET/${index}.json.gz\" \\\n    --s3Region ${S3_REGION} \\\n    --s3Endpoint ${S3_ENDPOINT} \\\n    --s3ForcePathStyle ${S3_FORCE_PATH_STYLE} \\\n    --s3Compress \\\n    --concurrency 40 \\\n    --concurrencyInterval 1000 \\\n    --intervalCap 20 \\\n    --limit 1000 \\\n    --searchBody \"${query}\"\n}\n</code></pre>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/long-term-log-retention/#deploying-cronjobs","title":"Deploying CronJobs","text":"<p>The simplest way to prepare this for Deployment is to build a container image including Bash, Elasticdump and s3cmd, and set up a CronJob to run this on a preferred schedule.</p> <p>Here are some examples of how to build and deploy them:</p> <p>Example: <code>Containerfile</code> / <code>Dockerfile</code></p> <pre><code>FROM docker.io/library/ubuntu:jammy\n\nARG DEBIAN_FRONTEND=noninteractive\nARG TZ=Etc/UTC\n\nRUN apt update &amp;&amp; \\\napt install -y --no-install-recommends ca-certificates curl npm s3cmd &amp;&amp; \\\napt clean -y &amp;&amp; \\\nrm -rf /var/lib/apt\n\nRUN npm install elasticdump@v6.88.0 -g\n\nCMD [\"elasticdump\"]\n</code></pre> <p>Example: Kubernetes resources</p> <pre><code>---\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: export-script\ndata:\n  script.sh: |-\n    &lt;bash-script&gt;\n---\napiVersion: v1\nkind: Secret\nmetadata:\n  name: export-secret\ntype: Opaque\nstringData:\n  # OpenSearch\n  OS_PROTOCOL: \"${OS_PROTOCOL}\"\n  OS_ENDPOINT: \"${OS_ENDPOINT}\"\n  OS_USERNAME: \"${OS_USERNAME}\"\n  OS_PASSWORD: \"${OS_PASSWORD}\"\n  OS_PATTERN: \"${OS_PATTERN}\"\n  # S3\n  S3_BUCKET: \"${S3_BUCKET}\"\n  S3_REGION: \"${S3_REGION}\"\n  S3_ENDPOINT: \"${S3_ENDPOINT}\"\n  S3_BUCKET_ENDPOINT: \"${S3_BUCKET_ENDPOINT}\"\n  S3_FORCE_PATH_STYLE: \"${S3_FORCE_PATH_STYLE}\"\n  AWS_ACCESS_KEY_ID: \"${AWS_ACCESS_KEY_ID}\"\n  AWS_SECRET_ACCESS_KEY: \"${AWS_SECRET_ACCESS_KEY}\"\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: export\nspec:\n  schedule: &lt;schedule&gt; # example \"@daily\"\n  concurrencyPolicy: Forbid\n  jobTemplate:\n    spec:\n      template:\n        metadata:\n          labels:\n            app: log-export\n        spec:\n          automountServiceAccountToken: false\n          restartPolicy: Never\n          containers:\n            - name: export\n              image: &lt;image&gt;\n              command:\n                - /scripts/script.sh\n              envFrom:\n                - secretRef:\n                    name: export-secret\n              resources:\n                requests:\n                  cpu: 500m\n                  memory: 500Mi\n                limits:\n                  cpu: 1000m\n                  memory: 750Mi\n              securityContext:\n                runAsNonRoot: true\n                capabilities:\n                  drop:\n                    - ALL\n              volumeMounts:\n                - name: script\n                  mountPath: /scripts\n                  readOnly: true\n          securityContext:\n            runAsNonRoot: true\n            runAsGroup: 65534\n            runAsUser: 65534\n            fsGroup: 65534\n          volumes:\n            - name: script\n              configMap:\n                name: export-script\n                defaultMode: 0777\n</code></pre> <p>For more information about CronJobs checkout the Kubernetes documentation and reference about the subject.</p>","tags":["HIPAA S48 - Audit Controls - \u00a7 164.312(b)","GDPR Art. 17 Right to erasure (\"right to be forgotten\")","HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter"],"boost":2},{"location":"user-guide/maintenance/","title":"What to expect from maintenance","text":"","boost":2},{"location":"user-guide/maintenance/#different-kinds-of-maintenance","title":"Different kinds of maintenance","text":"<ul> <li>Patching the underlying OS on the Nodes</li> <li>Upgrading the Welkin application stack</li> <li>Upgrading Kubernetes</li> </ul>","boost":2},{"location":"user-guide/maintenance/#what-impact-could-these-kinds-of-maintenance-have-on-your-application","title":"What impact could these kinds of maintenance have on your application?","text":"<p>Let's go through them one by one.</p>","boost":2},{"location":"user-guide/maintenance/#patching-the-os-on-the-nodes","title":"Patching the OS on the Nodes","text":"<p>Some service disruption is expected here, the Nodes need to reboot in order to install the OS upgrades/security patches. This should be done automatically by Kured in almost all cases going forward, luckily Kured can be scheduled to perform these upgrades during night-time or whenever application traffic is expected to be low. Thanks to Kured these upgrades are not usually a problem.</p>","boost":2},{"location":"user-guide/maintenance/#upgrading-the-welkin-application-stack","title":"Upgrading the Welkin application stack","text":"<p>There is barely any downtime expected from upgrading the base Welkin application stack. This is because most of the components being upgraded are not intertwined with your application, the only exception being NGINX Ingress Controller, which is not commonly upgraded.</p> <p>If you have any other managed services from us such as PostgreSQL, Redis, RabbitMQ or TimescaleDB, these services might be upgraded during the application maintenance windows. Upgrading these services can cause some short service disruptions and make them temporarily unreachable for your application.</p>","boost":2},{"location":"user-guide/maintenance/#upgrading-kubernetes","title":"Upgrading Kubernetes","text":"<p>The most impactful type of maintenance is the Kubernetes upgrade, which requires Nodes to be drained while they are being upgraded. We do these upgrades in scheduled maintenance windows during office hours. The way we handle the upgrades is that we drain and upgrade all Nodes, one at the time.</p> <p>If parts of your application is running on just one Node, then service disruptions are to be expected, and parts of the application may become unreachable for short periods during the maintenance window.</p> <p>The worst case would be if the Nodes were almost out of available resources, then the Pods may not be schedulable on another Node while getting evicted. This would mean that the Pods running on that Node would need to wait for its Node to be ready again before it can be scheduled, which could result in minutes of downtime.</p> <p>Note that this is not just a problem for Welkin, the same process would need to be followed when upgrading a \"vanilla\" Kubernetes Cluster.</p> <p>To minimize the impact on the application you should use two or more replicas for your application, set up topologySpreadConstraints to make sure that the replicas do not get scheduled on the same Node and if the impact should be minimized even more then you can also add PodDisruptionBudgets(PDBs) to make sure Kubernetes knows to not continue draining Nodes which has applications using PDBs in them until those applications are sufficiently running, though keep in mind that PDBs must be working within the criteria of this guardrail to allow Kubernetes maintenance to go through.</p>","boost":2},{"location":"user-guide/metrics/","title":"Metrics","text":"<p>This guide gives an introduction to Prometheus and Grafana and where they fit in Welkin, in terms of reducing the compliance burden.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#why-prometheus-and-grafana","title":"Why Prometheus and Grafana?","text":"<p>Prometheus is an open-source solution for monitoring and alerting. It works by collecting and processing metrics from the various services in the Cluster. It is widely used, stable, and a CNCF member. It is relatively easy to write ServiceMonitors for any custom services to get monitoring data from them into Prometheus.</p> <p>Grafana is the most widely used technology for visualization of metrics and analytics. It supports a multitude of data sources and it is easy to create custom dashboards. Grafana is created by Grafana Labs, a CNCF Silver Member.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#compliance-needs","title":"Compliance needs","text":"<p>The requirements to comply with ISO 27001 are stated in ISO 27001:2013 The annexes that mostly concerns monitoring and alerting are Annex 12, article A.12.1.3 \"capacity management\", and Annex 16 which deals with incident management.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#capacity-management","title":"Capacity management","text":"<p>Article A.12.1.3 states that \"The use of resources must be monitored, tuned and projections made of future capacity requirements to ensure the required system performance to meet the business objectives.\"</p> <ul> <li>Prometheus and Grafana helps with this as the resource usage, such as storage capacity, CPU, and network usage can be monitored. Using visualization in Grafana, projections can be made as to future capacity requirements.</li> </ul> <p>The article goes on to say that \"Capacity management also needs to be: Pro-active \u2013 for example, using capacity considerations as part of change management; Re-active \u2013 e.g. triggers and alerts for when capacity usage is reaching a critical point so that timely increases, temporary or permanent can be made.\"</p> <ul> <li>Prometheus has a rich alerting functionality, allowing you to set up alerts to warn if, for example, thresholds are exceeded or performance is degraded.</li> </ul>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#incident-management","title":"Incident management","text":"<p>Annex A.16.1 is about management of information security incidents, events and weaknesses. The objective in this Annex A area is to ensure a consistent and effective approach to the lifecycle of incidents, events and weaknesses. Incidents needs to be tracked, reported, and lessons learned from them to improve processes and reduce the possibility of similar incidents occurring in the future.</p> <p>Prometheus and Grafana can help with this by making it easier to:</p> <ul> <li>collect evidence as soon as possible after the occurrence.</li> <li>conduct an information security forensics analysis.</li> <li>communicate the existence of the information security incident or any relevant details to the leadership.</li> </ul>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#prometheus-and-grafana-in-welkin","title":"Prometheus and Grafana in Welkin","text":"","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#prometheus","title":"Prometheus","text":"<p>Welkin installs the prometheus-operator by default. The Prometheus Operator for Kubernetes provides easy monitoring definitions for Kubernetes services and Deployment and management of Prometheus instances as it can create/configure/manage Prometheus Clusters atop Kubernetes. The following CRDs are installed by default.</p> crd apigroup kind can be used by users alertmanagers monitoring.coreos.com Alertmanager NO podmonitors monitoring.coreos.com PodMonitor YES prometheuses monitoring.coreos.com Prometheus NO prometheusrules monitoring.coreos.com PrometheusRule YES servicemonitors monitoring.coreos.com ServiceMonitor YES thanosrulers monitoring.coreos.com ThanosRuler NO","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#accessing-the-prometheus-ui","title":"Accessing the Prometheus UI","text":"<p>If you want to access the web interface of Prometheus, proceed as follows:</p> <ol> <li>Type: <code>kubectl proxy</code></li> <li>Open this link in your browser</li> </ol>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#accessing-the-prometheus-api","title":"Accessing the Prometheus API","text":"<p>Note</p> <p>This is an optional feature that is disabled by default to reduce the attack surface and improve security. You can contact your Platform Administrator to enable it.</p> <p>There is a feature to grant certain Pods access to the Prometheus API, e.g. for the purpose of setting up your own Prometheus instance with remote read or federation against the Prometheus instance that is part of Welkin. This feature is disabled by default, but can be enabled by your Platform Administrator.</p> <p>In order to use this feature, you will need you to provide a list of allowed namespaces, in addition to you labeling any Pod within those namespaces that should have access with <code>elastisys.io/prometheus-access: allow</code>.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#grafana","title":"Grafana","text":"<p>Grafana can be accessed at the endpoint provided by the Welkin install scripts. If you have configured Dex you can login with a connected account, which can be limited to specific email domains.</p> <p>Welkin deploys Grafana with a selection of dashboards by default. Dashboards are accessed by clicking the Dashboard icon (four squares) at the left-hand side of the Grafana window and selecting Browse. Some examples of useful dashboards are listed below.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#node-health","title":"Node health","text":"<p>The Nodes dashboard (Node Exporter / Nodes) gives a quick overview of the status (health) of a Node in the Cluster. By selecting an instance in the \"instance\" dropdown metrics for CPU, Load, Memory, Disk and Network I/O is showed for that Node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.</p> <p></p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#pod-health","title":"Pod health","text":"<p>The Pods dashboard (Kubernetes/Compute resources/Pods) gives a quick overview of the status (health) of a Pod in the Cluster. By selecting a Pod in the \"Pod\" dropdown metrics for CPU, Memory, and Network I/O is showed for that Node. The time frame can be changed either by using the time dropdown or selecting directly in the graphs.</p> <p></p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#collecting-metrics","title":"Collecting metrics","text":"<p>Configuring Prometheus to collect metrics from an application requires either a ServiceMonitor or a PodMonitor, targeting a Kubernetes Service or Pod respectively. They are both described upstream in the API reference for Prometheus Operator. In general ServiceMonitors are recommended over PodMonitors, and it is the most common way to configure metrics collection.</p> <p>In Welkin the Prometheus Operator in the Workload Cluster is configured to pick up all ServiceMonitors and PodMonitors, regardless in which namespace they are or which labels they have.</p> <p>The default scrape interval is 30 seconds, this is how often Prometheus will gather metrics from the Pods. The default scrape timeout is 10 seconds, this is how long Prometheus will wait after starting a scrape before it considers the scrape to have failed. These can be configured in your ServiceMonitor or PodMonitor, but we do recommend you to keep the default.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#running-example","title":"Running Example","text":"<p>The user demo already includes a ServiceMonitor, as required for Welkin to collect metrics from its <code>/metrics</code> endpoint:</p> <pre><code>{{- if .Values.serviceMonitor.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: {{ include \"welkin-user-demo.fullname\" . }}\n  labels:\n    {{- include \"welkin-user-demo.labels\" . | nindent 4 }}\nspec:\n  selector:\n    matchLabels:\n    {{- include \"welkin-user-demo.selectorLabels\" . | nindent 6 }}\n  endpoints:\n  - port: http\n{{- end }}\n</code></pre> <p>The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query <code>rate(http_request_duration_seconds_count[1m])</code>. It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the <code>/users</code> endpoint is getting more traffic than the other endpoints.</p> <p></p> <p>The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels.</p> <p>Note</p> <p>You may want to save frequently used Dashboards. Welkin saves and backs these up for you.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#troubleshooting-metrics-collection","title":"Troubleshooting metrics collection","text":"<p>It is possible to see if monitors are picked up by accessing the Prometheus web interface. Navigating to <code>\"Status\" &gt; \"Service Discovery\"</code> will show all monitors picked by Prometheus, and to the right of each listed monitor there is a <code>x/y</code> indicating how many targets of those monitors are active. Active targets are actively scraped by Prometheus and inactive targets are those that fail to match the selectors of the monitor.</p> <p>Monitors can be expanded further down to list and inspect its targets, within each one the \"Discovered Labels\" column will list information about the object in Kubernetes, and in the \"Target Labels\" it will show the labels recorded from the target.</p> <p>If the \"Target Labels\" is \"Dropped\" for a target then it means that it has been excluded from scraping since it doesn't match the monitor. There are three key things to check to make sure a target is properly picked up by a monitor:</p> <ol> <li> <p>Make sure either that the monitor is in the same namespace as the target, or that the monitor has the correct namespace selector for the target:</p> <pre><code>apiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  ...\nspec:\n  ...\n+  namespaceSelector:\n+    matchNames:\n+      - &lt;namespace&gt;\n  ...\n</code></pre> </li> <li> <p>Make sure that the selector of the monitor matches the target:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  ...\n+  labels:\n+    app: target\n  ...\nspec:\n  ...\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  ...\nspec:\n  ...\n+  selector:\n+    matchLabels:\n+      app: target\n  ...\n</code></pre> </li> <li> <p>Make sure that the port of the monitor matches the target:</p> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  ...\nspec:\n  ...\n+  ports:\n+    - name: target\n+      port: 9000\n  ...\n---\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  ...\nspec:\n  ...\n+  endpoints:\n    # either\n+    - port: target\n    # or\n+    - port: 9000\n  ...\n</code></pre> </li> </ol> <p>The same concept applies to PodMonitors and Pods.</p> <p>Then when the targets are active it is possible to see scrape information by navigating to <code>\"Status\" &gt; \"Target health\"</code>. Here Prometheus gives information about the time, status, and duration for scrapes.</p> <p>Note</p> <p>The Prometheus UI was changed in Welkin v0.43, the <code>target</code> page was previously <code>\"Status\" &gt; \"Targets\"</code>.</p>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#grafana-dashboards-as-code","title":"Grafana Dashboards as Code","text":"<p>You can manage your Grafana Dashboards in a GitOps-fashion using the Terraform Provider for Grafana.</p> <p>Proceed as follows:</p> <ol> <li>Log in to Grafana.</li> <li>Create an API key, setting it to Editor or Admin.</li> <li>Provide the API key to Terraform either via the <code>GRAFANA_AUTH</code> environment variable or the auth provider variable.</li> <li>Provide the URL of Grafana (<code>https://grafana.$DOMAIN</code>) to Terraform either via the <code>GRAFANA_URL</code> environment variable or the <code>url</code> provider variable.</li> </ol>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/metrics/#further-reading","title":"Further reading","text":"<ul> <li>Prometheus</li> <li>Grafana</li> <li>Terraform Provider for Grafana</li> </ul>","tags":["NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/namespaces/","title":"Namespaces","text":"","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/namespaces/#hnc","title":"HNC","text":"<p>Hierarchical Namespace Controller (HNC) is included in Welkin. It allows the Application Developer to manage namespaces as subnamespaces and delegates access automatically. From the perspective of Kubernetes these are regular namespaces, but these can be modified via a namespaced resource by the user. Building a good namespace structure will enable you to apply namespace-scoped RBAC resources to multiple namespaces at once.</p>","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/namespaces/#namespace-management","title":"Namespace Management","text":"<p>Creating a subnamespace:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: &lt;descendant-namespace&gt;\n  namespace: &lt;parent-namespace&gt;\nEOF\n</code></pre> <p>Verify that it gets created:</p> <pre><code>kubectl get ns &lt;descendant-namespace&gt;\n</code></pre> <p>Verify that it gets configured:</p> <pre><code>$ kubectl get subns -n &lt;parent-namespace&gt; &lt;descendant-namespace&gt; -o yaml\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  ...\n  name: &lt;descendant-namespace&gt;\n  namespace: &lt;parent-namespace&gt;\n...\nstatus:\n  status: Ok\n</code></pre> <p>If the status is <code>Ok</code> then the subnamespace is ready to go.</p> <p>If you decide a subnamespace is no longer needed, then you can't delete it using <code>kubectl delete namespace &lt;descendant-namespace&gt;</code>. As you will get the following error:</p> <p>Error from server (Forbidden): namespaces <code>&lt;descendant-namespace&gt;</code> is forbidden: User <code>&lt;your user&gt;</code> cannot delete resource \"namespaces\" in API group \"\" in the namespace <code>&lt;descendant-namespace&gt;</code>: RBAC: [clusterrole.rbac.authorization.k8s.io \"user-crds\" not found, clusterrole.rbac.authorization.k8s.io \"user-crds-resourcename-limit\" not found]</p> <p>Instead you will have to delete it using either of these commands:</p> <pre><code>kubectl delete subns -n &lt;parent-namespace&gt; &lt;descendant-namespace&gt;\n</code></pre>","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/namespaces/#resource-propagation","title":"Resource Propagation","text":"<p>When a subnamespace is created all <code>Roles</code>, <code>RoleBindings</code> and <code>NetworkPolicies</code> will propagate from the parent namespace to the descendant namespace to ensure that correct access is set. This is what lets you apply namespace-scoped RBAC resources to multiple namespaces at once. Propagated copies cannot be modified, these types of resources cannot be created in a parent namespace if it conflicts with a resource in a descendant namespace. To put an exception, annotate the <code>Role</code>, <code>RoleBinding</code> or <code>NetworkPolicy</code> with <code>propagate.hnc.x-k8s.io/none: \"true\"</code> to prevent it from being propagated at all. Another option is to only propagate to selected descendant namespaces use <code>propagate.hnc.x-k8s.io/treeSelect: ...</code>, include descendant namespaces with <code>&lt;descendant-namespace&gt;</code> or exclude namespaces with <code>!&lt;descendant-namespace&gt;</code>.</p>","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/namespaces/#opt-in-propagation","title":"Opt-in Propagation","text":"<p>HNC has the option to enable opt-in propagation for additional resources such as <code>Secrets</code>. This allows you to specify additional resources that you want propagated, but only if the object has a valid selector annotation set, while ignoring others. If you want to enable this feature, you can file a service ticket or contact your Platform Administrator with a list of resources that you want it enabled for.</p>","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/namespaces/#further-reading","title":"Further Reading","text":"<ul> <li>HNC User Documentation</li> <li>Introducing HNC</li> </ul>","tags":["ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments"],"boost":2},{"location":"user-guide/network-model/","title":"Network Model","text":"<p>Note</p> <p>This is just a model to help application developers get their application running securely. Under the hood, things are a lot more complicated. See Further reading for resources giving more descriptions under-the-hood.</p> <p></p> <p>The diagram above present a useful model when reasoning about networking in Welkin.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#private-network","title":"Private Network","text":"<p>Your application Pods, as well as Pods of additional services, can communicate on a secure private network, via RFC1918 private IP addresses. It is analogous to a VPC in VM-based workloads.</p> <p>In Welkin, it is the responsibility of the administrator to ensure the in-Cluster private network is secure and trusted, either by performing an infrastructure audit or deploying Pod-to-Pod encryption.</p> <p>You should use NetworkPolicies to segregate your Pods. This improves your security posture by reducing the blast radius in case parts of your application are under attack.</p> <p>Example</p> <p>Feel free to take inspiration from the user demo.</p> <p>More example recipes for Kubernetes Network Policies that you can just copy paste can be found here.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#private-dns","title":"Private DNS","text":"<p>The private network also features a private DNS. A Service <code>my-svc</code> in the namespace <code>my-namespace</code> can be accessed from within the Kubernetes Cluster as <code>my-svc.my-namespace</code>.</p> <p>IP addresses of Pods are not stable. For example, the rollout of a new container image creates new Pods, which will have new IP addresses. Therefore, you should always use private DNS names of Services to connect your application Pods, as well as to connect your application to additional services.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#ingress","title":"Ingress","text":"<p>Your End Users should never ever access the private network directly. Instead external access is enabled by creating Ingress objects. Welkin already comes with cert-manager and is already configured with a ClusterIssuer. A secure ACME protocol is used to issue and rotate certificates using the Let's Encrypt public service.</p> <p>Assuming you configured a Service and a Deployment for you application, making End Users access your application involves two steps:</p> <ol> <li>Create the right DNS CNAME record.</li> <li>Create the right Ingress resource.</li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#running-example","title":"Running Example","text":"<p>Let us assume you want to host your application behind the nicely branded domain <code>demo.example.com</code>. Proceed as follows:</p> <p>For step 1, create a DNS CNAME as follows:</p> <pre><code>demo.example.com. 900 CNAME app.$DOMAIN.\n</code></pre> <p>where <code>$DOMAIN</code> is the environment-specific variable you received from the administrator. The line above is presented in DNS Zone file format and is widely accepted by DNS providers.</p> <p>After configuration, make sure the DNS record is properly configured and propagated, by typing:</p> <pre><code>host -a demo.example.com.\n</code></pre> <p>Important</p> <p>In the above examples, the domain name is fully qualified, i.e., it ends with a dot. Make sure your DNS provider does not mis-interpret it as a relative domain name. Otherwise, you risk creating a DNS record like <code>demo.example.com.example.com</code> which is rarely what you want.</p> <p>Important</p> <p>Be cautious when using CNAMEs and apex domains (e.g., <code>example.com</code>). See here for a long discussion of potential problems and current workarounds.</p> <p>For step 2, create an Ingress object with the right <code>metadata.annotations</code> and <code>spec.tls</code>, as exemplified below:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp-welkin-user-demo\n  annotations:\n    # To list your current ClusterIssuers, simply use 'kubectl get ClusterIssuers'.\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n    ## Uncomment the line below to implement source IP allowlisting.\n    ## Blocklisted IPs will get HTTP 403.\n    # nginx.ingress.kubernetes.io/whitelist-source-range: 98.128.193.2/32\n    ## Uncomment the lines below to get OAuth authentication\n    ## You will also need to configure and install oauth2-proxy.\n    ## For an example and more details, see https://github.com/elastisys/welkin/blob/main/user-demo/deploy/oauth2-proxy.yaml\n    # nginx.ingress.kubernetes.io/auth-url: \"https://$host/oauth2/auth\"\n    # nginx.ingress.kubernetes.io/auth-signin: \"https://$host/oauth2/start?rd=$escaped_request_uri\"\n    # nginx.ingress.kubernetes.io/auth-response-headers: \"authorization\"\nspec:\n  ingressClassName: nginx\n  rules:\n    - host: \"demo.example.com\"\n      http:\n        paths:\n          - path: /\n            pathType: Prefix\n            backend:\n              service:\n                name: myapp-welkin-user-demo\n                port:\n                  number: 3000\n  tls:\n    - hosts:\n        - \"demo.example.com\"\n      secretName: demo.example.com-tls\n</code></pre> <p>Example</p> <p>Feel free to take inspiration from the user demo.</p> <p>If you want to protect your Ingress with OAuth2-based authentication, check out oauth2-proxy.</p> <p>Important</p> <p>The DNS name in <code>spec.rules[0].host</code> and <code>spec.tls[0].hosts[0]</code> must be the same as the DNS entry used by your End Users, in the example above <code>demo.example.com</code>. Otherwise, the End Users will get a \"Your connection is not private\" error.</p> <p>Important</p> <p>Some load-balancers fronting Welkin do not preserve source IP. This makes source IP allowlisting unusable.</p> <p>To check if source IP is preserved, check the HTTP request headers received by your application, specifically <code>x-forwarded-for</code> and <code>x-real-ip</code>. The user demo logs all HTTP request headers, as shown in the screenshot below.</p> <p></p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#internal-load-balancer","title":"Internal Load Balancer","text":"<p>Note</p> <p>This is currently only supported on AWS, Azure, Elastx and UpCloud.</p> <p>You can request an additional Load Balancer specifically for internal traffic by filing a service ticket to the administrator. This is useful for separating internal traffic and external traffic and to protect the internal customers in case of a DDoS attack against the external Load Balancer.</p> <p>Warning</p> <p>There is a security concern if PROXY protocol is enabled on the Ingress Controller that can lead to an attacker being able to reach the internal endpoints from the public network. To avoid this, use the NGINX.Ingress.Kubernetes.io/whitelist-source-range annotation on the internal ingresses and only allow the private network. This is not a concern if PROXY protocol is disabled. If you are unsure whether PROXY protocol is enabled or not, ask the administrator.</p> <p>Once the Load Balancer is set up, a wildcard DNS record that points towards the internal Load Balancer will be provided. <code>*.internal.$DOMAIN</code> where <code>$DOMAIN</code> is the environment-specific variable you received from the administrator. It is recommended that you create a CNAME record that points towards the provided DNS record.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#lets-encrypt","title":"Let's Encrypt","text":"<p>Let\u2019s Encrypt is a certificate authority that provides free SSL/TLS certificates via an automated process. Their certificates are accepted by most of today\u2019s browsers.</p> <p>On Welkin, we provide a cert-manager setup which you can use to create, sign, install and renew certificates for your domains/apps running in Welkin.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#issuing-a-certificate","title":"Issuing a Certificate","text":"<p>You can use cert-manager setup for general purpose certificates not directly linked to an Ingress object.</p> <pre><code>apiVersion: cert-manager.io/v1\nkind: Certificate\nmetadata:\n  name: custom-cert\nspec:\n  dnsNames: # (1)\n    - \"domain.example.com\"\n  issuerRef: # (2)\n    kind: ClusterIssuer\n    name: letsencrypt-prod\n  secretName: custom-cert # (3)\n</code></pre> <ol> <li>For which domains the certificate will be valid for.</li> <li>Reference to the issuer to use.</li> <li>The created certificate is stored in this secret.</li> </ol> <p>And you can directly link a certificate to an Ingress object:</p> <pre><code>apiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod # (1)\n  name: webapp-ingress\n  namespace: default # (2)\nspec:\n  rules:\n    - host: example.com\n      http:\n        paths:\n          - pathType: Prefix\n            path: /\n            backend:\n              service:\n                name: myservice\n                port:\n                  number: 80\n  tls: # (3)\n    - hosts:\n        - example.com\n      secretName: webapp-certificate # (4)\n</code></pre> <ol> <li>Annotation indicating the issuer to use.</li> <li>Target namespace where the object will be created.</li> <li>Placing a host in the TLS configuration will determine what ends up in the cert\u2019s subjectAltNames.</li> <li>The created certificate is stored in this secret.</li> </ol> <p>Important</p> <p>You can monitor your own certificates using the User Alertmanager instance. Read more in our documentation about Alerts.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#lets-encrypt-environments","title":"Let's Encrypt Environments","text":"<p>Let's Encrypt provides two environments as part of their ACME V2 standardization: Staging &amp; Production.</p> <p>The ACME URL for Let's Encrypt ACME v2 staging environment is: https://acme-staging-v02.api.letsencrypt.org/directory</p> <p>The ACME URL for Let's Encrypt ACME v2 production environment is: https://acme-v02.api.letsencrypt.org/directory</p> <p>Both environments serve to issue valid certificates, the difference is the CA, on staging, the CA is not trusted by any application, web browser ..</p> <p>We highly recommend testing against the Let\u2019s Encrypt staging environment and use it for any non-production workloads. This will allow you to get things right before issuing trusted certificates and reduce the chance of you running up against rate limits. It should be to test that your client is working fine and can generate the challenges, certificates\u2026</p> <p>Important</p> <p>Certificates issued by the Let's Encrypt staging environment are signed by untrusted authorities, similar to self-signed certificates. They are typically not used in production environments.</p> <p>Warning</p> <p>Cert-manager's DNS-01 issuers that talk to the Internet will not work by default in Welkin due to restrictive Network Policies. If you need to make use of such issuers, e.g. for wildcard domains, please contact your Welkin Administrator.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#rate-limits","title":"Rate Limits","text":"<p>Let's Encrypt provides rate-limits on generated certificates to ensure fair usage across all clients. The production environment limits can be exceeded more frequently in environments where certificates are installed or reinstalled frequently. This can result in failed installations due to rate limit exceptions on certificate generation.</p> <p>In such environments, it is better to use the Let's Encrypt staging environment, which has much higher limits than the production environment.</p> <p>The default rate limits for the production environment are listed in the following page by Let's Encrypt: Production Rate Limits.</p> <p>The staging environment uses the same rate limits as described for the production environment with some exceptions that are listed here : Staging Rate Limits.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#demarcation-of-responsibilities","title":"Demarcation of Responsibilities","text":"<p>You are responsible for:</p> <ul> <li>creating Pods (via Deployments), Service and Ingress;</li> <li>segregating the private network via NetworkPolicies;</li> <li>configuring Ingresses as required to enable HTTPS encryption.</li> </ul> <p>The user demo already showcases the above.</p> <p>The Welkin administrator is responsible for:</p> <ul> <li>ensuring cert-manager works and is configured correctly;</li> <li>ensuring ClusterIssuers exist and are configured correctly;</li> <li>ensuring the private network is secure or trusted.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/network-model/#further-reading","title":"Further Reading","text":"<p>If you want to know more about Welkin's network internals, check out the following upstream documentation:</p> <ul> <li>Kubernetes Documentation: Networking</li> <li>Kubernetes Documentation: Services, Load Balancing, and Networking</li> <li>Kubernetes Documentation: DNS for Services and Pods</li> <li>Kubernetes Documentation: Ingress</li> <li>Kubernetes Documentation: Ingress Controllers</li> <li>Kubernetes Documentation: NetworkPolicies</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","MSBFS 2020:7 4 kap. 1 \u00a7","MSBFS 2020:7 4 kap. 2 \u00a7","MSBFS 2020:7 4 kap. 7 \u00a7","HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","NIST SP 800-171 3.1.20","NIST SP 800-171 3.4.7","NIST SP 800-171 3.13.1","NIST SP 800-171 3.13.6","NIS2 Minimum Requirement (e) Security in Network","NIS2 Minimum Requirement (h) Encryption-in-transit","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks","ISO 27001 Annex A 8.23 Web filtering","ISO 27001 Annex A 8.24 Use of Cryptography"],"boost":2},{"location":"user-guide/operate/","title":"Step 3: Operate","text":"<p>Welcome to the third step, Application Developer!</p> <p>In this step, you will learn how to operate your application on Welkin.</p>","boost":2},{"location":"user-guide/operate/#configure-dashboards-and-alerts","title":"Configure Dashboards and Alerts","text":"","boost":2},{"location":"user-guide/operate/#monitor-your-application","title":"Monitor your Application","text":"<p>To monitor your application, you will log in to your Grafana. Recall how to log in to your web portals from Step 1: Prepare.</p> <p>Grafana visually displays the monitoring data that Prometheus has collected on your behalf. A significant amount of metrics are already collected for you, out of the box, on Welkin. This means you can visualize data about the Cluster immediately.</p> <p>But Prometheus can also be instructed to collect specific metrics from your own application. Perhaps this is more useful to you than monitoring metrics that relate to Cluster health (in particular if somebody else managed Welkin for you).</p> <p>To instruct Prometheus on how to do this, you create a ServiceMonitor. This is a Kubernetes resource that configures Prometheus and specifies how to collect metrics from a particular application.</p> <p>The user demo already includes a ServiceMonitor, as required for Welkin to collect metrics from its <code>/metrics</code> endpoint:</p> <pre><code>{{- if .Values.serviceMonitor.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  name: {{ include \"welkin-user-demo.fullname\" . }}\n  labels:\n    {{- include \"welkin-user-demo.labels\" . | nindent 4 }}\nspec:\n  selector:\n    matchLabels:\n    {{- include \"welkin-user-demo.selectorLabels\" . | nindent 6 }}\n  endpoints:\n  - port: http\n{{- end }}\n</code></pre> <p>The screenshot below shows Grafana in \"Explore\" mode (the compass icon to the left) featuring the query <code>rate(http_request_duration_seconds_count[1m])</code>. It shows the request rate for the user demo application for each path and status code. As can be seen in the graph, the <code>/users</code> endpoint is getting more traffic than the other endpoints.</p> <p></p> <p>The \"Explore\" mode is great for developing queries and exploring the data set. If you want to save a query so you can refer back to it, you can create a Dashboard instead. Dashboards consist of multiple Panels, each of which, can display the results of running queries. Learn more about Grafana panels.</p> <p>Note</p> <p>You may want to save frequently used Dashboards. Welkin saves and backs these up for you.</p> <p>Go deeper into metrics.</p>","boost":2},{"location":"user-guide/operate/#alert-on-application-metrics","title":"Alert on Application Metrics","text":"<p>Visualizing monitoring metrics is one thing. Sometimes, you may need to act on what they show, immediately. For that reason, the Prometheus monitoring system includes Alertmanager.</p> <ul> <li>Prometheus is responsible for maintaining a set of Rules, which express trigger conditions via expressions. Once a rule has triggered, it has entered an alerting state.</li> <li>Alertmanager is responsible for forwarding information about any rules in the alerting state to your chosen destination, which could be your company's Slack or similar. A number of integrations are available.</li> </ul> <p>If you wish to create rules based on application-specific monitoring metrics, you must first create appropriate ServiceMonitors as described above.</p> <p>The user demo already includes a PrometheusRule, to configure an alert:</p> <pre><code>{{- if .Values.prometheusRule.enabled -}}\napiVersion: monitoring.coreos.com/v1\nkind: PrometheusRule\nmetadata:\n  name: {{ include \"welkin-user-demo.fullname\" . }}\n  labels:\n    {{- include \"welkin-user-demo.labels\" . | nindent 4 }}\nspec:\n  groups:\n  - name: ./example.rules\n    rules:\n    - alert: ApplicationIsActuallyUsed\n      expr: rate(http_request_duration_seconds_count[1m])&gt;1\n{{- end }}\n</code></pre> <p>The screenshot below gives an example of the application alert, as seen in Alertmanager.</p> <p></p> <p>Go deeper into metric alerts.</p>","boost":2},{"location":"user-guide/operate/#alert-on-log-contents","title":"Alert on Log Contents","text":"<p>Similar to alerting based on monitoring metrics, you may need to alert based on application log contents. For instance, it might make sense to send any log line of the <code>FATAL</code> log level to your Slack channel for immediate attention.</p> <p>The process of setting up log-based alerts is highly graphical, and supported by your OpenSearch Dashboards that is part of Welkin. Recall how to log in to your web portals from Step 1: Prepare.</p> <p>Go deeper into log-based alerts.</p>","boost":2},{"location":"user-guide/operate/#test-backups-and-capacity-management","title":"Test Backups and Capacity Management","text":"<p>Disaster recovery is about so much more than backing up and restoring data. Backing up data is a necessary, but not sufficient, part of that.</p> <p>Not having sufficient capacity is also a kind of disaster, albeit, one that is easy to mitigate.</p>","boost":2},{"location":"user-guide/operate/#back-up-application-data","title":"Back up Application Data","text":"<p>Welkin takes a daily backup of all Kubernetes Resources in all user namespaces. Persistent Volumes will be backed up if they are tied to a Pod. If backups are not wanted the label <code>compliantkubernetes.io/nobackup</code> can be added to opt-out of the daily backups.</p> <p>Application metrics (Grafana) and application log (OpenSearch) dashboards are also backed up by default.</p> <p>By default, backups are stored for 720 hours (30 days).</p> <p>Restoring from a backup with Velero is meant to be a type of disaster recovery. Velero will not overwrite existing Resources when restoring. As such, if you want to restore the state of a Resource that is still running, the Resource must be deleted first.</p> <p>To restore a backup on demand, contact your Welkin administrator.</p> <p>Go deeper into backups.</p>","boost":2},{"location":"user-guide/operate/#capacity-management","title":"Capacity Management","text":"<p>Capacity management is about having sufficient capacity for your needs, be they in terms of storage or computational power.</p> <p>Your Welkin administrator should perform capacity management of the platform, to ensure that there is a sufficient amount of spare capacity on a Cluster level.</p> <p>As an Application Developer, you should perform capacity management on a Pod level. This primarily means setting resource requests correctly for containers inside Pods, making use of multiple instances in your Deployments and Stateful Sets (possibly via horizontal Pod autoscaling). The use of resource requests and limits is enforced via an Welkin guardrail.</p>","boost":2},{"location":"user-guide/operate/#automate-with-cicd","title":"Automate with CI/CD","text":"<p>Welkin comes with Argo CD as an Additional Service.</p> <p>Welkin can also be integrated with an external CI/CD, such as GitHub Actions.</p>","boost":2},{"location":"user-guide/prepare-application/","title":"Prepare Your Application","text":"<p>To make the most out of Welkin, prepare your application so it features:</p> <ul> <li>some REST endpoints: NodeJS, .NET;</li> <li>structured logging: NodeJS, .NET;</li> <li>metrics endpoint: NodeJS, .NET;</li> <li>Dockerfile, which showcases:<ul> <li>How to run as non-root: NodeJS, .NET;</li> </ul> </li> <li>Helm Chart, which showcases:<ul> <li>HTTPS Ingresses;</li> <li>ServiceMonitor for metrics collection;</li> <li>PrometheusRule for alerting;</li> <li>topologySpreadConstraints for tolerating single Node or single Zone failure;</li> <li>resources for capacity management;</li> <li>NetworkPolicies for network segmentation;</li> </ul> </li> <li>Grafana dashboards for metrics visualization;</li> <li>script for local development and testing;</li> </ul> <p>Bonus:</p> <ul> <li>ability to make it crash (<code>/crash</code>).</li> </ul> <p>Feel free to clone our user demo for inspiration:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A21"],"boost":2},{"location":"user-guide/prepare-application/#make-sure-your-application-can-terminate-gracefully","title":"Make Sure Your Application Can Terminate Gracefully","text":"<p>In Kubernetes Pods and their Containers will sometimes be terminated. The cause can differ a lot, everything from you updating your application to a new version, to a Node being replaced or the Node running out of memory. Regardless of the cause, your application needs to be able to handle terminations unexpectedly.</p> <p>When a Pod termination is started there is usually a grace period where the Pod can clean up and then shut down gracefully. This grace period is usually 30 seconds, but can sometimes differ. If the Pod is not done shutting down at the end of this period, then it will be forcefully shut down. This process usually looks something like this:</p> <ol> <li>Something triggers the Pod termination</li> <li>Any <code>preStop</code> hooks in the Pod are triggered.</li> <li>TERM signal is sent to each Container in the Pod.</li> <li>If the <code>preStop</code> hook or the Pod has not terminated gracefully within the grace period, then the KILL signal is sent to all processes in the Pod.</li> </ol> <p>Your application might need to do some cleanup before terminating, like finishing transactions, closing connections, writing data to disk, etc. If that is the case, then you have two options to utilize the grace period before the Pod is forcefully terminated. You can utilize the <code>preStop</code> hook to start a script in a container or it can make a HTTP call to a container. You can have one <code>preStop</code> hook per Container in your Pod. You can also utilize the TERM signal that is sent to the containers by catching them in you application and having that trigger a graceful shutdown. You can both have <code>preStop</code> hooks and catch the TERM signal for the same container.</p> <p>You can read more about the Pod termination process in the official Kubernetes documentation.</p>","tags":["BSI IT-Grundschutz APP.4.4.A21"],"boost":2},{"location":"user-guide/prepare-application/#make-sure-your-application-tolerates-nodes-replacement","title":"Make Sure Your Application Tolerates Nodes Replacement","text":"<p>Important</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul> <p>In Kubernetes, Nodes will sometimes need to be replaced or drained. To mitigate any impact caused by these actions you have some options.</p> <p>As the user demo already showcases you can add replication and topologySpreadConstraints to your application. Another thing that can be done is add PodDisruptionBudgets (PDBs). This will make it so Kubernetes knows to pause draining Nodes which has applications using PDBs in them until those applications are sufficiently running. Normally PDBs have the downside of being easy to misconfigure but with this guardrail enabled, Welkin will deny the creation of PDBs that would block Node from draining. Lastly make sure to move state, even soft state, to specialized services.</p>","tags":["BSI IT-Grundschutz APP.4.4.A21"],"boost":2},{"location":"user-guide/prepare-application/#further-reading","title":"Further reading","text":"<ul> <li>Dealing with Disruptions</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A21"],"boost":2},{"location":"user-guide/prepare-application/#list-of-non-functional-requirements","title":"List of Non-Functional Requirements","text":"<p>In some contexts, it is useful to have a more-or-less exhaustive list of non-functional requirements which the application needs to fulfill to make the best use of the underlying platform. Sometimes, these may loosely be called \"IT requirements\".</p> <p>The key words \"MUST\", \"MUST NOT\", \"REQUIRED\", \"SHALL\", \"SHALL NOT\", \"SHOULD\", \"SHOULD NOT\", \"RECOMMENDED\",  \"MAY\", and \"OPTIONAL\" in this document are to be interpreted as described in RFC 2119.</p> Code Requirement      Justification for exclusion     OR     Evidence     OR     Comment  R1      Architectural requirements  R1.1      Application components MUST be loosely coupled. Data sharing MUST happen exclusively via versioned APIs.      Why is this important?         This allows the application team to scale. Each application developer only needs to work within a bounded context, reducing cognitive load.      R1.2      The application MUST use separate containers for synchronous API requests and running asynchronous batch jobs.      Why is this important?         This allows the application to make better use of resources and better tolerate failures. Being more exposed, synchronous code needs to be better tested and more secure than it's asynchronous counterpart. Furthermore, the asynchronous code may be managed by a queue to better handle spikes in work. The synchronous code can then focus on quick response times and good interaction with the user, without having to block for things such as sending a confirmation email.      R1.3      The application SHOULD crash if a fatal condition is encountered, such as bad configuration or inability to connect to a downstream service.      Why is this important?         This allows Kubernetes to stop a rolling update before end-user traffic is impacted.      R2      API requirements  R2.1      The application SHOULD accept inbound requests using the RESTful API.      Why is this important?         This allows the application to take advantage of the Ingress Controller for improved security via HTTPS, HSTS, IP allowlisting, rate-limiting, etc.      R2.2      All application APIs, internally and externally facing, synchronous (i.e. REST) and asynchronous (i.e. messages) MUST be versioned, and components MUST be able to process old API calls for as long as there are producers of these calls in production.      Why is this important?         This allows each application component to be release independently with zero downtime via a rolling update strategy.      R2.3      The application MUST validate all incoming API requests as well as responses to outgoing API requests.      Why is this important?         This is good security hygiene.      R2.4      If the application has asynchronous parts, then the application MUST use the message queue provided by the platform for communicating between its synchronous and asynchronous parts.      Why is this important?         The platform team will have put a lot of effort into making sure that the message queue is fault-tolerant. The application team can build upon this to improve application fault-tolerance, as opposed to reinventing the wheel.      R2.5      The asynchronous components in the application that consume messages MUST be compatible with old message formats until no producers of the old message format remain deployed.      Why is this important?         This allows the application to employ rolling update between components which communicate via the message queue.      R2.6      The application MUST set reasonable TTL for all messages sent via the platform-provide message queue.      Why is this important?         This ensures that messages don't accumulate in the message queue in case of application bugs, potentially leading to capacity issues. In best case, such capacity issues lead to unnecessary costs. In worst case, such capacity issues can lead to new messages not being accepted and application malfunctioning.      R2.7      The application MUST gracefully handle connection resets (e.g., due to fail-over) of downstream components.      Why is this important?         Sometimes the platform team needs to migrate Pods from one Node to another, e.g., for maintenance. The application component consuming a Pod which moved needs to tolerate this.      R3      State management requirements  R3.1      Database requirements  R3.1.1      The application MUST store structured state in an PostgreSQL-compatible database provided by the platform.      Why is this important?         The platform team will have invested a lot of effort in making sure that the database is fault-tolerant, backed up, etc. Instead of reinventing the wheel, the application team can build upon this.      R3.1.2      The application MUST perform database migration in a backwards-compatible manner.      Why is this important?         This allows the application team to rollback a buggy application. Despite best QA, some bugs may only manifest with production data and real users.      R3.1.3      The application MUST have a plan for rolling back changes, including dealing with database migrations.      Why is this important?         This allows the application team to rollback a buggy application. Despite best QA, some bugs may only manifest with production data and real users.      R3.1.4      The application MAY perform database migration in a Kubernetes init container.      Why is this important?         Database migration may take a long time. It's better to separate this from the container which is providing the actual service.      R3.2      Non-persistant state requirements  R3.2.1      The application MUST store non-persistant data, such as session information and cache, in the Redis-compatible key-value store provided by the platform.      Why is this important?         The platform team will have invested a lot of effort in making the key-value store fault-tolerant. By moving session state into the key-value store, each application replica is equal. No sticky load-balancing is needed and rolling updates are possible without disturbing the end-user.      R3.2.2      The application MUST set reasonable TTL for all key-value pairs.      Why is this important?         This ensures that the key-value store does not get filled with keys which are no longer in use, e.g., due to lack of cleanup or application crash.      R3.2.3      The application MUST gracefully tolerate loss of non-persistant data, e.g., ask the user to re-login.      Why is this important?         This is really a test for \"is this session data\"?      R3.2.4      The application SHOULD use the Redis Sentinel protocol to facilitate a highly available key-value store.      Why is this important?         This ensure the application can take advantage of the fault-tolerant key-value store provided by the platform.      R3.3      Object storage requirements  R3.3.1      The application MUST store large and/or unstructured data, like images, videos and PDF reports, in an S3-compatible object storage provided by the platform.      Why is this important?         This ensures the application is stateless. This in turn means that the application can be updated at will, without needing to implement a complicated data replication protocol.      R4      Configuration management requirements  R4.1      The application MUST accept configuration only via clearly documented configuration files and environment variables.      Why is this important?         This is good practice. It allows a container image to be built once and configured in many different ways.      R4.2      The application MUST separate secret from non-secret configuration information. Examples of secret configuration includes API keys and database access password.      Why is this important?         This allows tighter access control around secret configuration via Kubernetes RBAC.      R5      Observability requirements  R5.1      Observability requirements (metrics)  R5.1.1      The application SHOULD provide a metrics endpoint exposing application metrics in Prometheus Exposition format.      Why is this important?         This allows the application to take advantage of the observability stack provided by the platform.      R5.1.2      The metrics provided by the application SHOULD allow the application team to understand if it functions correctly.      Why is this important?         This allows the application team to close the DevOps loop and understand how their code runs in production.      R5.1.3      The application SHOULD provide alerting rules based on threshold on metrics.      Why is this important?         This allows the application team to discover issues with the application in production, before they are noticed by end-users.      R5.1.4      The application SHOULD provide metrics to understand if its deprecated APIs are still in use.      Why is this important?         This allows the application team to safely remove deprecated APIs, reducing technical debt without fear of disappointing end-users.      R5.2      Observability requirements (logs)  R5.2.1      The application MUST produce structured logs in multi-line JSON format on stdout.      Why is this important?         This allows the application to take advantage of the observability stack provided by the platform.      R5.2.2      The application MUST log exceptions and errors.      Why is this important?         This allows the application team to understand if something is malfunctioning with their application in production and issue bugfixes.      R5.2.3      The application SHOULD log all major boundary events, e.g., incoming and outgoing API requests.      Why is this important?         This allows the application team to understand how their application is working in production.      R5.2.4      The application MUST mark each log record with the relevant log level, e.g., debug, info, warn, error, exception.      Why is this important?         This allows the application team to filter log records based on importance and direct their attention to where it is needed.      R5.3      Observability requirements (tracing)  R5.3.1      The application SHOULD push traces to an endpoint provided by the platform using the OpenTelemetry standard.      Why is this important?         This allows the application team the finest possible observability of their application. They could, for example, determine which particular function is slow in some hard-to-replicate conditions, paving the path to a bugfix.      R5.4      Observability requirements (probes)  R5.4.1      The application MUST provide startup, readiness and liveliness probes, as relevant to the application component.      Why is this important?         This allows Kubernetes to understand if the container of the application is running proparly and issue corrective actions, such as directing traffic to a different replica or restarting the container.      R5.4.2      The application MUST fail its startup probe during database migration.      Why is this important?         If application initialization takes a long time, then a startup probe allows the application team to specify a different timeout for that phase.      R5.4.3      The application SHOULD fail its readiness probe if a downstream service is unavailable.      Why is this important?         This is readiness probe best practice. If the application cannot deliver a useful service, it should make this clear to the outside world to allow Kubernetes to take corrective actions.      R5.4.4      The application MUST handle SIGTERM by failing its readiness probe, draining connections and exiting gracefully.      Why is this important?         This allows \"hitless\" rolling updates.      R6      Build requirements  R6.1      The application MUST be containerized according to the OCI standard.      Why is this important?         This is pretty much a given nowadays, but is specified just to make sure.      R6.2      The application MUST run on linux/amd64 OCI platform.      Why is this important?         Welkin only supports Linux Nodes.      R6.3      The application MUST run as non-root.      Why is this important?         This is a security guardrail provided with Welkin. It improves security by ensuring that the application runs according to the least privilege principle.      R7      Deployment requirements  R7.1      Applications consisting of multiple components SHOULD be packaged in a versioned way (e.g., via Helm Chart).      Why is this important?         This ensure that the application is tested and deployed as a whole.      R7.2      The application SHOULD scale horizontally, i.e., its throughput should increase as more replicas are added.      Why is this important?         This ensure that the application can handle load spikes in a cost-efficient manner.      R7.3      The application MUST specify resource requests and limits for CPU and memory, and make suitable runtime configuration.      Why is this important?         This is a security guardrail provided with Welkin. It enforces good capacity management practices and reduces the risk of downtime due to capacity exhaustion.      R7.4      The application MUST adhere to the principle of least privilege in terms of network communication, and have the strictest possible set of firewall rules (i.e. Kubernetes Network Policies) in place.      Why is this important?         This is a security guardrail provided with Welkin. Good NetworkPolicies reduce the success of exploiting some vulnerabilities.      R7.5      The application SHOULD use a Blue/Green or Canary deployment strategy.      Why is this important?         This gives the application team a chance to detect a bug before it affects too many end-users and rollback.      R7.6      The application SHOULD use HorizontalPodAutoscaler to scale the number of replicas, as needed to react to load spikes.      Why is this important?         This ensure that the application can handle load spikes in a cost-efficient manner.      R7.8      The application MAY use a rolling upgrade strategy to ensure it can be upgraded with zero downtime.      Why is this important?         This allows the application team to deliver new features at high velocity, without worrying about downtime.      R8      Availability requirements  R8.1      The application MUST follow the \"rule of 2\". Every container needs to have at least two replicas.      Why is this important?         This ensures the application team cannot introduce bugs, e.g., state management via global variables, which compromise application high-availability and scalability.      R8.2      The application MUST tolerate its replicas running in different datacenters with latencies of up to 10 ms.      Why is this important?         This ensure the application can tolerate a datacenter failure, if this is a requirement.      R8.3      The application MUST tolerate the failure of a replica.      Why is this important?         This ensures the application is highly available and can tolerate the failure of a Node or (if needed) datacenter.","tags":["BSI IT-Grundschutz APP.4.4.A21"],"boost":2},{"location":"user-guide/prepare-idp/","title":"Prepare your Identity Provider (IdP)","text":"<p>For Welkin Managed Customers</p> <p>Please follow these steps to configure your IdP so that we can connect a new Welkin environment to it.</p> <p>To share credentials with Elastisys, please use our YoPass service.</p> <p>If you get stuck, get in touch with your contact person at Elastisys.</p> <p>To help you comply with various data protection regulations, Welkin only allows access to service endpoints (i.e., Kubernetes API, Harbor, Grafana and OpenSearch) via an IdP. Your organization's IdP acts as the single point to decide who gets access to what.</p> <p>This page describes how to configure Google Identity and Microsoft Entra ID so that Platform Administrators can connect a Welkin environment to them. Note, however, that Welkin supports any OpenID-compatible IdP, including GitHub and GitLab.</p> <p>This page show what information you need to send to Platform Administrators and where to find it.</p>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#microsoft-entra-id","title":"Microsoft Entra ID","text":"<p>Note</p> <p>Azure Active Directory is now Microsoft Entra ID.</p> <ol> <li>Sign in to the Azure portal.</li> <li>Search for and select Microsoft Entra ID.</li> <li>Under Manage, select App registrations &gt; New registration.</li> <li>Under Supported account types pick Accounts in any organizational directory (Any Microsoft Entra ID tenant - Multitenant).</li> <li>Under Redirect URI select web and insert the Dex URL that Platform Administrators provided. This is generally <code>https://dex.$DOMAIN/callback</code>.     If unsure, ask your Platform Administrators.</li> <li>Go to Overview and note down the Application (client) ID.</li> <li>Create a secret by going to Certificates &amp; secrets.</li> <li>Select the tab Client secrets and click New client secret.</li> <li>Set expiry date to 24 months.</li> <li>For improved security, navigate to Overview and note down the Directory (tenant) ID. This limits who can authenticate to your Welkin environment.</li> <li>Decide the name of the Microsoft Entra ID group that should have admin privileges in the environment.</li> <li> <p>Securely send, e.g., via YoPass, the following information to your Platform Administrators:      </p> <ul> <li>Directory (tenant) ID;</li> <li>Application (client) ID;</li> <li>Client secret;</li> <li>Admin group name.</li> </ul> </li> </ol>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#further-reading","title":"Further Reading","text":"<ul> <li>Quickstart: Register an application with the Microsoft identity platform</li> <li>Dex: Authentication through Microsoft</li> </ul>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#google","title":"Google","text":"<p>Important</p> <p>Some steps can only be done by an administrator account for a managed Google service,such as Google Workspace or Cloud Identity. (See this Google support page.)</p> <ol> <li>Go to Google Cloud -- Credentials.</li> <li>Create a new project through the top menu.</li> <li>In the new project, go to OAuth consent screen on the left side menu and create an internal consent screen.</li> <li>Go to Enabled APIs &amp; services on the left side menu and then click + ENABLE APIS AND SERVICES.</li> <li>Search for Admin SDK API and enable the API.</li> <li>Go back to Credentials on the left side menu.</li> <li>Click + CREATE CREDENTIALS and select OAuth client ID.</li> <li>Select Web Application for Application type, give it a suitable name.</li> <li>Set the Authorized redirect URIs to the Dex URL provided by your Administrators.     This is generally <code>https://dex.$DOMAIN/callback</code>.     If unsure, ask your Platform Administrators.</li> <li> <p>Finally, securely send, e.g., via YoPass, the following information to your Platform Administrators:</p> <ul> <li>client ID;</li> <li>client secret.</li> </ul> </li> </ol> <p>To set up groups follow these steps, note that steps 16-18 below can only be done by an administrator.</p> <ol> <li>Go to Google Cloud -- Service accounts.</li> <li>Make sure that you are in the same project that you created previously (see top menu).</li> <li>Click on + CREATE SERVICE ACCOUNT and give it a suitable name.</li> <li>Note down the Unique ID of the service account as you will need it soon.</li> <li>Go to the newly created service account and then under the KEYS tab click ADD KEY and create a new key of type JSON. Save the JSON file for the end.</li> <li>You need to give the service account read access to groups. Go to the Google admin console.</li> <li>Navigate through the menu to Security &gt; Access and data control &gt; API Controls and click Manage Domain Wide Delegation and then Add New.</li> <li>In the Client ID field put the Unique ID of the service account from step 4. and in the Oauth Scopes field enter this scope: <code>https://www.googleapis.com/auth/admin.directory.group.readonly</code>.</li> <li>Decide on the name of the Google group that should have admin privileges in the Welkin environment.</li> <li>Decide on a Google user email for the service account to impersonate when making calls to the admin API. The user would need at least the permission to retrieve a list of groups through the API.</li> <li> <p>Finally, securely send, e.g., via YoPass, the following information to your Platform Administrators:      </p> <ul> <li>the JSON file you downloaded;</li> <li>the admin group you decided on;</li> <li>the Google email you decided on.</li> </ul> </li> </ol>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#further-reading_1","title":"Further Reading","text":"<ul> <li>Dex: Authentication Through Google</li> <li>Google Identity: OpenID Connect</li> </ul>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#openid-providers","title":"OpenID Providers","text":"<p>Welkin should be compatible with any OpenID provider, although full compatibility cannot be guaranteed.</p> <p>The general instructions are as follows:</p> <ol> <li>Check that your IdP is OpenID compatible. You can check this by pointing your browser to: <code>https://$YOUR_IDP_DOMAIN/.well-known/openid-configuration</code>. If you get a well-formed JSON page, then your provider is OpenID compatible.</li> <li>Register an application with your OpenID provider. The callback or redirect URL is provided by your Administrators.     This is generally <code>https://dex.$DOMAIN/callback</code>.</li> <li>Allow at least the following scopes: <code>openid</code>, <code>email</code>, <code>groups</code>, <code>profile</code>.</li> <li>Securely send, e.g., via YoPass, the following information to your Platform Administrators:</li> </ol> <ul> <li>the IdP domain, i.e., <code>$YOUR_IDP_DOMAIN</code> which you used in step 1;</li> <li>client ID;</li> <li>client secret.</li> </ul>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare-idp/#further-reading_2","title":"Further Reading","text":"<ul> <li>Dex: Authentication Through and OpenID Connect Provider</li> <li>GitLab as OpenID Connect identity provider</li> <li>JumpCloud: SSO with OIDC</li> </ul>","tags":["HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","MSBFS 2020:7 4 kap. 5 \u00a7","NIST SP 800-171 3.1.1","NIST SP 800-171 3.3.2","NIS2 Minimum Requirement (i) Access Control","NIS2 Minimum Requirement (j) Multi-Factor Authentication","ISO 27001 Annex A 5.16 Identity Management","ISO 27001 Annex A 8.5 Secure Authentication"],"boost":2},{"location":"user-guide/prepare/","title":"Step 1: Prepare","text":"<p>For Welkin Managed Customers</p> <p>You can ask questions, get additional information, set up a call with our experts or order a managed service environment by emailing sales@elastisys.com.</p> <p>The highlights of ordering a managed service environment are:</p> <ul> <li>Infrastructure provider: You choose it, among the Elastisys Partners.</li> <li>Business Continuity: Kubernetes Cluster always feature 3 control plane Nodes.</li> <li>Retention: by default 30 days for logs, 90 days for metrics.</li> <li>Backup:<ul> <li>Scope: most Kubernetes resources and Persistent Volume Claims.</li> <li>RPO: daily, 3 backups</li> </ul> </li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 2 Managed Welkin Service Specification.</p>","boost":2},{"location":"user-guide/prepare/#step-1-prepare","title":"Step 1: Prepare","text":"<p>Hi there, Application Developer! Happy to have you on board with Welkin!</p> <p>In this part, you will learn about the things you should do to prepare to get started with the platform.</p> <p>Your administrator has already set up the platform for you. You will therefore have received:</p> <ul> <li>URLs for the Service Endpoints: OpenSearch Dashboards, Grafana, and Harbor;</li> <li>a kubeconfig file for configuring <code>kubectl</code> or Lens access to the Workload Cluster; and</li> <li>(optionally and rarely) a static username and password. Note that normally, you should log in via a username and a password of your organization's Identity Provider, such as LDAP, Azure Active Directory, or Google Identity.</li> </ul>","boost":2},{"location":"user-guide/prepare/#install-prerequisite-software","title":"Install Prerequisite Software","text":"<p>Required software:</p> <ul> <li>oidc-login, which helps you log into your Kubernetes Cluster via OpenID Connect integration with your Identity Provider of choice</li> </ul> <p>Your Cluster management software of choice, of which you can choose either or both:</p> <ul> <li>kubectl, a command-line tool to help manage your Kubernetes resources</li> <li>Kubernetes VS Code extension, a graphical user interface to help manage your Kubernetes resources (see also our dedicated page on Kubernetes UIs)</li> </ul> <p>Optional, but very useful, tools for developers and DevOps engineers:</p> <ul> <li>docker, if you want to build (Docker) container images locally</li> <li>Helm, if you want to manage your application with the Helm package manager</li> </ul> You can verify that configuration is correct by issuing the following simple commands <p>Make sure you have configured your tools properly:</p> <pre><code>export KUBECONFIG=path/of/kubeconfig.yaml  # leave empty if you use the default of ~/.kube/config\nexport DOMAIN=  # the domain you received from the administrator\n</code></pre> <p>To verify if the required tools are installed and work as expected, type:</p> <pre><code>docker version\nkubectl version  --client\nhelm version\n# You should see the version number of installed tools and no errors.\n</code></pre> <p>To verify the received KUBECONFIG, type:</p> <pre><code># Notice that you will be asked to complete browser-based single sign-on\nkubectl get nodes\n# You should see the Nodes of your Kubernetes cluster\n</code></pre> <p>To verify the received URLs, type:</p> <pre><code>curl --head https://dex.$DOMAIN/healthz\ncurl --head https://harbor.$DOMAIN/healthz\ncurl --head https://grafana.$DOMAIN/healthz\ncurl --head https://opensearch.$DOMAIN/api/status\ncurl --insecure --head https://app.$DOMAIN/healthz  # Ingress Controller\n# All commands above should return 'HTTP/2 200'\n</code></pre>","boost":2},{"location":"user-guide/prepare/#access-your-web-portals","title":"Access Your Web Portals","text":"<p>Those URLs that your Welkin administrator gave you all have a <code>$DOMAIN</code>, which will typically include your company name and perhaps the environment name.</p> <p>Your web portals are available at:</p> <ul> <li><code>harbor.$DOMAIN</code> -- the Harbor container image registry, which will be the home to all your container images</li> <li><code>opensearch.$DOMAIN</code> -- the OpenSearch Dashboards portal, where you will view your application and audit logs</li> <li><code>grafana.$DOMAIN</code> -- the Grafana portal, where you will view your monitoring metrics for both the platform, as such, and your application-specific metrics</li> </ul> <p>Additional endpoints are also available, depending on if your platform has these additional managed services (AMS) or not:</p> <ul> <li><code>jaeger.$DOMAIN</code> -- the Jaeger distributed tracing observability tool</li> <li><code>argocd.$DOMAIN</code> -- the Argo CD continuous Deployment GitOps tool</li> </ul>","boost":2},{"location":"user-guide/prepare/#containerize-your-application","title":"Containerize Your Application","text":"<p>Welkin runs containerized applications in a Kubernetes platform. It is a Certified Kubernetes distribution, which means that if an application is possible to deploy on a standard Kubernetes environment, it can be deployed on Welkin.</p> <p>However, there are some restrictions in place for security reasons. In particular, containers cannot be run as root. Following this best practice is a simple way to ensure additional security for your containerized applications deployed in Kubernetes.</p> <p>There are additional guardrails in place that reflect the security posture of Welkin that impact your application. These prevent users from doing potentially unsafe things. In particular, users are not allowed to:</p> <ul> <li>change the Kubernetes API through CustomResourceDefinitions or Dynamic Webhooks;</li> <li>run container images as root or mount <code>hostPath</code>s;</li> <li>mutate ClusterRoles or Roles so as to escalate privileges;</li> <li>mutate Kubernetes resources in administrator-owned namespaces, such as <code>monitoring</code> or <code>kube-system</code>;</li> <li>re-configure system Pods, such as Prometheus or Fluentd;</li> <li>access the hosts directly.</li> </ul>","boost":2},{"location":"user-guide/prepare/#next-step-deploying","title":"Next step? Deploying!","text":"<p>Ready with a containerized application? Head over to the next step, where you learn how to deploy it!</p>","boost":2},{"location":"user-guide/registry/","title":"Harbor - private container registry","text":"<p>This guide gives an introduction to Harbor and where it fits in Welkin, in terms of reducing the compliance burden.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#what-is-a-container-registry-and-why-it-is-needed","title":"What is a container registry and why it is needed?","text":"<p>A container registry is a system where you can store your container images in order to later use the images when you deploy your application (e.g. as a Pod in a Kubernetes Cluster). The images need a permanent storage since they are used many times by different instances, especially in Kubernetes where Pods (which are using the images) are considered ephemeral, so it is not enough to just store images directly on Nodes/virtual machines. There are many popular container registries available as services, e.g. Docker Hub and Google Container Registry.</p> <p>A common workflow with container registries is to build your images in a CI/CD pipeline, push the images to your registry, let the pipeline change your deployments that uses the images, and let the deployments pull down the new images from the repository.</p> <p></p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#what-is-harbor","title":"What is Harbor?","text":"<p>Harbor is an open source container registry tool that allows you to host a registry privately. It also comes with some extra features such as vulnerability scanning and role based access control, this increases security and eases compliance with certain regulations. Harbor is also a CNCF Graduated project, proving that it is widely used and is well supported.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#why-is-harbor-used-in-welkin","title":"Why is Harbor used in Welkin?","text":"<p>Harbor is used in Welkin to provide a secure container registry and a way to manage container image vulnerabilities. Harbor comes packaged with a container image vulnerability scanner that can check if there are any known vulnerabilities in the images you upload to Harbor. The default scanner is Trivy, which provides a comprehensive vulnerability detection both at the OS package and language-specific package levels.</p> <p>Below you can see both an image that has not been scanned and the same image after it has been scanned. After the image is scanned you can see the description, vulnerable package, and severity of each vulnerability as well as if it has been fixed in a later version. You can either scan the images manually or enable automatic scanning whenever a new image is pushed to Harbor, we recommend automatic scanning.</p> <p> </p> <p>In Harbor you can then also restrict so that you can't pull down images that have vulnerabilities of a certain severity or higher. This ensures that you don't accidentally start to use vulnerable images.</p> <p>If you try to deploy a Pod that uses a vulnerable image it will fail to pull the image. When you then inspect the Pod with <code>kubectl describe</code> you will find an error message similar to this:</p> <pre><code>Failed to pull image \"harbor.test.elastisys.io/test/ubuntu\": rpc error: code = Unknown desc = Error response from daemon: unknown: current image with 77 vulnerabilities cannot be pulled due to configured policy in 'Prevent images with vulnerability severity of \"Medium\" or higher from running.' To continue with pull, please contact your project administrator to exempt matched vulnerabilities through configuring the CVE whitelist.\n</code></pre> <p>By default we also prevent you from running images from anywhere else than your Harbor instance. This is to ensure that you use all of these security features and don't accidentally pull down vulnerable images from other container registries. We are using Open Policy Agent and Gatekeeper to manage this prevention. If you try to deploy a Pod with an image from another registry you will get an error message similar to this:</p> <pre><code>for: \"unsafe-image.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"unsafe-container\" has an invalid image repo \"unsafe.registry.io/ubuntu\", allowed repos are [\"harbor.test.elastisys.io\"]\n</code></pre>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#running-example","title":"Running Example","text":"","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#configure-container-registry-credentials","title":"Configure container registry credentials","text":"<p>First, retrieve your Harbor CLI secret and configure your local Docker client.</p> <ol> <li>In your browser, type <code>harbor.$DOMAIN</code> where <code>$DOMAIN</code> is the information you retrieved from your administrator.</li> <li>Log into Harbor using Single Sign-On (SSO) via OpenID.</li> <li>In the right-top corner, click on your username, then \"User Profile\".</li> <li>Copy your CLI secret.</li> <li>Now log into the container registry: <code>docker login harbor.$DOMAIN</code>.</li> <li>You should see <code>Login Succeeded</code>.</li> </ol>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#create-a-registry-project","title":"Create a registry project","text":"<p>Example</p> <p>Here is an example Dockerfile and .dockerignore to get you started. Don't forget to run as non-root.</p> <p>If you haven't already done so, create a project called <code>demo</code> via the Harbor UI, which you have accessed in the previous step.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#clone-the-user-demo","title":"Clone the user demo","text":"<p>If you haven't done so already, clone the user demo:</p> <pre><code>git clone https://github.com/elastisys/welkin/\ncd welkin/user-demo\n</code></pre>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#build-and-push-the-image","title":"Build and push the image","text":"<pre><code>REGISTRY_PROJECT=demo  # Name of the project, created above\nTAG=v1                 # Container image tag\n\ndocker build -t harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo:$TAG .\ndocker push harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo:$TAG\n</code></pre> <p>You should see no error message. Note down the <code>sha256</code> of the image.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#verification","title":"Verification","text":"<ol> <li>Go to <code>harbor.$DOMAIN</code>.</li> <li>Choose the <code>demo</code> project.</li> <li>Check if the image was uploaded successfully, by comparing the tag's <code>sha256</code> with the one returned by the <code>docker push</code> command above.</li> <li>(Optional) While you're at it, why not run the vulnerability scanner on the image you just pushed.</li> </ol>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#user-access","title":"User access","text":"<p>If OIDC was enabled (e.g. Dex) your Harbor user will be created when you first login to the web interface. That user will not have admin privileges, if you need admin rights please contact the administrator by opening a support ticket.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#create-tag-retention-rules","title":"Create Tag Retention Rules","text":"<p>Note</p> <p>Elastisys recommends that you use tag retention.</p> <p>Over time the number of artifacts in a repository can rapidly accumulate and can become difficult to manage them and remove the ones that might not be required after a given time, not to mention that they will use large quantities of storage. To address this problem Harbor offers the possibility for the application developers that are Harbor system administrators to define rules that govern how many artifacts of a given repository to retain, or for how long to retain certain artifacts. You can create several rules and specify which repositories and tags they apply to.</p> <p>The available options:</p> Option Description retain the most recently pushed # artifacts Enter the maximum number of artifacts to retain, keeping the ones that have been pushed most recently. There is no maximum age for an artifact. retain the most recently pulled # artifacts Enter the maximum number of artifacts to retain, keeping only the ones that have been pulled recently. There is no maximum age for an artifact. retain the artifacts pushed within the last # days Enter the number of days to retain artifacts, keeping only the ones that have been pushed during this period. There is no maximum number of artifacts. retain the artifacts pulled within the last # days Enter the number of days to retain artifacts, keeping only the ones that have been pulled during this period. There is no maximum number of artifacts. retain always Always retain the artifacts identified by this rule. <p>For instruction and examples on how to set Tag Retention Rules, please visit the Harbor documentation.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/registry/#further-reading","title":"Further reading","text":"<p>For more information please refer to the official Harbor, Trivy, Open Policy Agent and Gatekeeper documentation.</p>","tags":["HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","MSBFS 2020:7 4 kap. 20 \u00a7","NIST SP 800-171 3.1.13"],"boost":2},{"location":"user-guide/troubleshooting/","title":"Troubleshooting for Application Developers","text":"<p>Going through these basic troubleshooting steps should help you as an Application Developer identify where a problem may lie. If any of these steps do not give the expected \"fine\" output, use <code>kubectl describe</code> to investigate.</p> <p>If you are using Lens instead of the <code>kubectl</code> command-line interface, clicking through your Deployments and Pods will reveal the same information as the commands given below.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#is-the-kubernetes-cluster-fine","title":"Is the Kubernetes Cluster fine?","text":"<p>All Nodes need to have status <code>Ready</code>.</p> <pre><code>kubectl get nodes\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#are-my-application-pods-fine","title":"Are my application Pods fine?","text":"<p>Pods should be <code>Running</code> or <code>Completed</code>, and fully <code>Ready</code> (e.g., <code>1/1</code> or <code>6/6</code>)?</p> <pre><code>kubectl get pods\n</code></pre> <p>Check your Pods for excessive resource usage:</p> <pre><code>kubectl top pod\n</code></pre> <p>Inspect application logs and metrics.</p>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#are-my-deployments-fine","title":"Are my Deployments fine?","text":"<p>Are all Deployments fine? Deployments should show all Pods Ready, Up-to-date and Available (e.g., <code>2/2 2 2</code>).</p> <pre><code>kubectl get deployments\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#are-helm-releases-fine","title":"Are Helm Releases fine?","text":"<p>All Releases should be <code>deployed</code>.</p> <pre><code>helm list --all\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#are-my-certificates-fine","title":"Are my Certificates fine?","text":"<p>All Certificates needs to be Ready.</p> <pre><code>kubectl get certificates\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#is-the-api-server-healthy","title":"Is the API server healthy?","text":"<p>The command below should return <code>HTTP/2 200</code>.</p> <pre><code>curl --fail --verbose -k https://$loadbalancer_ip_address:6443/healthz\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/troubleshooting/#are-welkin-apps-services-healthy","title":"Are Welkin Apps services healthy?","text":"<p>All commands below should return <code>HTTP/2 200</code>.</p> <pre><code>curl --fail --verbose https://dex.$DOMAIN/healthz\ncurl --fail --verbose https://harbor.$DOMAIN/healthz\ncurl --fail --verbose https://grafana.$DOMAIN/healthz\ncurl --fail --verbose https://opensearch.$DOMAIN/\ncurl --fail --verbose -k https://app.$DOMAIN/healthz  # WC Ingress Controller\n</code></pre>","tags":["NIS2 Minimum Requirement (b) Incident Handling","ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation"],"boost":2},{"location":"user-guide/understand-the-basics/","title":"Understand the Basics","text":"<p>For Elastisys Managed Services Customers</p> <p>Elastisys is an Authorized Linux Foundation Training Partner and CNCF Certified Kubernetes Training Partner (KTP). Consider leveraging Elastisys Training to quickly get your application team up-to-speed.</p> <p>Before effectively using Welkin, you need to understand some basics. This section provides some useful links to build up this understanding.</p> <ul> <li>A Beginner-Friendly Introduction to Containers, VMs and Docker</li> <li>Kubernetes Application Development<ul> <li>We recommend Linux Foundation Certified Kubernetes Application Developer (CKAD)</li> </ul> </li> <li>Helm<ul> <li>We recommend Managing Kubernetes Applications with Helm (LFS244)</li> </ul> </li> </ul>"},{"location":"user-guide/additional-services/","title":"Additional Services","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed Additional Services by filing a service ticket.</p> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> <p></p> <p>Welkin simplifies usage of a complex and diverse infrastructure. By exposing simple and uniform concepts, it allows you to focus on application development.</p> <p>However, your application needs more than just running stateless containers. At the very least, you will need a database -- such as PostgreSQL -- to persist data. More complex applications will require a distributed cache -- such as Redis -- to store session information or offload the database. Finally, background tasks are best handled by separate containers, connected to your user-facing backend code via a message queue -- such as RabbitMQ.</p> <p>These additional services need to be delivered as securely as the rest of the platform. Access control, business continuity, disaster recovery, security patching and maintenance need to be core features, not afterthoughts.</p> <p>It turns out, the same simple and uniform concepts that benefit your application can also be used to simplify hosting additional services. And thanks to security-hardening included in Welkin, the burden of delivering additional services with the security you need is also reduced.</p> <p>Welkin is the \"hourglass waist\" of the platform. Think of it like HTTPS being the \"hourglass waist\" of the Internet: It unites the sprawl of wired and wireless network technologies to offer a uniform concept on which various web, gaming, chat and video streaming protocols can run.</p> <p>In the end, you win by having a feature-full platform to host your application. Not just VMs, but useful services. Administrators win by avoiding to re-invent the wheel and focus on the specifics of each additional service.</p> <p>This section of the user guide will help you benefit the most from the additional services hosted within Welkin.</p>","tags":["BSI IT-Grundschutz APP.4.4.A16"],"boost":2},{"location":"user-guide/additional-services/argocd/","title":"Argo\u2122 CD (preview)","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed Argo\u2122 CD by filing a service ticket. Here are the highlights:</p> <ul> <li>Disaster recovery:<ul> <li>Backup scope includes Argo CD resources that can be created by application developers, such as:<ul> <li>ApplicationSets</li> <li>Applications</li> <li>AppProjects</li> <li>Secrets and ConfigMaps as needed to store configuration on repositories and notifications.</li> </ul> </li> <li>A backup is taken every day between 0:00 am and 6:00 am CET. The backup retention period is 30 days unless otherwise requested by the customer.</li> <li>Recovery time objective (RTO): 4 hours.</li> </ul> </li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> <p>This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>Important</p> <p>You cannot ask Argo CD to create a Namespace. This also means that you cannot template the Namespace in ApplicationSets.</p> <p>Why? See ADR-0042</p> <p>This page will help you succeed in connecting to Argo CD application which meets your security and compliance requirements.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up the authentication inside Welkin, which will give you access to Argo CD UI.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#running-example","title":"Running Example","text":"<p>To deploy the user-demo, proceed as follows:</p> <ol> <li>Create a namespace called <code>welkin-demo</code>, as explained in the Namespaces page.</li> <li>Type the Argo CD Service Endpoint in the browsers. Generally, this is something like <code>https://argocd.$DOMAIN</code>.</li> <li>Complete the Single sign-on (SSO) authentication.</li> <li>In the right-hand menu, click Applications.</li> <li>In the top bar, click New app.</li> <li>In the right side, click Edit as YAML.</li> <li> <p>Copy-paste the YAML snippet below.</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: demo\nspec:\n  destination:\n    namespace: welkin-demo\n    server: https://kubernetes.default.svc\n  source:\n    path: user-demo/deploy/welkin-user-demo\n    repoURL: https://github.com/elastisys/welkin\n    targetRevision: main\n    helm:\n      values: |-\n        image:\n          repository: harbor.$DOMAIN/$REGISTRY_PROJECT/welkin-user-demo\n          tag: $TAG\n        ingress:\n          hostname: welkin-demo.$DOMAIN\n  project: default\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> </li> <li> <p>Replace <code>$DOMAIN</code>, <code>$REGISTRY_PROJECT</code> and <code>$TAG</code> as appropriate.</p> </li> <li>Click Save, then Create.</li> <li>Wait until Argo CD displays \"Status: Healthy Synced\", as shown in the picture below. .</li> <li>Click on the \"external link icon\" -- the one with a box and an arrow -- as shown in the image above to access your application.</li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#secret-management","title":"Secret Management","text":"<p>Secret values should never be stored in plain-text in code repositories. The following sections will describe three supported approaches on how Application Developers can do secret management with Elastisys Managed Argo CD.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#with-helm-secrets","title":"With Helm Secrets","text":"<p>Argo CD can be configured to decrypt files encrypted with <code>sops</code> using Helm Secrets. To be able to use Helm-secrets with Argo CD, you will need to contact your Platform Administrator requesting that you want to use this feature and for which OIDC group, users or ServiceAccounts (otherwise the Application Developer group used for accessing the Cluster is assumed). Once your Platform Administrator has approved your request, you should be able to create and manage a Kubernetes Secret resource in the <code>argocd-system</code> namespace as instructed below.</p> <p>The following steps will show how to get started with encrypting files using <code>sops</code> and <code>gpg</code>, storing the encrypted files in a Helm chart, and lastly deploying the Helm chart with Argo CD.</p> <ol> <li> <p>Generate a new GPG key-pair (make sure not to add a passphrase as otherwise Argo CD will not be able to use the key) and then export the private key. The following example will create a GPG key and export the private key to a file named <code>private.asc</code>:</p> <pre><code>gpg --batch --rfc4880 --passphrase '' --quick-generate-key \"set-me@example.com\" default default\nfpr=$(gpg --with-colons --list-keys \"set-me@example.com\" | awk -F: '$1 == \"fpr\" {print $10; exit}')\ngpg --output \"private.asc\" --armor --export-secret-key $fpr\n</code></pre> </li> <li> <p>For Argo CD to be able to use this key for decrypting <code>sops</code> encrypted values files in your Helm charts, you will need to create a Kubernetes Secret in the <code>argocd-system</code> namespace named <code>helm-secrets-private-keys</code>. This Secret will then be mounted and read by the Argo CD repository server. Create the Secret using the private key file created in the previous step:</p> <pre><code>PRIVATE_KEY_FILE=\"private.asc\"\nkubectl -n argocd-system create secret generic helm-secrets-private-keys --from-file=key.asc=$PRIVATE_KEY_FILE --dry-run=client -oyaml | \\\n    kubectl label -f- -oyaml argocd.argoproj.io/secret-type=repository --dry-run=client --local | \\\n    kubectl create -f-\n</code></pre> <p>Deploy the Secret:</p> <pre><code>kubectl create -f helm-secrets-private-keys.yaml\n</code></pre> <p>Note</p> <p>Once your Platform Administrator has enabled Helm-secrets with Argo CD, the Argo CD repo server Pod will not be able to initialize if this Secret does not exist. Check Pods in the <code>argocd-system</code> namespace after creating the Secret to confirm that the Secret got mounted properly by the repo server:</p> <pre><code>$ kubectl get pods -n argocd-system -l app.kubernetes.io/component=repo-server\nNAME                                  READY   STATUS    RESTARTS   AGE\nargocd-repo-server-77fd58b498-kjm95   1/1     Running   0          3m45s\n</code></pre> </li> <li> <p>Create a <code>.sops.yaml</code> file in the Helm chart in which you want to use Helm Secrets. Add your GPG public key to this file. You can get the public key with:</p> <pre><code>gpg --list-key \"set-me@example.com\"\n</code></pre> <p>The <code>.sops.yaml</code> file should look something like this:</p> <pre><code>---\ncreation_rules:\n  - pgp: &lt;public-gpg-key&gt;\n</code></pre> </li> <li> <p>Create a values file in your Helm chart repository and put values that should be encrypted into this file:</p> <pre><code>touch secrets.yaml\n</code></pre> <p>These values can then be referenced in templates as any other Helm values.</p> </li> <li> <p>Encrypt the values file using <code>sops</code> (as long as there is a <code>.sops.yaml</code> file in the same folder containing your public GPG key you do not have to specify the GPG key when running the following command):</p> <pre><code>sops --encrypt --in-place secrets.yaml\n</code></pre> <p>To test using the now encrypted <code>secrets.yaml</code> file together with the Helm chart, install the Helm Secrets plugin, and then run the following from the root folder of the Helm chart to template the Helm chart:</p> <pre><code>helm template . --values secrets://secrets.yaml\n</code></pre> </li> <li> <p>The file <code>secrets.yaml</code> can now safely be stored in a code repository as long as it is encrypted.</p> <p>Tip</p> <p>To edit values in the encrypted values file, you can use <code>sops</code> which will open the file in clear text in a configured editor:</p> <pre><code>sops secrets.yaml\n</code></pre> <p>There is also a VSCode plugin which can simplify editing <code>sops</code> encrypted files directly in the VSCode editor.</p> <p>Tip</p> <p>Add a pre-commit hook to ensure to not push files containing sensitive data that are not encrypted with <code>sops</code>.</p> </li> <li> <p>In Argo CD, when you create your Application, add the secret values file (<code>secrets.yaml</code>) as follow:</p> <p></p> <p>Here, <code>secrets.yaml</code> is the relative path and name of the file containing encrypted values.</p> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#with-sealedsecrets","title":"With SealedSecrets","text":"<p>The following steps assumes SealedSecrets is installed in the Cluster. For installing SealedSecrets in a Welkin Cluster, refer to the self-managed guide. You will need to contact your Platform Administrator requesting that you want to use SealedSecrets together with Argo CD.</p> <ol> <li> <p>Create a SealedSecret, the following steps will create a SealedSecret for the namespace in the current Kubernetes context:</p> <pre><code>export SEALED_SECRETS_CONTROLLER_NAMESPACE=sealed-secrets # set this to the namespace in which the controller is running in\nexport SECRET_VALUE=&lt;secret-data&gt; # this will be a value that should be encrypted/sealed\nexport KEY=foo # the key used for referencing SECRET_VALUE in the secret\n\nkubectl create secret generic mysecret --dry-run=client --from-literal=$KEY=$SECRET_VALUE -o yaml | \\\n    kubeseal --format yaml &gt; mysealedsecret.yaml\n</code></pre> <p>Note</p> <p>With SealedSecrets it is possible to set a scope of a Secret. By default this scope is set to <code>strict</code> in which the SealedSecret controller uses the name and namespace of the Secret as attributes during encryption, hence, the SealedSecret needs to be created with the same values for these attributes if the controller is to be able to decrypt the SealedSecret. It is possible to change the scope with the <code>--scope</code> flag for <code>kubeseal</code>, refer to the official documentation for SealedSecrets for possible scopes.</p> </li> <li> <p>The generated SealedSecret manifest file <code>mysealedsecret.yaml</code> will contain the encrypted <code>$SECRET_VALUE</code> and is safe to store on, for example, GitHub. Push this manifest file to the repository containing the rest of your application manifests.</p> </li> <li> <p>Deploy the application containing the SealedSecret with Argo CD as any other application.</p> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#using-vals-with-hashicorps-vault","title":"Using <code>vals</code> with HashiCorp's Vault","text":"<p>If you want to use <code>vals</code> with Vault, you will need to contact your Platform Administrator requesting that you want to use <code>vals</code> with Vault together with Argo CD.</p> <ol> <li> <p>Create a Secret in the <code>argocd-system</code> namespace called <code>vals-secret</code>:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: vals-secret\n  namespace: argocd-system\n  labels:\n    argocd.argoproj.io/secret-type: vals-secret\ntype: Opaque\ndata:\n  VAULT_ADDR: \"\"\n  VAULT_NAMESPACE: \"\"\n  VAULT_AUTH_METHOD: \"\"\n  VAULT_ROLE_ID: \"\"\n  VAULT_SECRET_ID: \"\"\n  VAULT_TOKEN: \"\"\n</code></pre> <p>Fill out required data in secret to connect it to your Vault.</p> <p>Note</p> <p>When encoding your data, make sure to use <code>echo -n &lt;data&gt; | base64</code> to avoid adding a new line at the end of your encoded secret.</p> </li> <li> <p>Contact your Platform Administrator to restart the <code>argocd-repo-server</code>. It is needed to load your secret environment variables so that Argo CD can connect to your Vault.</p> </li> <li> <p>Argo CD should now be able to connect to your Vault, to replace a value with a secret from your Vault, use the following syntax:</p> <pre><code>secrets+literal://vals!ref+vault://path/to/#/secret\n</code></pre> <p>Here's an example of an Argo CD Application manifest:</p> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: argocd-application\n  namespace: argocd-system\n  finalizers:\n    - resources-finalizer.argocd.argoproj.io\nspec:\n  destination:\n    namespace: staging\n    server: https://kubernetes.default.svc\n  project: default\n  source:\n    helm:\n      valueFiles:\n        - values.yaml\n      fileParameters:\n        - name: password\n          path: secrets+literal://vals!ref+vault://secret/user/#/password\n    path: deploy/helm/app\n    repoURL: https://github.com/username/repository.git\n    targetRevision: HEAD\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n</code></pre> </li> </ol>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#argo-cd-notifications","title":"Argo CD Notifications","text":"<p>Argo CD Notifications continuously monitors Argo CD applications and provides a flexible way to notify users about important changes in the application state. Using a flexible mechanism of triggers and templates you can configure when the notification should be sent as well as notification content.</p> <p>To configure Argo CD notifications, make sure you are allowed to update the current Kubernetes objects:</p> <ul> <li><code>secret/argocd-notifications-secret</code></li> <li><code>configmap/argocd-notifications-cm</code></li> </ul> <p>For Elastisys Managed Argo CD, you should be able to modify those objects if you belong to any customer admin group, or if you are a customer admin user.</p> <p>In case you're not, ask your Platform Administrator to add you accordingly.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#triggers-templates","title":"Triggers &amp; Templates","text":"<p>The trigger defines the condition when the notification should be sent. The definition includes name, condition and notification templates reference.</p> <p>The notification template is used to generate the notification content. Templates are meant to be reusable and can be referenced by multiple triggers.</p> <p>Both triggers &amp; notification templates can be configured in <code>argocd-notifications-cm</code> ConfigMap.</p> <p>Argo CD Notifications includes the catalog of useful triggers and templates. So you can just use them instead of reinventing new ones.</p> <p>Note</p> <p>The catalog triggers and templates can be found in the argocd-notifications-cm ConfigMap.</p> <pre><code>$ kubectl edit cm/argocd-notifications-cm -n argocd-system\n</code></pre> <p>If you don't have any triggers or templates defined, you can add the catalog template and trigger definitions</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: argocd-notifications-cm\ndata:\n    context: |\n        argocdUrl: \"https://argocd.example.com\"\n    template.app-deployed: |\n        ...\n    template.app-health-degraded: |\n        ...\n    template.app-sync-failed: |\n        ...\n    template.app-sync-running: |\n        ...\n    template.app-sync-status-unknown: |\n        ...\n    template.app-sync-succeeded: |\n        ...\n    trigger.on-deployed: |\n        ...\n    trigger.on-sync-failed: |\n        ...\n    trigger.on-sync-running: |\n        ...\n    trigger.on-sync-status-unknown: |\n        ...\n    trigger.on-sync-succeeded: |\n        ...\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#notification-services","title":"Notification Services","text":"<p>The notification services represent integration with services such as Slack, email or custom webhook. Services are configured in <code>argocd-notifications-cm</code> ConfigMap using <code>service.&lt;type&gt;.(&lt;custom-name&gt;)</code> keys and might reference sensitive data from <code>argocd-notifications-secret</code> Secret.</p> <p>Argo CD Notifications support multiple service types, and provide detailed steps on how to configure each service. To learn more, see Notification services.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#subscriptions","title":"Subscriptions","text":"<p>The subscription to Argo CD application events can be defined using <code>notifications.argoproj.io/subscribe.&lt;trigger&gt;.&lt;service&gt;: &lt;recipient&gt;</code> annotation. For example, the following annotation subscribes two Slack channels to notifications about every successful synchronization of the Argo CD application:</p> <pre><code>kubectl edit Application my-argo-application\n</code></pre> <pre><code>apiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: my-argo-application\n  annotations:\n    notifications.argoproj.io/subscribe.on-sync-succeeded.slack: my-channel1;my-channel2\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#example","title":"Example","text":"<p>To configure Email service for example:</p> <ol> <li> <p>Add email username and password token to <code>argocd-notifications-secret</code> Secret</p> <pre><code>echo -n \"sender@example.com\" | base64 -w0 # c2VuZGVyQGV4YW1wbGUuY29t\necho -n \"secretPassword\" | base64 -w0 #  c2VjcmV0UGFzc3dvcmQ=\nkubectl edit -n argocd-system argocd-notifications-secret\n</code></pre> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: argocd-notifications-secret\n  namespace: argocd-system\ndata:\n  email-username: c2VuZGVyQGV4YW1wbGUuY29t\n  email-password: c2VjcmV0UGFzc3dvcmQ=\n</code></pre> </li> <li> <p>Register Email notification service</p> <pre><code>kubectl edit -n argocd-system argocd-notifications-cm\n</code></pre> <p>Add the service under the ConfigMap data</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  service.email.gmail: |\n    username: $email-username\n    password: $email-password\n    host: smtp.gmail.com\n    port: 465\n    from: $email-username\n</code></pre> <p>In case you want to use a separate SMTP server instead of gmail</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: argocd-notifications-cm\ndata:\n  service.email.custom: |\n    username: $email-username\n    password: $email-password\n    host: smtp.custom.com\n    port: 587\n    from: $email-username\n</code></pre> </li> <li> <p>Subscribe to notifications by adding the notifications.argoproj.io/subscribe.on-sync-succeeded.gmail annotation to the Argo CD application:</p> <pre><code>kubectl patch app my-argo-application -p '{\"metadata\": {\"annotations\": {\"notifications.argoproj.io/subscribe.on-sync-succeeded.gmail\":\"receiver@example.com\"}}}' --type merge\n</code></pre> </li> </ol> <p>Now, if we try syncing an application, we will get the notification once sync is completed.</p> <p>Note</p> <p>Email notification will not work if the sender has 2FA enabled.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#restrictions","title":"Restrictions","text":"<p>Example error:</p> <p><code>Failed to load live state: Cluster level Namespace \"application\" can not be managed when in namespaced mode</code></p> <p>Our Argo CD installation is using the namespaced method. This means that Argo CD has access to Roles with permissions to CRUD on objects in the inclusion list. It has a list of namespaces that it can look at and reconcile things every few seconds. Any feature that requires Argo CD Cluster-wide installation will not be supported with our offering.</p> <p>The reason for this choice is that, according to the Welkin mission and vision, the platform should make it hard for Application Developers to do the wrong thing by employing guardrails and secure defaults. With this configuration, we prevent Argo CD from having access to objects in the inclusions list across the entire Cluster. This prevents objects from being deployed into namespaces owned by the Platform Administrator, which could compromise platform security and stability. For example, this choice adds another layer of protection, preventing the Application Developer from interfering with backups. Read more about it here.</p> <p>Argo CD is not allowed to manage its own namespace. This means that features such as Apps of Apps does not work by default. Read more about the decision here.</p> <p>Apps of Apps (preview)</p> <p>This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>Using Apps of Apps with our offering is currently a preview feature, and customers can request it, provided they accept risks such as:</p> <ul> <li> <p>When Apps-of-Apps is enabled, we will not be able to provide Uptime SLAs for you on the Argo Endpoint (This will be a best effort support even in future premium environments, unless the Argo project develops the feature upstream in a more secure and isolated way)</p> </li> <li> <p>Apps-of-Apps will currently be regarded as a preview feature and, as such, we reserve the right to disable it if it interferes with our OPS policies/practices \u2013 should that unfortunate event happen, it will be in dialogue with the customer.</p> </li> </ul> <p>Argo CD cannot create HNC namespaces and deploy services into them. This means that as an Application Developer you cannot template the namespace as a value in a manifest. As an Application Developer you need to create subnamespaces manually and deploy applications into it. Read more about the decision here.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#known-issues","title":"Known Issues","text":"<ul> <li>With Argo CD CLI access granted, an issue authenticating via Dex can occur and you will see an error message of</li> </ul> <p><code>failed to get token: oauth2: \"invalid_client\u201d \u201cInvalid client credentials.\"</code></p> <p>This is a known issue in Argo CD and the progress for it can be tracked here. Sometimes, when encountering this login issue, opening a new tab and re-trying the login process after entering the Argo CD homepage URL can resolve the problem.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#welkin-argo-cd-release-notes","title":"Welkin Argo CD Release Notes","text":"<p>Check out the release notes for the Argo CD setup that runs in Welkin environments!</p>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/argocd/#further-reading","title":"Further Reading","text":"<ul> <li>Argo CD documentation</li> <li>Helm Secrets usage</li> <li>SOPS</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A2","BSI IT-Grundschutz APP.4.4.A10","ISO 27001 Annex A 8.25 Secure Development Life Cycle","ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/additional-services/pgbouncer/","title":"PgBouncer","text":"<p>For Elastisys Managed Services Customers</p> <p>Managed PgBouncer is an addition to the Managed PostgreSQL\u00ae offering. You can request Managed PgBouncer without any additional cost by filing a service ticket.</p>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#install-prerequisites","title":"Install Prerequisites","text":"<p>When requesting an installation of Managed PgBouncer, you have the option to decide if PgBouncer will share Nodes with the PostgreSQL Cluster or if it can be scheduled on any worker Node. If PgBouncer shares Nodes with the PostgreSQL Cluster, allocated resources for PostgreSQL will be slightly decreased. PgBouncer usually does not use a lot of resources, compared to PostgreSQL.</p>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#getting-access","title":"Getting Access","text":"<p>When Managed PgBouncer is installed, the Secret set up by your administrator for your PostgreSQL Cluster is updated with some new information:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: $SECRET\n  namespace: $NAMESPACE\nstringData:\n  # PGHOST represents a cluster-scoped DNS name, which only makes sense inside the Kubernetes cluster.\n  # E.g., postgresql1-pgbouncer.postgres-system.svc.cluster.local\n  # PGHOST is changed to the DNS name for the PgBouncer service and should be used for all application connections.\n  PGHOST: $PGHOST\n  # PGBOUNCER_AUTH_FILE represents the name of the auth file Secret which stores database usernames and passwords for PgBouncer.\n  PGBOUNCER_AUTH_FILE: $PGBOUNCER_AUTH_FILE\n</code></pre> <p>Important</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To extract this information, proceed as follows:</p> <pre><code>export SECRET=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport PGHOST=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode)\nexport PGBOUNCER_AUTH_FILE=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGBOUNCER_AUTH_FILE}' | base64 --decode)\n</code></pre>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#authentication-file","title":"Authentication file","text":"<p>The <code>${PGBOUNCER_AUTH_FILE}</code> Secret contains the <code>userlist.txt</code> file. This file defines which users and passwords are allowed to authenticate, format for this file is described here. By default only the provided admin user <code>${PGUSER}</code> is allowed to authenticate.</p> <p>To change the PgBouncer auth file, update the <code>userlist.txt</code> file by patching or editing the <code>${PGBOUNCER_AUTH_FILE}</code> Secret.</p> <p>Warning</p> <p>When changes to the auth file are detected, the PgBouncer Deployment will be automatically restarted to load the new auth file. This is disruptive for active connections.</p>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#configuration","title":"Configuration","text":"<p>PgBouncer is configured via the <code>&lt;cluster-name&gt;-pgbouncer-config</code> ConfigMap, which contains the <code>pgbouncer.ini</code> file.</p> <p>Since PgBouncer configuration can be very specific to the PostgreSQL Cluster needs, the application developer with access to the PostgreSQL Cluster is allowed to change the PgBouncer configuration.</p> <p>Caution</p> <p>The application developer is then responsible for making sure that any changed configuration works. Misconfiguration can lead to loss of service.</p> <p>To change the PgBouncer configuration, update the <code>pgbouncer.ini</code> file by patching or editing the <code>&lt;cluster-name&gt;-pgbouncer-config</code> ConfigMap.</p> <p>Warning</p> <p>When changes to the configuration are detected, the PgBouncer Deployment will be automatically restarted to load the new configuration. This is disruptive for active connections.</p> <p>Refer to the upstream documentation for configuration details.</p>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#default-configuration","title":"Default configuration","text":"<p>The default PgBouncer configuration in the Managed PgBouncer offering will work in most cases. The default <code>max_client_conn</code> and <code>default_pool_size</code> settings are based on the PostgreSQL Cluster size and that the PostgreSQL Cluster has 5 active databases and 2 users per database.</p> <p>Noteworthy default configuration:</p> <ul> <li><code>listen_port</code> is set to <code>5432</code>.</li> <li><code>pool_mode</code> is set to <code>transaction</code>.</li> <li><code>auth_type</code> is set to <code>scram-sha-256</code>.</li> <li><code>client_tls_sslmode</code> is set to <code>required</code>. Clients must use TLS. If this is not possible for some reason the settings needs to be changed.</li> <li><code>server_tls_sslmode</code> is set to <code>required</code>. Communication between PgBouncer and the PostgreSQL Cluster uses TLS.</li> <li><code>admin_users</code> is set to the provided admin user <code>${PGUSER}</code>.</li> <li><code>auth_file</code> should not be changed.</li> <li><code>pidfile</code> should not be changed.</li> </ul>","boost":2},{"location":"user-guide/additional-services/pgbouncer/#migration-for-application-developers","title":"Migration for application developers","text":"<p>Follow these steps if you as an application developer already use the Managed PostgreSQL offering and want to start using Managed PgBouncer.</p> <ol> <li> <p>File a service ticket to request installation of Managed PgBouncer.</p> </li> <li> <p>Once PgBouncer is installed, configure the usernames and passwords in the <code>userlist.txt</code> file in the <code>${PGBOUNCER_AUTH_FILE}</code> Secret.</p> </li> <li> <p>Review PgBouncer configuration in the <code>&lt;cluster-name&gt;-pgbouncer-config</code> ConfigMap and update the <code>pgbouncer.ini</code> file if necessary. If unsure the default configuration can be used.</p> </li> <li> <p>Update the PostgreSQL hostname to the new <code>${PGHOST}</code> value in the application, the new service name that is referred to should contain <code>-pgbouncer</code>.</p> </li> </ol>","boost":2},{"location":"user-guide/additional-services/postgresql/","title":"PostgreSQL\u00ae","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed PostgreSQL\u00ae by filing a service ticket. Here are the highlights:</p> <ul> <li>Business continuity:<ul> <li>Standard Plan is configured with two replicas (one primary and one standby).</li> <li>Premium Plan is configured with three replicas (one primary and two standby-s).</li> <li>Both plans come with a PodDisruptionBudget to minimize downtime.</li> </ul> </li> <li>Disaster recovery:<ul> <li>Backup scope includes user definitions, data definitions, and the data per-se.</li> <li>A full backup is taken every day between 0:00 am and 6:00 am CET. The backup retention period is 30 days unless otherwise requested by the customer.</li> <li>Point-in-Time Recovery (PITR) is provided for the last 7 days with a recovery point objective of 5 minutes.</li> <li>Long-term backup schemes can be enabled after discussion with the customer.</li> </ul> </li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> PostgreSQL on Welkin Deployment Model          This help you build a mental model on how to access PostgreSQL as an Application Developer and how to connect your application to PostgreSQL.      <p>This page will help you succeed in connecting your application to a primary relational database PostgreSQL which meets your security and compliance requirements.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt-get install postgresql-client\n</code></pre>","boost":2},{"location":"user-guide/additional-services/postgresql/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a Secret inside Welkin, which contains all information you need to access your PostgreSQL Cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: $SECRET\n  namespace: $NAMESPACE\nstringData:\n  # PGHOST represents a cluster-scoped DNS name, which only makes sense inside the Kubernetes cluster.\n  # E.g., postgresql1.postgres-system.svc.cluster.local\n  PGHOST: $PGHOST\n  # PGHOST_REPLICA represents a cluster-scoped DNS name that references the replica, can be used for read-only queries.\n  PGHOST_REPLICA: $PGHOST_REPLICA\n\n  # These fields map to the environment variables consumed by psql.\n  # Ref https://www.postgresql.org/docs/13/libpq-envars.html\n  PGUSER: $PGUSER\n  PGPASSWORD: $PGPASSWORD\n  PGSSLMODE: $PGSSLMODE\n\n  # This is the Kubernetes Service name to which you can port-forward to in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster.\n  # Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\n  USER_ACCESS: $USER_ACCESS\n</code></pre> <p>Important</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To extract this information, proceed as follows:</p> <pre><code>export SECRET=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport PGHOST=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode)\nexport PGUSER=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode)\nexport PGPASSWORD=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode)\nexport PGSSLMODE=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode)\nexport USER_ACCESS=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode)\n</code></pre> <p>Important</p> <p>Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> <p>Important</p> <p>If you change the password for $PGUSER, you are responsible for keeping track of the new password.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#create-an-application-user","title":"Create an Application User","text":"<p>First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master.</p> <pre><code>kubectl -n $NAMESPACE port-forward svc/$USER_ACCESS 5432\n</code></pre> <p>Important</p> <p>Since humans are bad at generating random passwords, we recommend using pwgen.</p> <p>Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user:</p> <pre><code>export APP_DATABASE=myapp\nexport APP_USERNAME=myapp\nexport APP_PASSWORD=$(pwgen 32)\n\ncat &lt;&lt;EOF | psql -d postgres -h 127.0.0.1 -U $PGUSER \\\n    --set=APP_DATABASE=$APP_DATABASE \\\n    --set=APP_USERNAME=$APP_USERNAME \\\n    --set=APP_PASSWORD=$APP_PASSWORD\ncreate database :APP_DATABASE;\ncreate user :APP_USERNAME with encrypted password :'APP_PASSWORD';\ngrant all privileges on database :APP_DATABASE to :APP_USERNAME;\nEOF\n</code></pre> <p>Continue with the second console in the next section to create a Secret with this information.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Welkin Cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by <code>psql</code>.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-postgresql-secret\ntype: Opaque\nstringData:\n    PGHOST: ${PGHOST}\n    PGPORT: '5432'\n    PGSSLMODE: ${PGSSLMODE}\n    PGUSER: ${APP_USERNAME}\n    PGPASSWORD: ${APP_PASSWORD}\n    PGDATABASE: ${APP_DATABASE}\nEOF\n</code></pre> <p>Warning</p> <p>Although most client libraries follow the <code>libpq</code> definition of these environment variables, some do not, and this will require changes to the application Secret. Notably <code>node-postgres</code> does not currently do so for <code>PGSSLMODE</code>. When this variable is set to <code>require</code>, it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for <code>require</code> set the variable to <code>no-verify</code> instead.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#expose-postgresql-credentials-to-your-application","title":"Expose PostgreSQL credentials to Your Application","text":"<p>To expose the PostgreSQL Cluster credentials to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>","boost":2},{"location":"user-guide/additional-services/postgresql/#connection-pooling","title":"Connection pooling","text":"<p>Connection pooling can be enabled without any additional cost via Managed PgBouncer.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#welkin-postgresql-release-notes","title":"Welkin PostgreSQL Release Notes","text":"<p>Check out the release notes for the PostgreSQL Cluster that runs in Welkin environments!</p>","boost":2},{"location":"user-guide/additional-services/postgresql/#further-reading","title":"Further Reading","text":"<ul> <li>Creating users</li> <li>Creating databases</li> <li>Granting permissions</li> <li>Kubernetes Secrets</li> </ul>","boost":2},{"location":"user-guide/additional-services/rabbitmq/","title":"RabbitMQ\u00ae","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed RabbitMQ\u00ae by filing a service ticket. Here are the highlights:</p> <ul> <li>Business continuity: three replicas.</li> <li>Disaster recovery:<ul> <li>Backup scope includes user definitions, vhost definitions, topology definitions.</li> <li>Backup does NOT include messages -- RabbitMQ core Maintainers discourage this.</li> <li>A full backup is taken every day between 0:00 am and 6:00 am CET. The backup retention period is 30 days unless otherwise requested by the customer.</li> </ul> </li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> RabbitMQ on Welkin Deployment Model          This help you build a mental model on how to access RabbitMQ as an Application Developer and how to connect your application to RabbitMQ.      <p>This page will help you succeed in connecting your application to a RabbitMQ-based message queue which meets your security and compliance requirements.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#accessing-a-rabbitmq-cluster","title":"Accessing a RabbitMQ Cluster","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the RabbitMQ client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt install rabbitmq-server\n</code></pre>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#accessing-the-management-ui","title":"Accessing the Management UI","text":"<p>RabbitMQ provides the management plugin which allows control over everything within it, including messaging topology and access control.</p> <p>Your administrator will set up a Secret inside Welkin containing the credentials of the default admin user, which is all you need to access your RabbitMQ Cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: ${RABBITMQ_CLUSTER}-default-user\n  namespace: ${RABBITMQ_NAMESPACE}\nstringData:\n  username: ${RABBITMQ_ADMIN_USERNAME}\n  password: ${RABBITMQ_ADMIN_PASSWORD}\n</code></pre> <p>Danger</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To access the management UI, proceed as follows:</p> <ol> <li> <p>Retrieve the admin default username and password.</p> <pre><code>export RABBITMQ_CLUSTER=     # Get this from your administrator\nexport RABBITMQ_NAMESPACE=   # Get this from your administrator\n\necho -n \"RabbitMQ admin username: \"\nkubectl -n \"${RABBITMQ_NAMESPACE}\" get secret \"${RABBITMQ_CLUSTER}-default-user\" -o jsonpath=\"{.data.username}\" | base64 --decode &amp;&amp; echo\n\necho -n \"RabbitMQ admin password: \"\nkubectl -n \"${RABBITMQ_NAMESPACE}\" get secret \"${RABBITMQ_CLUSTER}-default-user\" -o jsonpath=\"{.data.password}\" | base64 --decode &amp;&amp; echo\n</code></pre> <p>Danger</p> <p>Do not configure your application with the RabbitMQ default admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> </li> <li> <p>Start the port-forwarding:</p> <pre><code>kubectl port-forward -n \"${RABBITMQ_NAMESPACE}\" \"svc/${RABBITMQ_CLUSTER}\" 15672\n</code></pre> </li> <li> <p>Open the admin dashboard (at http://localhost:15672) and log in using the credentials retrieved in step 1.</p> </li> <li> <p>Create an application username, password and vhost, and store these in variables as named below:</p> <pre><code>APP_USER=\nAPP_PASS=\nAPP_VHOST=\n</code></pre> </li> </ol>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Welkin Cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the AMPQ URL:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-amqp-secret\ntype: Opaque\nstringData:\n    AMQP_URL: amqp://${APP_USER}:${APP_PASS}@${RABBITMQ_CLUSTER}.${RABBITMQ_NAMESPACE}/${APP_VHOST}\nEOF\n</code></pre>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#expose-amqp-url-to-your-application","title":"Expose AMQP URL to Your Application","text":"<p>To expose the AMQP URL to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#allow-your-pods-to-communicate-with-the-rabbitmq-cluster","title":"Allow your Pods to communicate with the RabbitMQ Cluster","text":"<p>The RabbitMQ Cluster is protected by Network Policies.</p> <p>To allow your application to communicate with the RabbitMQ Cluster, add the following label to your Pods: <code>elastisys.io/rabbitmq-${RABBITMQ_CLUSTER}-access: allow</code></p> <p>Using the <code>${RABBITMQ_CLUSTER}</code> from before, you can also get it from your administrator</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#using-a-rabbitmq-cluster","title":"Using a RabbitMQ Cluster","text":"<p>Best practices for using RabbitMQ in Welkin.</p> <p>Note</p> <p>RabbitMQ is built as a plugin based system, by default only a limited set of plugins are enabled, contact your service-specific administrator about enabling the ones you require.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#cluster-division","title":"Cluster Division","text":"<p>RabbitMQ is a multi tenant system and has the concept of virtual host or vhost which provides logical separation of resources within it. Each vhost acts as their own RabbitMQ Cluster and have their own connection, channels, exchanges, queues, and bindings.</p> <p>Messages cannot flow directly between exchanges and queues in different vhosts, instead if that is required there must be a client that consumes messages in one vhost and then publishes messages in another vhost. This can also be realised with two plugins:</p> <ul> <li> <p>Federation plugin - Provides federated exchanges and queue which connect to an upstream Cluster or vhost, allowing consumers to access message from the upstream source.</p> </li> <li> <p>Shovel plugin - Provides a client which connect both to an upstream and downstream Cluster or vhost, consuming and publishing messages between them.</p> </li> </ul>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#connection-and-channel-management","title":"Connection and Channel Management","text":"<p>In RabbitMQ connections and channels are intended to and optimised to be long-lived. Therefore one should avoid connection and channel churn by using and reusing them for as long as possible.</p> <p>Each connection and channel consumes resources and should therefore be kept at a minimal number as required by the application. It is important to close unused connections and channels to free their resources.</p> <p>Consumers and publishers should use separate connections and channels to allow RabbitMQ to better manage clients when under high pressure, allowing consumers to catch up by restricting publishers.</p> <p>Recovering from connection failures should use automatic or recommended methods when provided by the client library, custom recovery methods need to be carefully made to not put too much pressure on RabbitMQ.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#reliable-messaging","title":"Reliable Messaging","text":"<p>The messaging protocol AMQP provide two mechanisms for client to manage reliable messaging:</p> <ul> <li> <p>Consumer Acknowledgements - Ensure that the client has properly received a message. RabbitMQ will redeliver message left unacknowledged.</p> </li> <li> <p>Publisher Confirms - Ensures that the server has properly received a message. RabbitMQ may delay confirms when under to high pressure to be able to handle the load.</p> </li> </ul> <p>Additionally RabbitMQ has different queue types and modes that affect the reliability of messaging as will be explained in the next section.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#queue-selection","title":"Queue Selection","text":"<p>RabbitMQ supports three different types of queues which all have different pro's and con's:</p> <ul> <li>Classic Queues - Best for high performance with transient messages.</li> </ul> <p>This is the standard choice for transient queues and messages since they consume little resources and offer high performance.</p> <p>Danger</p> <p>Classic queues are not replicated by default and should not be used with replication since mirrored classic queues are deprecated.</p> <p>Durable classic queues are tied to the server they are created on and will become unavailable if that server is unavailable.</p> <ul> <li>Quorum Queues - Best for high availability with durable messages.</li> </ul> <p>This is the standard choice for durable queues and messages since they are replicated by default and offer high availability.</p> <p>Warning</p> <p>Similar to connections and channels quorum queues are intended to be long-lived, avoid scenarios where quorum queues are frequently removed and redeclared as it may lock internal resources within RabbitMQ.</p> <p>Keep the queues small and use multiple queues when possible for best performance.</p> <ul> <li>Stream Queues - Best for high volume with durable messages.</li> </ul> <p>This is the standard choice for high volume durable queues and messages since they store messages as a persistent replicated log. This allows for messages to persist after they are consumed to be recalled or replayed, or with the stream protocol efficiently processed in batches.</p> <p>Warning</p> <p>When using stream queues you must set reasonable retention to keep them from filling up RabbitMQ as it will prevent it from accepting new messages.</p> <p>Also note that messages in RabbitMQ are not backed up in the case of disaster.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#welkin-rabbitmq-release-notes","title":"Welkin RabbitMQ Release Notes","text":"<p>Check out the release notes for the RabbitMQ Cluster that runs in Welkin environments!</p>","boost":2},{"location":"user-guide/additional-services/rabbitmq/#further-reading","title":"Further Reading","text":"<ul> <li>RabbitMQ Management UI</li> <li>AMQP URL spec</li> <li>AMQP Clients</li> <li>Kubernetes Secrets</li> </ul>","boost":2},{"location":"user-guide/additional-services/timescaledb/","title":"TimescaleDB\u00ae","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed TimescaleDB\u00ae Apache 2 Edition by filing a service ticket. Here are the highlights:</p> <ul> <li>Business continuity:<ul> <li>Standard Plan is configured with two replicas (one primary and one standby).</li> <li>Premium Plan is configured with three replicas (one primary and two standby-s).</li> </ul> </li> <li>Disaster recovery:<ul> <li>Backup scope includes user definitions, data definitions, and the data per-se.</li> <li>A full backup is taken every day between 0:00 am and 6:00 am CET. The backup retention period is 30 days unless otherwise requested by the customer.</li> <li>Point-in-Time Recovery (PITR) is provided for the last 7 days with a recovery point objective of 5 minutes.</li> <li>Long-term backup schemes can be enabled after discussion with the customer.</li> </ul> </li> <li>Only Apache 2 edition features are included (see below)</li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> <p>Open source vs source available</p> <p>Please note that there are two versions of TimescaleDB:</p> <ul> <li>TimescaleDB Apache 2 Edition, which is open source;</li> <li>TimescaleDB \"Community Edition\", which -- despite its misleading name -- is not open source.</li> </ul> <p>Although the so-called \"Community Edition\" is source-available, it is technically proprietary, given that it is licensed under the Timescale License (TSL). Amongst others, the TSL prohibits using TimescaleDB \"Community Edition\" for database-as-a-service.</p> <p>Therefore, Elastisys does not offer TimescaleDB \"Community Edition\".</p> TimescaleDB on Welkin Deployment Model          This help you build a mental model on how to access TimescaleDB as an Application Developer and how to connect your application to TimescaleDB.      <p>This page will help you succeed in connecting your application to the time-series database TimescaleDB which meets your security and compliance requirements.</p> <p>TimescaleDB is an extension on top of our managed PostgreSQL. This means that your administrator will be setting up a complete PostgreSQL Cluster for you and you just use it for TimescaleDB via the TimescaleDB extension.</p> <p>Note</p> <p>TimescaleDB is not a viable option for collecting all metrics from the Kubernetes Cluster. The data is uncompressed and would take a lot of space to store and use a lot of resources to analyze, unless you want to use it with a very short retention period. This is not usually a problem for collecting application specific metrics, since they are not as many as the metrics that are generated from the Kubernetes Cluster.</p> <p>Important</p> <p>Due to very different performance-tuning characteristics, Timescale and PostgreSQL databases should never run on the same PostgreSQL Cluster. To comply with this, it is essential that every PostgreSQL database that gets created on the PostgreSQL Cluster also has the Timescale extension created for it.</p> <p>If you want to use TimescaleDB on your Welkin Cluster, ask your administrator to provision a new PostgreSQL Cluster inside your Welkin environment. Then set up the TimescaleDB extension.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the PostgreSQL client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt-get install postgresql-client\n</code></pre>","boost":2},{"location":"user-guide/additional-services/timescaledb/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a Secret inside Welkin, which contains all information you need to access your PostgreSQL Cluster. The Secret has the following shape:</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: $SECRET\n  namespace: $NAMESPACE\nstringData:\n  # PGHOST represents a cluster-scoped DNS name, which only makes sense inside the Kubernetes cluster.\n  # E.g., postgresql1.postgres-system.svc.cluster.local\n  PGHOST: $PGHOST\n  # PGHOST_REPLICA represents a cluster-scoped DNS name that references the replica, can be used for read-only queries.\n  PGHOST_REPLICA: $PGHOST_REPLICA\n\n  # These fields map to the environment variables consumed by psql.\n  # Ref https://www.postgresql.org/docs/13/libpq-envars.html\n  PGUSER: $PGUSER\n  PGPASSWORD: $PGPASSWORD\n  PGSSLMODE: $PGSSLMODE\n\n  # This is the Kubernetes Service name to which you can port-forward to in order to get access to the PostgreSQL cluster from outside the Kubernetes cluster.\n  # Ref https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/\n  USER_ACCESS: $USER_ACCESS\n</code></pre> <p>Important</p> <p>The Secret is very precious! Prefer not to persist any information extracted from it, as shown below.</p> <p>To extract this information, proceed as follows:</p> <pre><code>export SECRET=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport PGHOST=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGHOST}' | base64 --decode)\nexport PGUSER=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGUSER}' | base64 --decode)\nexport PGPASSWORD=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGPASSWORD}' | base64 --decode)\nexport PGSSLMODE=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.PGSSLMODE}' | base64 --decode)\nexport USER_ACCESS=$(kubectl -n $NAMESPACE get secret $SECRET -o 'jsonpath={.data.USER_ACCESS}' | base64 --decode)\n</code></pre> <p>Important</p> <p>Do not configure your application with the PostgreSQL admin username and password. Since the application will get too much permission, this will likely violate your access control policy.</p> <p>Important</p> <p>If you change the password for $PGUSER, you are responsible for keeping track of the new password.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#create-an-application-user","title":"Create an Application User","text":"<p>First, in one console, fetch the information from the access Secret as described above and port forward into the PostgreSQL master.</p> <pre><code>kubectl -n $NAMESPACE port-forward svc/$USER_ACCESS 5432\n</code></pre> <p>Important</p> <p>Since humans are bad at generating random passwords, we recommend using pwgen.</p> <p>Second, in another console, fetch the information from the access Secret again and run the PostgreSQL client to create the application database and user:</p> <pre><code>export APP_DATABASE=myapp\nexport APP_USERNAME=myapp\nexport APP_PASSWORD=$(pwgen 32)\n\ncat &lt;&lt;EOF | psql -d postgres -h 127.0.0.1 -U $PGUSER \\\n    --set=APP_DATABASE=$APP_DATABASE \\\n    --set=APP_USERNAME=$APP_USERNAME \\\n    --set=APP_PASSWORD=$APP_PASSWORD\ncreate database :APP_DATABASE;\ncreate user :APP_USERNAME with encrypted password :'APP_PASSWORD';\ngrant all privileges on database :APP_DATABASE to :APP_USERNAME;\nEOF\n</code></pre> <p>Continue with the second console in the next section to create a Secret with this information.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#create-an-application-secret","title":"Create an Application Secret","text":"<p>First, check that you are on the right Welkin Cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes Secret in your application namespace to store the PostgreSQL application username and password. For consistency, prefer sticking to naming connection parameters as the environment variables consumed by <code>psql</code>.</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: Secret\nmetadata:\n    name: app-postgresql-secret\ntype: Opaque\nstringData:\n    PGHOST: ${PGHOST}\n    PGPORT: '5432'\n    PGSSLMODE: ${PGSSLMODE}\n    PGUSER: ${APP_USERNAME}\n    PGPASSWORD: ${APP_PASSWORD}\n    PGDATABASE: ${APP_DATABASE}\nEOF\n</code></pre> <p>Warning</p> <p>Although most client libraries follow the <code>libpq</code> definition of these environment variables, some do not, and this will require changes to the application Secret. Notably <code>node-postgres</code> does not currently do so for <code>PGSSLMODE</code>. When this variable is set to <code>require</code>, it will do a full verification instead, requiring access to the PostgreSQL certificates to allow a connection. To get the intended mode for <code>require</code> set the variable to <code>no-verify</code> instead.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#expose-postgresql-credentials-to-your-application","title":"Expose PostgreSQL credentials to Your Application","text":"<p>To expose the PostgreSQL Cluster credentials to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the secret data through a Volume</li> <li>Define container environment variables using Secret data</li> </ul>","boost":2},{"location":"user-guide/additional-services/timescaledb/#connection-pooling","title":"Connection pooling","text":"<p>Connection pooling can be enabled without any additional cost via Managed PgBouncer.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#set-up-the-timescaledb-extension-on-postgresql","title":"Set up the TimescaleDB extension on PostgreSQL","text":"<ul> <li>Connect to the created database:</li> </ul> <pre><code>\\c $APP_DATABASE\n</code></pre> <ul> <li>Add the TimescaleDB extension:</li> </ul> <pre><code>CREATE EXTENSION IF NOT EXISTS timescaledb;\n</code></pre>","boost":2},{"location":"user-guide/additional-services/timescaledb/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#welkin-timescaledb-release-notes","title":"Welkin TimescaleDB Release Notes","text":"<p>Check out the release notes for the TimescaleDB/PostgreSQL Cluster that runs in Welkin environments!</p>","boost":2},{"location":"user-guide/additional-services/timescaledb/#further-reading","title":"Further Reading","text":"<ul> <li>Getting started with Timescale</li> <li>Creating users</li> <li>Creating databases - Remember to create Timescale extension on the new databases.</li> <li>Granting permissions</li> <li>Kubernetes Secrets</li> </ul>","boost":2},{"location":"user-guide/additional-services/valkey/","title":"Valkey\u2122 (Previously Redis\u2122)","text":"<p>For Welkin Managed Customers</p> <p>You can order Managed Ephemeral Valkey\u2122 by filing a service ticket. Here are the highlights:</p> <ul> <li>Business continuity: Replicated across three dedicated Nodes.</li> <li>Disaster recovery: none -- we recommend against using Valkey as a primary database.</li> <li>Monitoring, security patching and incident management: included.</li> </ul> <p>For more information, please read ToS Appendix 3 Managed Additional Service Specification.</p> Valkey on Welkin Deployment Model          This help you build a mental model on how to access Valkey as an Application Developer and how to connect your application to Valkey.      <p>This page will help you succeed in connecting your application to a low-latency in-memory cache Valkey which meets your security and compliance requirements.</p> <p>Important: Access Control with NetworkPolicies</p> <p>Please note the follow information about Valkey access control from the upstream documentation:</p> <p>Valkey is designed to be accessed by trusted clients inside trusted environments.</p> <p>Valkey access is protected by NetworkPolicies. To allow your applications access to a Valkey cluster the Pods need to be labeled with <code>elastisys.io/valkey-&lt;cluster_name&gt;-access: allow</code>.</p> <p>Important: No Disaster Recovery</p> <p>We do not recommend using Valkey as primary database. Valkey should be used to store:</p> <ul> <li>Cached data: If this is lost, this data can be quickly retrieved from the primary database, such as the PostgreSQL cluster.</li> <li>Session state: If this is lost, the user experience might be impacted -- e.g., the user needs to re-login -- but no data should be lost.</li> </ul> <p>Important: Sentinel support</p> <p>We recommend a highly available setup with at minimum three instances. The Valkey client library that you use in your application needs to support Valkey Sentinel. Notice that clients with Sentinel support need extra steps to discover the Valkey primary.</p>","boost":2},{"location":"user-guide/additional-services/valkey/#install-prerequisites","title":"Install Prerequisites","text":"<p>Before continuing, make sure you have access to the Kubernetes API, as describe here.</p> <p>Make sure to install the Valkey client on your workstation. On Ubuntu, this can be achieved as follows:</p> <pre><code>sudo apt install redis-tools\n</code></pre>","boost":2},{"location":"user-guide/additional-services/valkey/#getting-access","title":"Getting Access","text":"<p>Your administrator will set up a ConfigMap inside Welkin, which contains all information you need to access your Valkey Cluster. The ConfigMap has the following shape:</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: $CONFIG_MAP\n    namespace: $NAMESPACE\ndata:\n    # VALKEY is the name of the Valkey Cluster. You need to know the name to label your Pods correctly for network access.\n    VALKEY_CLUSTER_NAME: $VALKEY_CLUSTER_NAME\n\n    # VALKEY_SENTINEL_HOST represents a cluster-scoped Valkey Sentinel host, which only makes sense inside the Kubernetes cluster.\n    # E.g., rfs-valkey-cluster.valkey-system\n    VALKEY_SENTINEL_HOST: $VALKEY_SENTINEL_HOST\n\n    # VALKEY_SENTINEL_PORT represents a cluster-scoped Valkey Sentinel port, which only makes sense inside the Kubernetes cluster.\n    # E.g., 26379\n    VALKEY_SENTINEL_PORT: \"$VALKEY_SENTINEL_PORT\"\n</code></pre> <p>To extract this information, proceed as follows:</p> <pre><code>export CONFIG_MAP=            # Get this from your administrator\nexport NAMESPACE=         # Get this from your administrator\n\nexport VALKEY_SENTINEL_HOST=$(kubectl -n $NAMESPACE get configmap $CONFIG_MAP -o 'jsonpath={.data.VALKEY_SENTINEL_HOST}')\nexport VALKEY_SENTINEL_PORT=$(kubectl -n $NAMESPACE get configmap $CONFIG_MAP -o 'jsonpath={.data.VALKEY_SENTINEL_PORT}')\n</code></pre> <p>Important</p> <p>At the time of this writing, we do not recommend to use a Valkey Cluster in a multi-tenant fashion. One Valkey Cluster should have only one purpose.</p>","boost":2},{"location":"user-guide/additional-services/valkey/#create-a-configmap","title":"Create a ConfigMap","text":"<p>First, check that you are on the right Welkin Cluster, in the right application namespace:</p> <pre><code>kubectl get nodes\nkubectl config view --minify --output 'jsonpath={..namespace}'; echo\n</code></pre> <p>Now, create a Kubernetes ConfigMap in your application namespace to store the Valkey Sentinel connection parameters:</p> <pre><code>cat &lt;&lt;EOF | kubectl apply -f -\napiVersion: v1\nkind: ConfigMap\nmetadata:\n    name: app-valkey-config\ndata:\n    VALKEY_SENTINEL_HOST: $VALKEY_SENTINEL_HOST\n    VALKEY_SENTINEL_PORT: \"$VALKEY_SENTINEL_PORT\"\nEOF\n</code></pre>","boost":2},{"location":"user-guide/additional-services/valkey/#allow-your-pods-to-communicate-with-the-valkey-cluster","title":"Allow your Pods to communicate with the Valkey Cluster","text":"<p>The Valkey Cluster is protected by Network Policies. Add the following label to your Pods: <code>elastisys.io/valkey-&lt;cluster_name&gt;-access: allow</code></p> <p><code>cluster_name</code> can be retrieved from the ConfigMap provided by your administrator:</p> <pre><code>kubectl -n $NAMESPACE get configmap $CONFIG_MAP -o 'jsonpath={.data.VALKEY_CLUSTER_NAME}'\n</code></pre>","boost":2},{"location":"user-guide/additional-services/valkey/#expose-valkey-connection-parameters-to-your-application","title":"Expose Valkey Connection Parameters to Your Application","text":"<p>To expose the Valkey Cluster to your application, follow one of the following upstream documentation:</p> <ul> <li>Create a Pod that has access to the ConfigMap data through a Volume</li> <li>Define container environment variables using ConfigMap data</li> </ul> <p>Important</p> <p>Make sure to use a Valkey client library with Sentinel support. For example:</p> <ul> <li>Django-Valkey Client that supports Sentinel Cluster HA   If the linked code example doesn't work, try <code>LOCATION: redis://mymaster/db</code>.</li> </ul> <p>If your library doesn't support sentinel you could use this project - Redis sentinel proxy. Note that the default configuration in this repository will not ensure HA for Valkey. For this you'll either need to use multiple replicas or use it as a sidecar for your applications.</p>","boost":2},{"location":"user-guide/additional-services/valkey/#follow-the-go-live-checklist","title":"Follow the Go-Live Checklist","text":"<p>You should be all set. Before going into production, don't forget to go through the go-live checklist.</p>","boost":2},{"location":"user-guide/additional-services/valkey/#welkin-valkey-release-notes","title":"Welkin Valkey Release Notes","text":"<p>Check out the release notes for the Valkey Cluster that runs in Welkin environments!</p>","boost":2},{"location":"user-guide/additional-services/valkey/#best-practices-recommended","title":"Best Practices Recommended","text":"<ul> <li> <p>Eviction Policy: Choose the eviction policy that works for your application. The <code>default</code> eviction policy for our Managed Valkey is <code>allkeys-lru</code>, which means any key can be evicted under memory pressure irrespective of whether the key is expired or not. It will keep the most recently used keys and remove the least recently used (LRU) key.</p> <p>Note</p> <p>Since this is a server setting, it cannot be set by the user itself, but needs to be set by the administrators. Please send a support ticket with the values you would like to set.</p> </li> <li> <p>Set TTL: If possible, take the advantage of expiring keys, such as temporary OAuth authentication keys. When you set the key, set it with some expiration and Valkey will clean up for you. Refer to TTL</p> </li> <li>Avoid expensive or blocking operations: Since Valkey command processing is single-threaded, operations like the KEYS command are expensive and should be avoided. You can avoid <code>KEYS</code> by using SCAN to reduce CPU spikes.</li> <li>Monitor memory usage: Monitor the usage in Grafana dashboard to ensure that you don't run out of memory and have the chance to scale your cache before seeing issues.</li> <li> <p>Manage idle connection: The number of connections to Valkey increases if connections are not properly terminated. This can lead to bad performance. Therefore, we recommend to setting <code>timeout</code> which allows you to set the number of seconds before idle client connections are automatically terminated.   The default <code>timeout</code> for our Managed Valkey is set to <code>0</code>, which means the idle clients do not timeout and remain connected until the client issues the termination.</p> <p>Note</p> <p>Since this is a server setting, it cannot be set by the user itself, but needs to be set by the administrators. Please send a support ticket with the values you would like to set.</p> </li> <li> <p>Cache-hit ratio: You should regularly monitor your <code>cache-hit</code> ratio so that you know what percentage of key lookups are successfully returned by keys in your Valkey instance.   <code>info stats</code> command provides <code>keyspace_hits</code> &amp; <code>keyspace_misses</code> metric data to further calculate cache hit ratio for a running Valkey instance.</p> </li> </ul>","boost":2},{"location":"user-guide/additional-services/valkey/#further-reading","title":"Further Reading","text":"<ul> <li>Valkey Sentinel</li> <li>Sentinel client spec</li> <li>Valkey Commands</li> <li>Kubernetes ConfigMap</li> </ul>","boost":2},{"location":"user-guide/safeguards/","title":"Guardrails","text":"<p>Important</p> <p>In 2021-01-27, the French Data Protection Authority (CNIL) imposed a fine on both the data controller and the data processor for failing to comply with their security obligations. For details, please read this article.</p> <p>Some of these guardrails might be \"inconvenient\" and \"easy to disable\". Faced with tight deadlines, it might be tempting to pressure administrators to disable some of these guardrails.</p> <p>Prefer to keep these guardrails to avoid costly fines. A guardrail should only be disabled if a risk assessment determined that the cost of implementation outweighs the risk to personal data.</p> <p>\"Det ska vara l\u00e4tt att g\u00f6ra r\u00e4tt.\" (English: \"It should be easy to do it right.\")</p> <p>We know you care about the security and uptime of your application. But all that effort goes wasted if the platform allows you to make trivial mistakes.</p> <p>That is why Welkin is built with various guardrails. These are safeguards which allow you to make security and reliability easy for you. Please look through the different pages in this Guardrails section to make yourself familiar with what the guardrails expect from your application.</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"],"boost":2},{"location":"user-guide/safeguards/#relevant-regulations","title":"Relevant Regulations","text":"<ul> <li>GDPR Article 32:</li> </ul> <p>Taking into account the state of the art [...] the controller and the processor shall implement [...] as appropriate [...] a process for regularly testing, assessing and evaluating the effectiveness of technical and organisational measures for ensuring the security of the processing.</p> <p>In assessing the appropriate level of security account shall be taken in particular of the risks that are presented by processing, in particular from accidental or unlawful destruction, loss, alteration, unauthorised disclosure of, or access to personal data transmitted, stored or otherwise processed. [highlights added]</p>","tags":["BSI IT-Grundschutz APP.4.4.A13"],"boost":2},{"location":"user-guide/safeguards/#further-reading","title":"Further Reading","text":"<ul> <li>Introduction to Guardrails and Paved Paths.</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A13"],"boost":2},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/","title":"Default Pod Topology Spread Constraints","text":"","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#default-pod-topology-spread-constraints","title":"Default Pod Topology Spread Constraints","text":"<p>Important</p> <ul> <li>This guardrail is enabled by default as since Welkin Kubespray v2.25.0-ck8s1 and Welkin Cluster API v0.3.0.</li> </ul>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#problem","title":"Problem","text":"<p>A healthy security posture requires you to ensure your application tolerates failures. This involves two things:</p> <ol> <li>Running your application replicated with at least two Pods. Specifically, this implies that your Deployment has <code>.spec.replicas</code> of at least 2.</li> <li>Ensuring that the Pods are spread across failure domains. The latter was usually achieved by setting correct <code>topologySpreadConstraints</code>.</li> </ol> <p>In Welkin, dealing with (1) above is still the Application Developer's responsibility.</p> <p>However, with Welkin, you don't need to deal with (2).</p>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#solution","title":"Solution","text":"<p>Welkin comes with strong Cluster-level default <code>topologySpreadConstraints</code>.</p>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#single-zone-clusters","title":"Single-Zone Clusters","text":"<p>If your Cluster is hosted on a single zone, then your administrator will have configured the following default <code>topologySpreadConstraints</code>:</p> <pre><code>- maxSkew: 1\n  topologyKey: kubernetes.io/hostname\n  whenUnsatisfiable: ScheduleAnyway\n</code></pre> <p>This means that the Kubernetes scheduler will try to spread Pods of the same Deployment across Nodes. If this is not possible, it will still try to run the Pod on any Node.</p> <p>This implies that your application is more likely to tolerate a Node going down.</p>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#multi-zone-clusters","title":"Multi-Zone Clusters","text":"<p>If your Cluster is hosted on at least three zones in the same region, then your administrator will have configured the following default <code>topologySpreadConstraints</code>:</p> <pre><code>- maxSkew: 1\n  topologyKey: topology.kubernetes.io/zone\n  whenUnsatisfiable: ScheduleAnyway\n</code></pre> <p>This means that the Kubernetes scheduler will try to spread Pods of the same Deployment across zones. If this is not possible, it will still try to run the Pod on any zone.</p> <p>This implies that your application is more likely to tolerate a zone going down.</p>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#what-if-i-need-to-customize-my-topologyspreadconstraints","title":"What if I need to customize my <code>topologySpreadConstraints</code>?","text":"<p>Simply override this in your application Helm Chart. The user demo provides an example on how to achieve this.</p>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/default-pod-topology-spread-constraints/#further-reading","title":"Further Reading","text":"<ul> <li>Cluster-level default constraints</li> </ul>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"]},{"location":"user-guide/safeguards/enforce-job-ttl/","title":"Improve platform stability: Job TTL","text":"<p>Important</p> <ul> <li>This guardrail is enabled by default and will mutate violations. As a result, resources that violate this policy will be modified with a default config so that it does follow the policy.</li> </ul> <p>In Kubernetes, Jobs that are not managed by a higher-level resource such as a Cronjob, will most likely not get cleaned up automatically as Jobs do not have a default time-to-live, TTL, configured. In worst case the number of finished jobs could accumulate to such a volume that it might impact the stability of the Kubernetes Cluster.</p> <p>However, by default in Welkin, there is a policy that demands Jobs to have a TTL. Jobs that do not explicitly set a TTL (<code>spec.ttlSecondsAfterFinished</code>) will automatically get a TTL of 7 days.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-job-ttl/#further-reading","title":"Further Reading","text":"<ul> <li>Jobs</li> </ul>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-minimum-replicas/","title":"Avoid Downtime with Replicas","text":"<p>Important</p> <ul> <li>This guardrail is enabled by default and will warn on violations. As a result, resources that violate this policy will generate warning messages, but will still be created.</li> </ul> <p>Welkin by default recommends a minimum of 2 replicas for Deployments and StatefulSets.</p> <p>Therefore a warning will be issued when you add or update a Deployment or StatefulSet where the number of replicas is less than 2.</p> <pre><code>[elastisys-warn-minimum-replicas] The provided number of replicas is low for Deployment: demo-welkin-user-demo. Welkin recommends a minimum of 2 replicas.\n</code></pre> <p>You can either resolve this by setting the number of replicas to a minimum of 2.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-demo\nspec:\n  replicas: 2\n</code></pre> <p>Or, in the case where the Deployment or StatefulSet is not of high priority or there are technical limitations preventing you from using more than one replica, you can suppress the warning by adding an annotation.</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: user-demo\n  annotations:\n    elastisys.io/ignore-minimum-replicas: \"yes\" # The value part can be anything.\nspec:\n  replicas: 1\n</code></pre>","tags":["ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities"],"boost":2},{"location":"user-guide/safeguards/enforce-networkpolicies/","title":"Reduce blast radius: NetworkPolicies","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.13.1.1 Network Controls</li> <li>A.13.1.2 Security of Network Services</li> <li>A.13.1.3 Segregation in Networks</li> </ul> <p>Important</p> <ul> <li>This guardrail is enabled by default and will warn on violations. As a result, resources that violate this policy will generate warning messages, but will still be created.</li> </ul> <p>NetworkPolicies are useful in two cases: segregating tenants hosted in the same environment and further segregating application components. Both help you achieve better data protection.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"],"boost":2},{"location":"user-guide/safeguards/enforce-networkpolicies/#segregating-tenants-hosted-in-the-same-environment","title":"Segregating tenants hosted in the same environment","text":"<p>Say you want to host a separate instance of your application for each tenant. For example, your end-users may belong to different -- potentially competing -- organizations, and you promised them to take extra care of not mixing their data. Say you want to reduce complexity by hosting all tenants inside the same environment, but without compromising data protection.</p> <p>Each application instance could be installed as a separate Helm Release, perhaps even in its own Namespace. These instances should be segregated from other application instances using NetworkPolicies. This ensures that network traffic from one application instance cannot reach another application instance. Besides reducing attack surface, it also prevents embarrassing mistakes, like connecting one application to the database of another.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"],"boost":2},{"location":"user-guide/safeguards/enforce-networkpolicies/#further-segregation-of-application-components","title":"Further segregation of application components","text":"<p>If you run several applications -- e.g., frontend, backend, back office, database, message queue -- in a single Kubernetes Cluster, it is a best practice to segregate them. By segregating your applications and only allowing required Ingress and egress network traffic, you further reduce blast radius in case of an attack.</p>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"],"boost":2},{"location":"user-guide/safeguards/enforce-networkpolicies/#welkin-helps-enforce-segregation","title":"Welkin helps enforce segregation","text":"<p>Welkin allows you to segregate applications by installing suitable NetworkPolicies. These are a bit like firewalls, but in the container world: Since containers are supposed to be deleted and recreated frequently, they change IP address a lot. Clearly the old \"allow/deny IP\" method does not scale. Therefore, NetworkPolicies select source and destination Pods based on labels or namespace labels.</p> <p>To make sure you don't forget to configure NetworkPolicies, the administrator can configure Welkin to deny creation of Pods with no matching NetworkPolicies.</p> <p>If you get the following error:</p> <pre><code>Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by elastisys-require-networkpolicy] No matching networkpolicy found\n</code></pre> <p>Then you are missing NetworkPolicies which select your Pods. The user demo gives a good example to get you started.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8srequirenetworkpolicy.constraints.gatekeeper.sh elastisys-require-networkpolicy -ojson | jq .status.violations\n</code></pre>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"],"boost":2},{"location":"user-guide/safeguards/enforce-networkpolicies/#further-reading","title":"Further Reading","text":"<ul> <li>NetworkPolicies</li> </ul>","tags":["BSI IT-Grundschutz APP.4.4.A7","BSI IT-Grundschutz APP.4.4.A18","ISO 27001 Annex A 8.12 Data Leakage Prevention","ISO 27001 Annex A 8.20 Networks Security","ISO 27001 Annex A 8.21 Security of Network Services","ISO 27001 Annex A 8.22 Segregation of Networks"],"boost":2},{"location":"user-guide/safeguards/enforce-no-latest-tag/","title":"Avoid unexpected changes: disallowed tags","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.1.2 Change Management</li> <li>A.14.2.2 System Change Control Procedures</li> <li>A.14.2.4 Restrictions on Changes to Software Packages</li> </ul> <p>Important</p> <p>This guardrail is enabled by default and will deny violations. As a result, resources that violate this policy will not be created.</p> <p>Using the <code>:latest</code> tag can lead to inconsistent deployments, where it is difficult to rollback. In Welkin we suggest using explicit tags for your container images. This way you know that image version <code>v1.0.0</code> will be deployed if you are using the <code>:v1.0.0</code> tag.</p>","tags":["ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/safeguards/enforce-no-latest-tag/#how-to-solve-container-image-must-not-have-disallowed-tags","title":"How to solve: [container-image-must-not-have-disallowed-tags]","text":"<p>You may encounter the following issue:</p> <pre><code>Error from server ([container-image-must-not-have-disallowed-tags] container &lt;example-container&gt; uses a disallowed tag &lt;harbor.$DOMAIN/$REGISTRY_PROJECT/example-container:latest&gt;; disallowed tags are [\"latest\"])\n</code></pre> <p>This means that you are not allowed to use the <code>:latest</code> tag on your images. If no tag is specified, Kubernetes assumes <code>:latest</code>, but that does not mean that the most recent version of the image will actually be used. <code>:latest</code> is just a tag and is not dynamically updated to the most recent version of the image. It also becomes difficult to track which version of the image was used if you were to do a rollback.</p> <p>To fix this, you have to start specifying tags for your images i.e. <code>v1.0.0</code>.</p> <p>We recommend that you treat all tags as immutable, i.e. you don't use tags like <code>dev</code> or <code>prod</code> that you intend to continue to push tags to. Do this by always creating a new tag when you push a new image, you can include some versioning or a hash to make the tag unique. E.g. <code>prod-v1.2.3</code> or <code>dev-f6451806e5b6</code>. If you do not treat tags as immutable, then you get the same risk of having inconsistent deployments.</p> <p>It is possible to add more tags that will be denied. Perhaps you want to deny images with <code>dev</code> tags in your production environment. If you want to add more tags that should be denied, then contact your administrator.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8sdisallowedtags.constraints.gatekeeper.sh container-image-must-not-have-disallowed-tags -ojson | jq .status.violations\n</code></pre>","tags":["ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/safeguards/enforce-no-latest-tag/#further-reading","title":"Further Reading","text":"<ul> <li>Images</li> </ul>","tags":["ISO 27001 Annex A 8.32 Change Management"],"boost":2},{"location":"user-guide/safeguards/enforce-no-load-balancer-service/","title":"Avoid deploying a Load Balancer Service on unsupported cloud providers","text":"<p>Important</p> <p>This guardrail is enabled by default for some infrastructure providers and will deny violations. As a result, on those specific infrastructure providers resources that violate this policy will not be created.</p> <p>Some infrastructure providers do not support <code>Service</code> of <code>type: LoadBalancer</code>, e.g. because the load-balancers don't integrate with Kubernetes. In such cases your administrator will deploy a policy to prevent such Services from being deployed.</p> <p>When attempting to apply a <code>Service</code> with <code>type: LoadBalancer</code> anyway, the following error message would be returned:</p> <pre><code>Creation of LoadBalancer Service is not supported.\nContact your platform administrator for questions about Load Balancers.\n</code></pre> <pre><code>apiVersion: v1\nkind: Service\nmetadata:\n  name: my-service\nspec:\n  selector:\n    app.kubernetes.io/name: MyApp\n  type: LoadBalancer\n  ports:\n    - protocol: TCP\n      port: 80\n      targetPort: 9376\n</code></pre>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-no-load-balancer-service/#how-to-solve","title":"How to solve","text":"<p>Consult your platform documentation for how to handle load balancing.</p> <ul> <li>Service type: LoadBalancer</li> </ul>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-no-local-storage-emptydir/","title":"Enforce No Local Storage EmptyDir","text":"","boost":2},{"location":"user-guide/safeguards/enforce-no-local-storage-emptydir/#avoid-cluster-autoscaler-failing-to-scale-down-nodes-due-to-local-storage-emptydir-usage","title":"Avoid Cluster autoscaler failing to scale down Nodes due to local storage emptydir usage","text":"<p>Important</p> <p>This guardrail is enabled by default and will warn on violations, but only on clusters with cluster autoscaling. As a result, resources that violate this policy will generate warning messages, but will still be created.</p>","boost":2},{"location":"user-guide/safeguards/enforce-no-local-storage-emptydir/#problem","title":"Problem","text":"<p>The Cluster autoscaler can manage the amount of Nodes in the Cluster by creating or removing Nodes based on resources requested by Pods. It will scale down Nodes once there is a Node that is not needed anymore. However, there are some scenarios where the autoscaler will not remove certain Nodes. One such scenario is if there is a Pod running on the Node that is using local storage, since that Pod would lose state if it was moved to a different Node. In Welkin this is primarily Pods that are using local storage emptyDir volumes.</p> <p>Since Nodes in Kubernetes often have limited disk space, using empty dir volumes with local storage can also be disruptive for the Node if the Pod is using a larger amount of disk space. So you should only use emptyDir volumes if you need a small amount of storage. If you need a moderate or large amount of storage, then consider using PersistentVolumes instead.</p>","boost":2},{"location":"user-guide/safeguards/enforce-no-local-storage-emptydir/#solution","title":"Solution","text":"<p>There are a few different solutions to avoid the scenario stated above:</p> <ol> <li>Use emptyDir volume with <code>medium: Memory</code> instead, the Cluster autoscaler does not consider this as local storage and using this will not prevent the Pod from being evicted by the autoscaler.     Note that this will instead use the RAM memory of the Node, instead of disk space.     Also note that this is still ephemeral storage and the data in it will be permanently deleted if the Pod is removed from the Node.</li> <li>Use a permanent form of storage that is not bound to a Node.     Most likely this would be a PersistentVolume, which can be automatically provisioned by creating a PersistentVolumeClaim and binding that to the Pod.</li> <li>If it is ok that the Pod is deleted by the autoscaler even though it is using local storage.     Then you can annotate the Pod with <code>\"cluster-autoscaler.kubernetes.io/safe-to-evict-local-volumes\": \"volume-1,volume-2,..\"</code>, where the value of the annotation is a list that would include the emptyDir volumes.     Alternatively you can annotate the Pod with <code>\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"</code> to note that the Pod is safe to evict in general, this would also allow the autoscaler to delete the Pod even if it is also violating some of the other rules that normally prevent the autoscaler from deleting Pods.</li> </ol>","boost":2},{"location":"user-guide/safeguards/enforce-no-local-storage-emptydir/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>To make sure you don't create Pods that prevent the autoscaler from deleting Nodes, the administrator can configure Welkin to deny creation of Pods that would prevent the autoscaler.</p> <p>If you get the following error:</p> <pre><code>Error: UPGRADE FAILED: failed to create resource: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by elastisys-reject-local-storage-emptydir] The volume \"emptydir-volume\" emptyDir is using local storage emptyDir. This can prevent autoscaler from scaling down a node where this is running. Read more about this and possible solutions at &lt;link-to-public-documentation&gt;\n</code></pre> <p>Then your Pod have an emptyDir volume that would prevent the autoscaler from scaling down a Node.</p> <p>Note that this is only a problem if the Pod will be scheduled on a Node that is subject to autoscaling. However, this policy does not know where the Pod would be scheduled, so it will trigger even if the Pod would not be scheduled on an Node subject to autoscaling. If you are certain that the Pod will not be scheduled on a Node subject to autoscaling, then you can safely add one of the annotations mentioned in the solutions above.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8srejectemptydir.constraints.gatekeeper.sh elastisys-reject-local-storage-emptydir -ojson | jq .status.violations\n</code></pre>","boost":2},{"location":"user-guide/safeguards/enforce-no-pod-without-controller/","title":"Enforce No Pod Without Controller","text":"","boost":2},{"location":"user-guide/safeguards/enforce-no-pod-without-controller/#avoid-cluster-autoscaler-failing-to-scale-down-nodes-due-to-usage-of-pods-without-any-backing-controller","title":"Avoid Cluster autoscaler failing to scale down Nodes due to usage of Pods without any backing controller","text":"<p>Important</p> <p>This guardrail is enabled by default and will warn on violations, but only on clusters with cluster autoscaling. As a result, resources that violate this policy will generate warning messages, but will still be created.</p>","boost":2},{"location":"user-guide/safeguards/enforce-no-pod-without-controller/#problem","title":"Problem","text":"<p>The Cluster autoscaler can manage the amount of Nodes in the Cluster by creating or removing Nodes based on resources requested by Pods. It will scale down Nodes once there is a Node that is not needed anymore. However, there are some scenarios where the autoscaler will not remove certain Nodes. One such scenario is if there is a Pod running on the Node that is not backed by a controller object (such as a Deployment or StatefulSet).</p> <p>A Pod that is running without any controller object backing it is also not resilient, since if that Pod is deleted, it will not be re-created. This could be ok for temporary Pods, but in general it should be avoided.</p>","boost":2},{"location":"user-guide/safeguards/enforce-no-pod-without-controller/#solution","title":"Solution","text":"<p>There are a few different solutions to avoid the scenario stated above:</p> <ol> <li>Create Pods using some sort of controller. Such as Deployment, ReplicaSet, StatefulSet, DaemonSet, Job, or CronJob.</li> <li>If it is ok that the Pod is deleted by the autoscaler even though it is not backed by a controller.     Then you can annotate the Pod with <code>\"cluster-autoscaler.kubernetes.io/safe-to-evict\": \"true\"</code> to note that the Pod is safe to evict in general, this also allows the autoscaler to delete the Pod even if it is also violating some of the other rules that normally prevent the autoscaler from deleting Pods.</li> </ol>","boost":2},{"location":"user-guide/safeguards/enforce-no-pod-without-controller/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>To make sure you don't create Pods that prevent the autoscaler from deleting Nodes, the administrator can configure Welkin to deny creation of Pods that would prevent the autoscaler.</p> <p>If you get the following error:</p> <pre><code>Error: UPGRADE FAILED: failed to create resource: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by elastisys-reject-pod-without-controller] The Pod \"Pod-without-controller\" does not have any ownerReferences. This can prevent autoscaler from scaling down a node where this is running. Read more about this and possible solutions at &lt;link-to-public-documentation&gt;\n</code></pre> <p>Then your Pod does not have a backing controller, which would prevent the autoscaler from scaling down a Node.</p> <p>Note that this is only a problem if the Pod will be scheduled on a Node that is subject to autoscaling. However, this policy does not know where the Pod would be scheduled, so it will trigger even if the Pod would not be scheduled on an Node subject to autoscaling. If you are certain that the Pod will not be scheduled on a Node subject to autoscaling, then you can safely add the annotation mentioned in the solutions above.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8srejectpodwithoutcontroller.constraints.gatekeeper.sh elastisys-reject-pod-without-controller -ojson | jq .status.violations\n</code></pre>","boost":2},{"location":"user-guide/safeguards/enforce-no-root/","title":"Reduce blast radius: Preventing forgotten roots","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.9.4.4 Use of Privileged Utility Programmes</li> <li>A.12.6.1 Management of Technical Vulnerabilities</li> <li>A.14.2.5 Secure System Engineering Principles</li> </ul> <p>Important</p> <p>This guardrail is enabled by default and will deny violations. As a result, resources that violate this policy will not be created.</p> <p>Many container runtimes and operating system vulnerabilities need code running as root to become a threat. To minimize this risk, application should only run as root when strictly necessary.</p> <p>Unfortunately, many Dockerfiles -- and container base images -- today are shipped running as root by default. This makes it easy to slip code running as root into production, exposing data to unnecessary risks.</p> <p>To reduce blast radius, Welkin will protect you from accidentally deploying application running as root.</p>","tags":["NIST SP 800-171 3.1.7","ISO 27001 Annex A 8.18 Use of Privileged Utility Programs"],"boost":2},{"location":"user-guide/safeguards/enforce-no-root/#how-to-solve-createcontainerconfigerror","title":"How to solve: CreateContainerConfigError","text":"<p>You may encounter the following issue:</p> <pre><code>$ kubectl get pods\nNAME                                   READY   STATUS                       RESTARTS   AGE\nmyapp-welkin-user-demo-564f8dd85-2bs8r   0/1     CreateContainerConfigError   0          84s\nmyapp-welkin-user-demo-bfbf9c459-dmk4l   0/1     CreateContainerConfigError   0          13m\n$ kubectl describe pods myapp-welkin-user-demo-564f8dd85-2bs8r\n[...]\nError: container has runAsNonRoot and image has non-numeric user (node), cannot verify user is non-root (pod: \"myapp-welkin-user-demo-bfbf9c459-dmk4l_demo1(1b53b1a8-4845-4db5-aecf-6bebcc54e396)\", container: welkin-user-demo)\n</code></pre> <p>This means that your Dockerfile uses a non-numeric user and Kubernetes cannot validate whether the image truly runs as non-root.</p> <p>Alternatively, you may get:</p> <pre><code>$ kubectl describe pods myapp-welkin-user-demo-564f8dd85-2bs8r\n[...]\nError: container has runAsNonRoot and image will run as root (pod: \"myapp-welkin-user-demo-564f8dd85-2bs8r_demo1(a55a25f3-7b77-4fae-9f92-11e264446ecc)\", container: welkin-user-demo)\n</code></pre> <p>This means that your Dockerfile has no <code>USER</code> directive and your application would run as root.</p> <p>To ensure your application does not run as root, you have two options:</p> <ol> <li>Change the Dockerfile to <code>USER 1000</code> or whatever numeric ID corresponds to your user. This is what the user demo does.</li> <li> <p>Add the following snippet to the <code>spec</code> of your Pod manifest:</p> <pre><code>securityContext:\n  runAsUser: 1000\n</code></pre> </li> </ol> <p>If possible, prefer changing the Dockerfile, to ensure your application runs as non-root not only in production, but also during development and testing. The smaller the difference between development, testing and production, the fewer surprises down the time.</p>","tags":["NIST SP 800-171 3.1.7","ISO 27001 Annex A 8.18 Use of Privileged Utility Programs"],"boost":2},{"location":"user-guide/safeguards/enforce-no-root/#further-reading","title":"Further Reading","text":"<ul> <li>Dockerfile USER</li> <li>SecurityContext</li> </ul>","tags":["NIST SP 800-171 3.1.7","ISO 27001 Annex A 8.18 Use of Privileged Utility Programs"],"boost":2},{"location":"user-guide/safeguards/enforce-podsecuritypolicies/","title":"Reduce blast radius: Enforcing restricted privileges","text":"<p>Important</p> <ul> <li>This group of guardrails are enabled by default and will deny or mutate on violations. As a result, resources that violate this policy will either not be created or they will be mutated to conform to the policy, depending on the type of violation.</li> </ul> <p>This page helps you understand why warnings are emitted when deploying workloads similar to:</p> <pre><code>Warning: would violate PodSecurity \"restricted:latest\":\n    allowPrivilegeEscalation != false (container \"&lt;container-name&gt;\" must set securityContext.allowPrivilegeEscalation=false),\n    unrestricted capabilities (container \"&lt;container-name&gt;\" must set securityContext.capabilities.drop=[\"ALL\"]),\n    runAsNonRoot != true (container \"&lt;container-name&gt;\" must not set securityContext.runAsNonRoot=false),\n    seccompProfile (pod or container \"&lt;container-name&gt;\" must set securityContext.seccompProfile.type to \"RuntimeDefault\" or \"Localhost\")\n</code></pre> <p>Additionally, why Pods are not scheduled and events are emitted from workloads similar to:</p> <pre><code>$ kubectl -n &lt;namespace&gt; get events\n...\n&lt;time&gt;    Warning    FailedCreate    replicaset/&lt;replicaset-name&gt;    Error creating: pods \"&lt;pod-name&gt;\" is forbidden: violates PodSecurity \"restricted:latest\": runAsNonRoot != true (container \"&lt;container-name&gt;\" must not set securityContext.runAsNonRoot=false)\n...\n</code></pre> <p>For Welkin Managed Customers</p> <p>These restrictions are put in place to protect your data. They are meant to help you comply with GDPR. For more details, please read our Terms of Service, specifically:</p> <ul> <li>ToS A1.3 Security Measures</li> <li>ToS A2.3 Safeguards.</li> </ul> <p>If any of these restrictions causes friction when deploying your application, please file a service ticket and we'll happily advise you on how to reduce privileges required by your application.</p> <p>Kubernetes by default allows any Pod to run with any privileges it requests, which easily allows an application to take full control over a Cluster and everything in it. To minimise this risk Welkin employs two systems to restrict what privileges an application can request:</p> <ul> <li>Kubernetes - Pod Security Admission (PSA)<ul> <li>Coarse-grained enforcement built into Kubernetes</li> </ul> </li> <li>Open Policy Agent Gatekeeper - Pod Security Policies (PSP)<ul> <li>Fine-grained enforcement built with OPA Gatekeeper</li> </ul> </li> </ul> <p>In addition to enforcement Welkin also employ OPA Gatekeeper mutations to modify security contexts of applications to make it easier to comply with the enforced rules. This modification happens at the stage when Pods are created, which means that their security context may contain additional content compared to the resource they were created for. Only fields that are unset can be modified in this way.</p> <p>Warning</p> <p>This means that Kubernetes may warn on workloads that may be permitted based on the results of the mutations. Example of this will follow below.</p> <p>Note</p> <p>This model is used to emulate the behaviour of Kubernetes own Pod Security Policies which have been deprecated and removed in favour of Pod Security Admission.</p> <p>One limitation with the OPA Gatekeeper constraints and mutations is that they target resource only based on labels, in contrast to the old Pod Security Policies which gave access to additional permissions through Kubernetes RBAC.</p>","tags":["NIST SP 800-171 3.1.7"],"boost":2},{"location":"user-guide/safeguards/enforce-podsecuritypolicies/#restricted-pod-security-standard","title":"Restricted Pod Security Standard","text":"<p>The default enforcement in Welkin follows the upstream Restricted Pod Security Standard as defined by Kubernetes. This standard includes the following:</p> <ul> <li>Escalation and privileged mode are disallowed.</li> </ul> <p>Usually applications don't need this unless they need to have low-level access to the Nodes to access and manage hardware.</p> <ul> <li>Host namespaces, host networks, host ports, and host paths are disallowed.</li> </ul> <p>Usually applications don't need this unless they need to have low-level access to the Nodes to access and manage system resources.</p> <ul> <li>Running with the <code>seccomp</code> profile <code>Localhost</code> or <code>RuntimeDefault</code> (set by default).</li> </ul> <p>This restricts the system call applications can make, the <code>RuntimeDefault</code> profile is provided by containerd, the container runtime, with sane defaults that should not be an issue for most applications.</p> <ul> <li>Running as non root user (with a non-zero UID).</li> </ul> <p>Recommended is to run a high UID and GID over 10000 that doesn't match with other running software.</p> <ul> <li>Running with the <code>NET_BIND_SERVICE</code> capability added and <code>ALL</code> capabilities dropped (set by default).</li> </ul> <p>Usually applications don't need any capabilities unless they need to have low-level access to the Nodes to access and manage system resources. The <code>NET_BIND_SERVICE</code> is an exception that allows processes to bind ports under 1024.</p> <ul> <li> <p>Running with the following volume types:</p> <ul> <li><code>configMap</code></li> <li><code>csi</code></li> <li><code>downwardAPI</code></li> <li><code>emptyDir</code></li> <li><code>ephemeral</code></li> <li><code>persistentVolumeClaim</code></li> <li><code>projected</code></li> <li><code>secret</code></li> </ul> </li> </ul> <p>Warning</p> <p>This standard only enforces that <code>runAsNonRoot</code> is set to <code>true</code>, one must still either configure a numerical user in the container image or with <code>runAsUser</code> that is non-zero for the Pod to be allowed as described in Enforce No Root.</p> <p>Note</p> <p>This standard does not enforce <code>fsGroup</code>, <code>runAsGroup</code> and <code>supplementalGroups</code> to be non-zero, however these will be set to <code>1</code> by default.</p> Example of a minimal Pod template and the resulting mutated Pod spec <p>This example of a minimal Pod template does not conform to the Restricted Pod Security Standard, and will generate warnings as they are applied. <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  ...\nspec:\n  templates:\n    spec:\n      containers:\n        - name: nginx\n          ...\n          securityContext:\n            runAsUser: 1000 # May be skipped if the Contatinerfile / Dockerfile contains the USER directive with numerical user\n    ...\n    securityContext: {}\n    ...\n</code></pre></p> <p>However with the help of mutations the template will turn into this Pod spec that conforms to the restricted standard:</p> <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  ...\nspec:\n  containers:\n    - name: nginx\n      ...\n      securityContext:\n        allowPrivilegeEscalation: false\n        privileged: false\n        runAsNonRoot: true\n        runAsUser: 1000\n        runAsGroup: 1\n        seccompProfile:\n          type: RuntimeDefault\n        capabilities:\n          drop:\n            - ALL\n      ...\n  ...\n  hostIPC: false\n  hostPID: false\n  securityContext:\n    fsGroup: 1\n    supplementalGroups:\n      - 1\n  ...\n</code></pre> <p>It is still recommended that non-conforming workloads are updated to conform to the restricted standard and with the minimal set of privileges it requires.</p>","tags":["NIST SP 800-171 3.1.7"],"boost":2},{"location":"user-guide/safeguards/enforce-podsecuritypolicies/#custom-pod-security-policy","title":"Custom Pod Security Policy","text":"<p>Certain applications may need more privileges than what is allowed from the restricted standard, and platform administrators may configure custom policies to allow application developers more freedom to applications in certain namespaces.</p> <p>To do so application developers should put together a Pod Security Policy to be evaluated and accepted by the platform administrator, which then can allow access to these privileges in a certain namespace for resources with a certain label. The format should be as follows:</p> <p>For Welkin Managed Customers</p> <p>Please file a service ticket and we will review and discuss your request for increased privileges. We will try to find a solution that works for you, but there is no guarantee that your request will be approved.</p> <pre><code>podSelectorLabels: # Must be provided\n  &lt;key&gt;: &lt;value&gt;\n  ...\nallow:\n  allowPrivilegeEscalation: &lt;boolean&gt; # Default false\n  privileged: &lt;boolean&gt; # Default false\n  allowedCapabilities: # Default empty\n    - &lt;linux capability&gt;\n    - ...\n  allowedUnsafeSysctls: # Default empty (1)\n    - &lt;unsafe sysctl&gt;\n    - ...\n  hostNetworkPorts: &lt;boolean&gt; # Default false (2)\n  hostNamespace: &lt;boolean&gt; # Default false\n  allowedHostPaths: # Default empty\n    - pathPrefix: &lt;hostpath&gt;\n      readOnly: &lt;boolean&gt;\n    - ...\n  runAsUser: MustRunAsNonRoot | RunAsAny  # Default MustRunAsNonRoot\n  runAsGroup:\n    rule: MustRunAs | RunAsAny # Default RunAsAny\n    ranges: # Only required with MustRunAs\n      - max: &lt;GID&gt;\n        min: &lt;GID&gt;\n      - ...\n  fsGroup:\n    rule: MustRunAs | RunAsAny # Default RunAsAny\n    ranges: # Only required with MustRunAs\n      - max: &lt;GID&gt;\n        min: &lt;GID&gt;\n      - ...\n  supplementalGroups:\n    rule: MustRunAs | RunAsAny # Default RunAsAny\n    ranges: # Only required with MustRunAs\n      - max: &lt;GID&gt;\n        min: &lt;GID&gt;\n      - ...\n  volumes: # Default [ configMap, downwardAPI, emptyDir, persistentVolumeClaim, projected, secret ]\n    - &lt;volume-type&gt;\n    - ...\nmutations:\n  dropAllCapabilities: &lt;boolean&gt; # Default true\n  setDefaultSeccompProfile: &lt;boolean&gt; # Default true\n  runAsUser: &lt;UID&gt; # Default none (3)\n  runAsGroup: &lt;GID&gt; # Default 1\n  fsGroup: &lt;GID&gt; # Default 1\n</code></pre> <ol> <li>Sysctls may still be denied.</li> <li>Allows both host network and host ports.</li> <li>Must be configured in the container image or security context.</li> </ol> <p>Danger</p> <p>Custom Pod Security Policies opens the platform up for potential security threats and should be as restrictive as possible and to a minimum to safeguard the security of the platform!</p> Example of a Pod manifest with higher privileges and associated custom Pod Security Policy <p>This Pod manifest for an application that would be capable of modifying routes on nodes: <pre><code>apiVersion: v1\nkind: Pod\nmetadata:\n  labels:\n    app.kubernetes.io/name: route-manager\n  name: route-manager\nspec:\n  containers:\n    - name: manager\n      ...\n      securityContext:\n        allowPrivilegeEscalation: true # (1)!\n        runAsNonRoot: true\n        runAsUser: 400\n        capabilities:\n          add:\n            - NET_ADMIN\n  ...\n  hostNetwork: true\n  ...\n</code></pre></p> <ol> <li>Required when adding privileges that exceed the container runtime, in this case the <code>NET_ADMIN</code> capability.</li> </ol> <p>Would translate into this Pod Security Policy: <pre><code>podSelectorLabels:\n  app.kubernetes.io/name: route-manager\nallow:\n  allowPrivilegeEscalation: true\n  allowedCapabilities:\n    - NET_ADMIN\n  hostNetworkPorts: true\n  runAsUser:\n  rule: MustRunAs\n  ranges:\n    - max: 400\n      min: 400\n</code></pre></p>","tags":["NIST SP 800-171 3.1.7"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/","title":"Enforce Resources","text":"","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/#avoid-downtime-with-resource-requests-and-limits","title":"Avoid downtime with Resource Requests and Limits","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.1.3 Capacity Management</li> </ul> <p>Important</p> <p>This guardrail is enabled by default and will deny violations. As a result, resources that violate this policy will not be created.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/#problem","title":"Problem","text":"<p>A major source of application downtime is insufficient capacity. For example, if a Node reaches 100% CPU utilization, then application Pods hosted on it will run slow, leading to bad end-user experience. If a Node runs into memory pressure, the application will run slower, as less memory is available for the page cache. High memory pressure may lead to the Node triggering the infamous Out-of-Memory (OOM) Killer, killing a victim, either your application or a platform component.</p>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/#solution","title":"Solution","text":"<p>To avoid running into capacity issues, Kubernetes allows Pods to specify resource requests and limits for each of its containers. This achieves two benefits:</p> <ol> <li>It ensures that Pods are scheduled to Nodes that have the requested resources.</li> <li>It ensures that a Pod does not exceed its resource limits, hence limiting its blast radius and protecting other application or platform Pods.</li> </ol>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>To make sure you don't forget to configure resource requests and limits, the administrator can configure Welkin to deny creation of Pods without explicit resource specifications.</p> <p>If you get the following error:</p> <pre><code>Error: UPGRADE FAILED: failed to create resource: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-resource-requests] Container \"welkin-user-demo\" has no resource requests\n</code></pre> <p>Then you are missing resource requests for some containers of your Pods. The user demo gives a good example to get you started.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8sresourcerequests.constraints.gatekeeper.sh require-resource-requests -ojson | jq .status.violations\n</code></pre>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-resources/#further-reading","title":"Further Reading","text":"<ul> <li>Managing Resources for Containers</li> </ul>","tags":["ISO 27001 Annex A 8.6 Capacity Management"],"boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/","title":"Enforce Maintenance-Friendly PodDisruptionBudgets","text":"","boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/#avoid-installing-poddisruptionbudgets-which-prevent-maintenance-and-security-patches","title":"Avoid Installing PodDisruptionBudgets which Prevent Maintenance and Security Patches","text":"<p>Important</p> <p>This guardrail is enabled by default and will deny violations. As a result, resources that violate this policy will not be created.</p>","boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/#problem","title":"Problem","text":"<p>PodDisruptionBudgets is a tool that can ensure your application does not suffer from too much disruption during normal maintenance operations. PodDisruptionBudgets work by limiting how many Pods in a given Deployment or other Pod controller (StatefulSet, ReplicaSet, or ReplicationController) can be evicted at the same time, when a platform administrator drains a Node. Platform administrators typically drain Nodes for maintenance purposes like restarting and upgrading Nodes or for removing and replacing Nodes, some of these actions might be done automatically by different tools.</p> <p>When configured correctly PodDisruptionBudgets can be a good tool to collaborate with your platform administrators. But it is possible to misconfigure them in a way that prevents or hinders platform administrators from draining Nodes. If you create a PodDisruptionBudget that does not allow for any disruptions, then draining Nodes with a matching Pod will fail unless the platform administrator manually kills the Pod.</p>","boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/#solution","title":"Solution","text":"<p>To solve this problem you need to ensure that all PodDisruptionBudgets allow for at least one Pod disruption at a time.</p> <ul> <li>For PodDisruptionBudgets with <code>maxUnavailable</code> you need to set that option to anything above <code>0</code> or <code>0%</code>.</li> <li>For PodDisruptionBudgets with <code>minAvailable</code> you need to set that option to anything lower than the number of replicas in the Pod controller.<ul> <li>For percentages the <code>minAvailable</code> is rounded up to nearest integer. E.g. if the number of replicas for a Deployment is <code>4</code>, then any percentage from <code>1%</code> to <code>25%</code>would evaluate to require at least 1 available replica. In the same example you would then not want to set the percentage to <code>76%</code> or higher, since that would require all 4 replicas to be available, i.e. it would not allow for any Pod disruptions.</li> </ul> </li> </ul>","boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>To make sure you don't create PodDisruptionBudgets that does not allow for Pod disruptions, the administrator can configure Welkin to deny creation of PodDisruptionBudgets and Pod controllers that does not allow for Pod disruptions. So you might have both PodDisruptionBudgets and Pod controllers be denied by this policy.</p> <p>If you get the following error:</p> <pre><code>Error from server (Forbidden): error when creating \"pdb.yaml\": admission webhook \"validation.gatekeeper.sh\" denied the request: [elastisys-restrict-pod-disruption-budgets] PodDisruptionBudget rejected: Deployment &lt;test-deployment&gt; has 4 replica(s) but PodDisruptionBudget &lt;test-pdb&gt; has minAvailable of 4, minAvailable should always be lower than replica(s), and not used when replica(s) is set to 1. Read more about this and possible solutions at &lt;link-to-public-documentation&gt;\n</code></pre> <p>Then your PodDisruptionBudget and Pod controller does not allow for any Pod disruption.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8srestrictpoddisruptionbudgets.constraints.gatekeeper.sh elastisys-restrict-pod-disruption-budgets -ojson | jq .status.violations\n</code></pre>","boost":2},{"location":"user-guide/safeguards/enforce-restricted-pod-disruption-budgets/#further-reading","title":"Further reading","text":"<p>You can read more about Pod disruption and PodDisruptionBudgets in the upstream documentation for Kubernetes.</p> <ul> <li>Pod disruptions</li> <li>Specifying a Disruption Budget for your Application</li> </ul>","boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/","title":"Enforce Signed Image Verification","text":"<p>Important</p> <p>This guardrail is disabled by default. Contact your Platform Administrator if you want to enable it.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#problem","title":"Problem","text":"<p>How can you ensure the integrity and authenticity of a container image between building it, storing it in a container registry and deploying a workload to run it?</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#solution","title":"Solution","text":"<p>One method to protect against image tampering throughout the pipeline is the use of image signatures and verification. This method uses a public key signing algorithm to sign container images as close to build time as possible, and enforcing verification of signatures before a container image is run. Thus an image that has been tampered with will not be run, as it will not have the correct signature.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>Your Platform Administrator can configure Welkin to technically enforce verification of container image signatures before they are run. You provide your administrator with one or more public keys or certificates that will be used for this verification, and you have to sign all your container images with at least one of the corresponding private keys.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#image-signing","title":"Image Signing","text":"<p>This safeguard supports image signing and verification using Sigstore Cosign or Notary Notation.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#sigstore-cosign","title":"Sigstore Cosign","text":"<p>To get started using Cosign to sign your images, you should install the CLI. After you have built and pushed your container image to your container registry, you can then sign it using the CLI:</p> <ol> <li> <p>Sign an image:</p> <pre><code>cosign sign --key &lt;PRIVATE-KEY&gt; &lt;IMAGE&gt;\n</code></pre> <p>If you don't already have a key pair, you can generate one with the following command:</p> <pre><code>cosign generate-key-pair\n</code></pre> <p>When signing an image you will be prompted to upload a transparency log entry. If you do not wish to do so, you need to run the following command instead:</p> <pre><code>cosign sign --key &lt;PRIVATE-KEY&gt; &lt;IMAGE&gt; --tlog-upload=false\n</code></pre> </li> <li> <p>After signing your image, you can verify the signature with:</p> <pre><code>cosign verify --key &lt;PUBLIC-KEY&gt; &lt;IMAGE&gt;\n</code></pre> <p>If you decided to opt out of uploading a transparency log entry, you would need to run the following instead:</p> <pre><code>cosign verify --key &lt;PUBLIC-KEY&gt; &lt;IMAGE&gt; --insecure-ignore-tlog\n</code></pre> </li> </ol> <p>Important</p> <p>You should also inform your Platform Administrator if you are not using the transparency log, so that they can configure the verification policy accordingly.</p>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#notary-notation","title":"Notary Notation","text":"<p>To get started using Notary to sign your images, you should install the CLI. After you have built and pushed your container image to your container registry, you can then sign it using the CLI:</p> <ol> <li> <p>Add an existing certificate with the following command:</p> <pre><code>notation cert add --type &lt;TYPE&gt; --store &lt;STORE&gt; &lt;PATH-TO-CERT-FILE&gt;\n</code></pre> <p>You can also generate a new test certificate with:</p> <pre><code>notation cert generate-test &lt;STORE&gt;\n</code></pre> </li> <li> <p>Identify the signing <code>&lt;KEY&gt;</code> corresponding to your certificate with:</p> <pre><code>notation key ls\n</code></pre> </li> <li> <p>Sign an image with the following command, using the <code>&lt;KEY&gt;</code> from the previous step:</p> <pre><code>notation sign --key &lt;KEY&gt; &lt;IMAGE&gt;\n</code></pre> </li> <li> <p>Create and import a trust policy, replacing <code>&lt;TYPE&gt;</code> and <code>&lt;STORE&gt;</code> with what was used in step one:</p> <pre><code>cat &lt;&lt;EOF &gt; ./trustpolicy.json\n{\n    \"version\": \"1.0\",\n    \"trustPolicies\": [\n        {\n            \"name\": \"my-trust-policy\",\n            \"registryScopes\": [ \"*\" ],\n            \"signatureVerification\": {\n                \"level\" : \"strict\"\n            },\n            \"trustStores\": [ \"&lt;TYPE&gt;:&lt;STORE&gt;\" ],\n            \"trustedIdentities\": [\n                \"*\"\n            ]\n        }\n    ]\n}\nEOF\n</code></pre> </li> <li> <p>Import the policy with:</p> <pre><code>notation policy import trustpolicy.json\n</code></pre> </li> <li> <p>Verify the image with:</p> <pre><code>notation verify &lt;IMAGE&gt;\n</code></pre> </li> </ol>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-signed-image-verification/#troubleshooting","title":"Troubleshooting","text":"<p>The following error indicates that you are attempting to deploy a workload with an image that has not been signed. You should verify that it has been signed correctly.</p> <p>This error could also indicate that no transparency log has been uploaded when signing the image, but the policy has been configured to check for transparency logs when verifying signatures. This is configured by your Platform Administrator.</p> <pre><code>error: failed to create deployment: admission webhook \"mutate.kyverno.svc-fail\" denied the request:\n\nresource Deployment/namespace/deployment-name was blocked due to the following policies\n\nverify-image-signature:\n  autogen-verify-image-signature: 'failed to verify image &lt;IMAGE&gt;:\n    .attestors[0].entries[0]: failed to verify &lt;IMAGE&gt;@sha256:xxx:\n    no signature is associated with \"&lt;IMAGE&gt;@sha256:xxx:\",\n    make sure the artifact was signed successfully'\n</code></pre> <p>The following error indicates that you are attempting to deploy a workload with an image that is signed, but not by a trusted signer. You should verify that the image is signed with the correct signing keys, and that the correct public keys or certificates have been provided to your Platform Administrator.</p> <pre><code>error: failed to create deployment: admission webhook \"mutate.kyverno.svc-fail\" denied the request:\n\nresource Deployment/namespace/deployment-name was blocked due to the following policies\n\nverify-image-signature:\n  autogen-verify-image-signature: |-\n    failed to verify image &lt;IMAGE&gt;: .attestors[0].entries[0]: failed to verify &lt;IMAGE&gt;@sha256:xxx: signature verification failed\n    failed to verify signature with digest sha256:xxx, signature is not produced by a trusted signer\n</code></pre>","tags":[],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/","title":"Enforce Trusted Registries","text":"","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/#avoid-vulnerable-container-images","title":"Avoid vulnerable container images","text":"<p>Note</p> <p>This section helps you implement ISO 27001, specifically:</p> <ul> <li>A.12.6.1 Management of Technical Vulnerabilities</li> </ul> <p>Important</p> <ul> <li>This guardrail is enabled by default and will warn on violations. As a result, resources that violate this policy will generate warning messages, but will still be created.</li> </ul>","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/#problem","title":"Problem","text":"<p>A healthy security posture requires you to ensure your code has no known vulnerabilities. Welkin comes with a registry which includes vulnerability scanning of container images. It can even be configured to prevent the Kubernetes Cluster from pulling images with vulnerabilities above a set criticality. This is a per-project setting, so you could, for example, have a stricter policy for publicly facing application components -- e.g., the front office -- and a less strict policy for internal application components -- e.g., the back office.</p> <p>Public container registry, such as Docker Hub and Quay, might not stick to the vulnerability management you require, perhaps being at times too strict or too loose.</p>","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/#solution","title":"Solution","text":"<p>You can designate a set of registries, a project within a registry or specific container images as trusted. By this you declared that you did a risk analysis and determined that they fulfill your security requirements.</p>","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/#how-does-welkin-help","title":"How Does Welkin Help?","text":"<p>Your administrator can configure Welkin to technically enforce a set of trusted container registries. This means that if you accidentally reference an image in an untrusted registry, you will get the following error:</p> <pre><code>Error: admission webhook \"validation.gatekeeper.sh\" denied the request: [denied by require-harbor-repo] container \"welkin-user-demo\" has an invalid image repo \"harbor.example.com/demo/welkin-user-demo:1.16.0\", allowed repos are [\"harbor.cksc.a1ck.io\"]\n</code></pre> <p>The resolution is rather simple. You have two options:</p> <ol> <li>Change the container image to point to a trusted registry.</li> <li>Get in touch with your administrator and discuss augmenting the set of trusted registries.</li> </ol> <p>Important</p> <p>Instead of adding a not-really-trusted registry to the set of trusted registries, prefer mirroring some public images in your Welkin registry.</p> <p>If your administrator has not enforced this policy yet, you can view current violations of the policy by running:</p> <pre><code>kubectl get k8sallowedrepos.constraints.gatekeeper.sh require-harbor-repo -ojson | jq .status.violations\n</code></pre>","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/safeguards/enforce-trusted-registries/#further-reading","title":"Further Reading","text":"<ul> <li>Container Images</li> <li>Harbor Vulnerability Scanning</li> </ul>","tags":["NIST SP 800-171 3.4.8","NIST SP 800-171 3.4.9","ISO 27001 Annex A 8.7 Protection Against Malware"],"boost":2},{"location":"user-guide/self-managed-services/","title":"Self-Managed Service","text":"<p>We designed Welkin with guardrails by default. Also, application developers have rather limited permissions. Together, these help your organization comply with various regulations and information security standards, such as:</p> <ul> <li>BDEW Whitepaper Anforderungen an sichere Steuerungs- und Telekommunikationssysteme v3.0 Requirement 4.1.1 Minimal-Need-To-Know-Prinzip.</li> </ul> <p>However, Welkin also empathises with application developers who need to \"get things done\". Indeed, application developer may need to leverage the Kubernetes Operator pattern for increased productivity and may perceive Welkin to be too restrictive.</p> <p>To fulfill the needs of application developers, Welkin introduces the concept of self-managed services. Each self-managed service is a Welkin feature which provides just enough permissions to install an Operator, while at the same time complying with the principle of least privilege. Self-managed services are disabled by default and need to be enabled by the platform administrator to comply with secure-by-default principles.</p> <p>Check out the other pages in this section to discover which self-managed services are already supported by Welkin.</p>"},{"location":"user-guide/self-managed-services/#requesting-a-new-self-managed-service","title":"Requesting a New Self-Managed Service","text":"<p>Each self-managed service is treated by Welkin like a feature to ensure it gets the needed attention in terms of quality assurance and security assessment.</p> <p>If the self-managed service is not yet supported by Welkin, feel free to Request a Feature</p>"},{"location":"user-guide/self-managed-services/#contributing-a-new-self-managed-service","title":"Contributing a New Self-Managed Service","text":"<p>If you have the needed expertise to add a new self-managed service yourself, we welcome your contribution.</p> <p>Please issue a PR against Welkin Apps and we'll duly review it. To get started, we recommend you check out this PR which shows how we added the Kafka self-managed service.</p>"},{"location":"user-guide/self-managed-services/#further-reading","title":"Further Reading","text":"<ul> <li>Kubernetes Operator Pattern</li> <li>ISO 27019:2024</li> </ul> <ul> <li>BDEW Whitepaper Anforderungen an sichere Steuerungs- und Telekommunikationssysteme v3.0</li> </ul>"},{"location":"user-guide/self-managed-services/ferretdb/","title":"FerretDB\u00ae (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>Danger</p> <p>FerretDB\u00ae tries to be a drop-in replacement for MongoDB\u00ae. However:</p> <ul> <li>There are known differences.</li> <li>There might also be performance implications.</li> </ul> <p>Make sure to load-test your application with FerretDB before going into production.</p> <p>FerretDB is a database that is an open-source alternative to MongoDB that uses PostgreSQL as its backend database. This documentation details how to run FerretDB in a Welkin Cluster using the Managed PostgreSQL service.</p>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#pushing-ferretdb-image-to-harbor","title":"Pushing FerretDB image to Harbor","text":"<p>These instructions will pull the FerretDB container image and push it to another registry. If you are using managed Harbor as your container registry, please follow these instructions on how to authenticate, create a new project, and how to create a robot account and using it in a pull-secret to be able to pull an image from Harbor to your Cluster safely:</p> <pre><code>TAG=1.0.0\nREGISTRY=harbor.$DOMAIN\nREGISTRY_PROJECT=demo\n\ndocker pull ghcr.io/ferretdb/ferretdb:$TAG\ndocker tag ghcr.io/ferretdb/ferretdb:$TAG $REGISTRY/$REGISTRY_PROJECT/ferretdb:$TAG\ndocker push $REGISTRY/$REGISTRY_PROJECT/ferretdb:$TAG\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#install","title":"Install","text":"<p>In a managed Welkin environment, follow these instructions on how to access the Managed PostgreSQL service and how to create an application user and database.</p> <p>Create secret containing a PostgreSQL URL to authenticate to the Managed PostgreSQL service and newly created database with the application user credentials:</p> <pre><code>kubectl create secret generic --from-literal=ferretdb-url=\"postgresql://$APP_USERNAME:$APP_PASSWORD@$PGHOST:$PGPORT/$APP_DATABASE\" ferretdb-postgres-credentials\n</code></pre> <p>Deploy:</p> <pre><code>apiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ferretdb\n  labels:\n    run: ferretdb\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: ferretdb\n  template:\n    metadata:\n      labels:\n        run: ferretdb\n    spec:\n      containers:\n        - name: ferretdb\n          image: $REGISTRY/$REGISTRY_PROJECT/ferretdb:$TAG # replace this\n          args:\n            - --listen-addr=0.0.0.0:27017\n            - --telemetry=disable\n            - --postgresql-url=$(FERRETDB_URL)\n          env:\n            - name: FERRETDB_URL\n              valueFrom:\n                secretKeyRef:\n                  name: ferretdb-postgres-credentials\n                  key: ferretdb-url\n          resources:\n            requests:\n              cpu: \"1000m\"\n              memory: \"15M\"\n          securityContext:\n            capabilities:\n              drop:\n                - ALL\n            runAsNonRoot: true\n            runAsUser: 1001\n          volumeMounts:\n            - mountPath: /state\n              name: state\n      securityContext:\n        fsGroup: 1001\n      volumes:\n        - name: state\n          emptyDir: {}\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ferretdb-service\n  labels:\n    run: ferretdb\nspec:\n  type: ClusterIP\n  selector:\n    run: ferretdb\n  ports:\n    - protocol: TCP\n      port: 27017\n      targetPort: 27017\n</code></pre> <p>Check that FerretDB started properly:</p> <pre><code>$ kubectl get svc,deploy,pod -l run=ferretdb\nNAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)     AGE\nservice/ferretdb-service   ClusterIP   10.233.42.102   &lt;none&gt;        27017/TCP   75s\n\nNAME                       READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/ferretdb   1/1     1            1           75s\n\nNAME                            READY   STATUS    RESTARTS   AGE\npod/ferretdb-5887cc848c-brwjf   1/1     Running   0          75s\n</code></pre> <p>The Deployment should show <code>STATUS</code> is <code>Running</code>. The Pod(s) should have <code>STATUS</code> is <code>Running</code>.</p> <p>To try out access to FerretDB, you can port-forward the Service to localhost and connect using <code>mongosh</code>:</p> <pre><code>kubectl port-forward svc/ferretdb-service 27017\n\nmongosh mongodb://localhost:27017\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#python-client-example","title":"Python client example","text":"<p>The following is an example of how to connect to FerretDB using the PyMongo Python library (using the localhost port-forwarding described above).</p> <pre><code>from pymongo import MongoClient\n\nclient = MongoClient('mongodb://localhost:27017')\n\n# List all databases\nprint(client.list_database_names())\n\n# List collections in the database \"mongodb\"\nprint(client['mongodb'].list_collection_names())\n\n# Create db and insert element into a collection\ndatabase   = client['test_db']\ncollection = database['customers']\n\nmydict = { \"name\": \"John\", \"address\": \"Highway 38\" }\n\ncollection.insert_one(mydict)\nprint(collection.find_one())\n</code></pre> <p>See the following <code>pg_dump</code> below to see how the example above is mapped in the actual backend PostgreSQL database.</p> `pg_dump` <pre><code>--\n-- PostgreSQL database dump\n--\n\n-- Dumped from database version 14.6 (Ubuntu 14.6-1.pgdg22.04+1)\n-- Dumped by pg_dump version 15.1 (Ubuntu 15.1-1.pgdg22.04+1)\n\nSET statement_timeout = 0;\nSET lock_timeout = 0;\nSET idle_in_transaction_session_timeout = 0;\nSET client_encoding = 'UTF8';\nSET standard_conforming_strings = on;\nSELECT pg_catalog.set_config('search_path', '', false);\nSET check_function_bodies = false;\nSET xmloption = content;\nSET client_min_messages = warning;\nSET row_security = off;\n\n--\n-- Name: test_db; Type: SCHEMA; Schema: -; Owner: ferretdb\n--\n\nCREATE SCHEMA test_db;\n\n\nALTER SCHEMA test_db OWNER TO ferretdb;\n\nSET default_tablespace = '';\n\nSET default_table_access_method = heap;\n\n--\n-- Name: _ferretdb_database_metadata; Type: TABLE; Schema: test_db; Owner: ferretdb\n--\n\nCREATE TABLE test_db._ferretdb_database_metadata (\n    _jsonb jsonb\n);\n\n\nALTER TABLE test_db._ferretdb_database_metadata OWNER TO ferretdb;\n\n--\n-- Name: customers_c09344de; Type: TABLE; Schema: test_db; Owner: ferretdb\n--\n\nCREATE TABLE test_db.customers_c09344de (\n    _jsonb jsonb\n);\n\n\nALTER TABLE test_db.customers_c09344de OWNER TO ferretdb;\n\n--\n-- Data for Name: _ferretdb_database_metadata; Type: TABLE DATA; Schema: test_db; Owner: ferretdb\n--\n\nCOPY test_db._ferretdb_database_metadata (_jsonb) FROM stdin;\n{\"$s\": {\"p\": {\"_id\": {\"t\": \"string\"}, \"table\": {\"t\": \"string\"}, \"indexes\": {\"i\": [{\"t\": \"object\", \"$s\": {\"p\": {\"key\": {\"t\": \"object\", \"$s\": {\"p\": {\"_id\": {\"t\": \"int\"}}, \"$k\": [\"_id\"]}}, \"name\": {\"t\": \"string\"}, \"unique\": {\"t\": \"bool\"}, \"pgindex\": {\"t\": \"string\"}}, \"$k\": [\"pgindex\", \"name\", \"key\", \"unique\"]}}], \"t\": \"array\"}}, \"$k\": [\"_id\", \"table\", \"indexes\"]}, \"_id\": \"customers\", \"table\": \"customers_c09344de\", \"indexes\": [{\"key\": {\"_id\": 1}, \"name\": \"_id_\", \"unique\": true, \"pgindex\": \"customers__id__e06693c2_idx\"}]}\n\\.\n\n\n--\n-- Data for Name: customers_c09344de; Type: TABLE DATA; Schema: test_db; Owner: ferretdb\n--\n\nCOPY test_db.customers_c09344de (_jsonb) FROM stdin;\n{\"$s\": {\"p\": {\"_id\": {\"t\": \"objectId\"}, \"name\": {\"t\": \"string\"}, \"address\": {\"t\": \"string\"}}, \"$k\": [\"_id\", \"name\", \"address\"]}, \"_id\": \"6454cd232da4567e5cd31f39\", \"name\": \"John\", \"address\": \"Highway 37\"}\n\\.\n\n\n--\n-- Name: _ferretdb_database_metadata_id_idx; Type: INDEX; Schema: test_db; Owner: ferretdb\n--\n\nCREATE UNIQUE INDEX _ferretdb_database_metadata_id_idx ON test_db._ferretdb_database_metadata USING btree (((_jsonb -&gt; '_id'::text)));\n\n\n--\n-- Name: customers__id__e06693c2_idx; Type: INDEX; Schema: test_db; Owner: ferretdb\n--\n\nCREATE UNIQUE INDEX customers__id__e06693c2_idx ON test_db.customers_c09344de USING btree (((_jsonb -&gt; '_id'::text)));\n\n\n--\n-- PostgreSQL database dump complete\n--\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#security","title":"Security","text":"<p>FerretDB supports securing connections between FerretDB and client with TLS. All you need is to specify additional run-time arguments or environment variables, as described in the FerretDB documentation.</p>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#known-issues-limitations","title":"Known Issues / Limitations","text":"<ul> <li>FerretDB currently does not support user management.</li> <li>FerretDB currently does not support role management.</li> <li>FerretDB currently does not allow for optimizations or tweaks to the underlying PostgreSQL schema that is used by FerretDB as it translates MongoDB collections to PostgreSQL tables.</li> </ul> <p>Note  As of May 2023, the project was recently released to its first major version (v1.0.0) and is constantly being developed and improved, hence, these issues may have already been solved depending on when you are reading this. Check supported commands for FerretDB to see what is currently available.</p>","boost":2},{"location":"user-guide/self-managed-services/ferretdb/#further-reading","title":"Further Reading","text":"<ul> <li>FerretDB GitHub</li> <li>FerretDB documentation</li> <li>FerretDB supported commands</li> <li>MongoDB Shell (<code>mongosh</code>)</li> </ul>","boost":2},{"location":"user-guide/self-managed-services/flux/","title":"Flux\u2122 (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>This page is a preview for self-managed cluster-wide resources</p> <p>Welkin restricts Application Developers to manage CustomResourceDefinitions (CRDs) and other cluster-wide resources for security purposes. This means that some applications that require such cluster-wide resources are not possible to install for you as an Application Developer. As a trade-off, we are launching this preview feature that allows for self-management of specific cluster-wide resources required for certain popular applications. It is disabled by default, so please ask your Platform Administrator to enable this preview feature.</p> <p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket. This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>Flux is an open-source tool for continuous delivery (CD) and GitOps in Kubernetes. It allows you to automate and manage the Deployment of applications and configurations in a Kubernetes Cluster using a Git repository as the source of truth.</p> <p>Flux is a CNCF Graduated project.</p> <p>This page will help you install Flux in a Welkin environment.</p> <p>Supported versions</p> <p>This installation guide has been tested with Flux version 2.1.2.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#initial-prep","title":"Initial Prep","text":"","boost":2},{"location":"user-guide/self-managed-services/flux/#dependencies","title":"Dependencies","text":"<p>This guide depends on the self-managed Cluster resources feature to be enabled. This is so Flux gets the necessary CRDs and ClusterRoles installed.</p> <p>Flux also requires the image repository <code>ghcr.io/fluxcd</code> to be allowlisted. Ask your Platform Administrator to do this while enabling the self-managed Cluster resources feature.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#git","title":"Git","text":"<p>You need to setup a Git repository that will contain the manifest files. This can be a personal or organization repository. It is strongly recommended that it is a private (as in: not public) repository.</p> <p>Next you need to generate an SSH key that will be used to communicate with the Git repository. The private key will be used as a Kubernetes Secret in a later step.</p> <pre><code>ssh-keygen -t rsa -C \"flux-deploymentkey\" -f &lt;path-to-store-key&gt;\n</code></pre> <p>After you have generated an SSH Key, you want to add it as a deploy key in your Git repository. This can be done through <code>Settings</code> -&gt; <code>Deploy keys</code>. Copy your public key that you just generated and paste here.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#kubernetes","title":"Kubernetes","text":"<p>In Kubernetes you will need to:</p> <ol> <li>Install CRDs</li> <li>Create a namespace for Flux</li> <li>Create a Git Secret in the namespace</li> <li>Create Roles/RoleBindings for Flux.</li> </ol>","boost":2},{"location":"user-guide/self-managed-services/flux/#crds","title":"CRDs","text":"<p>You need to apply the Custom Resource Definitions (CRDs) required by Flux. This is typically not allowed in a Welkin Environment, but with Flux enabled with the self-managed Cluster resources feature, this allows you to apply these yourself.</p> <pre><code>mkdir crds\n\n# Fetches Flux CRDs for v2.1.2 and saves it in the crds directory\ncurl https://raw.githubusercontent.com/fluxcd/flux2/v2.1.2/manifests/crds/kustomization.yaml &gt; crds/kustomization.yaml\n\nkubectl apply -k crds\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/flux/#namespace","title":"Namespace","text":"<p>You need to create a namespace where Flux will work. This namespace must be called <code>flux-system</code>. Create this sub-namespace under your parent namespace, e.g. <code>production</code>.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: flux-system\n  namespace: production\nEOF\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/flux/#git-secret","title":"Git Secret","text":"<p>Next to allow Flux to interact with your Git Repository you need to create a Secret containing the ssh private key created earlier. This can be done with the Flux CLI:</p> <pre><code>flux create secret git &lt;repo-name&gt;-auth \\\n    --url=ssh://git@github.com/&lt;owner&gt;/&lt;repo-name&gt;.git \\\n    --private-key-file=&lt;path-to-ssh-private-key&gt;\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/flux/#roles-and-rolebindings","title":"Roles and RoleBindings","text":"<p>You need to create the necessary Roles for Flux to function. This needs to be done in every namespace that you want Flux to work in.</p> <p>Since Welkin uses the Hierarchical Namespace Controller, the easiest way to achieve this is to place the Roles and RoleBindings in the parent namespace that <code>flux-system</code> was created from. By doing so, all namespaces created under the same parent namespace will inherit the Roles and RoleBindings.</p> <p>If you have multiple namespaces that ought to be targets for Flux, you can add the Roles and RoleBindings to more than one \"parent\" namespace. For instance, to <code>staging</code>, to get Flux to work with the <code>staging</code> namespace and any namespace anchored to it.</p> <pre><code>mkdir roles\n\n# Fetches the necessary Roles and saves it in the roles directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/flux-files/roles/all-controllers-role.yaml &gt; roles/all-controllers-role.yaml\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/flux-files/roles/source-controller-role.yaml &gt; roles/source-controller-role.yaml\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/flux-files/roles/kustomization.yaml &gt; roles/kustomization.yaml\n\n# If you installed Flux in another namespace other than production, edit the namespace in roles/kustomization.yaml\n\nkubectl apply -k roles\n</code></pre> <p>The kustomize and Helm controller needs some extra permissions as well since it wants to deploy. The simplest is to add these controller ServiceAccounts to the <code>extra-workload-admins</code> RoleBinding in the parent namespace e.g., <code>production</code>. This will grant Flux the maximum permission an Application Developer can give in the namespaces where it is configured. Edit the RoleBinding and add the lines below.</p> <pre><code>kubectl edit rolebindings extra-workload-admins -n production\n\n...\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: admin\n# Add these lines below\nsubjects:\n- kind: ServiceAccount\n  name: kustomize-controller\n  namespace: flux-system\n- kind: ServiceAccount\n  name: helm-controller\n  namespace: flux-system\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/flux/#setup","title":"Setup","text":"","boost":2},{"location":"user-guide/self-managed-services/flux/#generate-manifests","title":"Generate Manifests","text":"<p>Note</p> <p>Installing Flux with <code>flux bootstrap</code> command does not work when installing in a Welkin environment, please follow our instructions instead.</p> <p>The script below can be used to generate Flux manifests and a basic Cluster folder structure similar to <code>flux bootstrap</code>. Be sure to configure the environment variables in the script.</p> <pre><code># Generate Manifest files\n\n# Be sure to edit the variables below!\nCLUSTER_NAME=\"&lt;cluster-name&gt;\"\nREPO_NAME=\"&lt;repo-name&gt;\"\nURL=\"github.com/&lt;owner&gt;/&lt;repo-name&gt;\"\n\nmkdir -p clusters/$CLUSTER_NAME/flux-system\ntouch clusters/$CLUSTER_NAME/flux-system/gotk-components.yaml \\\n    clusters/$CLUSTER_NAME/flux-system/gotk-sync.yaml \\\n    clusters/$CLUSTER_NAME/flux-system/kustomization.yaml\n\n# Generate Flux manifests\nflux install --export &gt; clusters/$CLUSTER_NAME/flux-system/gotk-components.yaml\n\n# Create repo manifest\nflux create source git $REPO_NAME-repo \\\n    --url=ssh://git@\"$URL\" \\\n    --branch=main \\\n    --secret-ref=$REPO_NAME-auth \\\n    --export &gt; clusters/$CLUSTER_NAME/flux-system/gotk-sync.yaml\n\n# Create Kustomization manifest for Flux\nflux create kustomization $REPO_NAME-repo \\\n    --namespace=flux-system \\\n    --source=GitRepository/$REPO_NAME-repo.flux-system \\\n    --path=\"./clusters/$CLUSTER_NAME/flux-system\" \\\n    --prune=true \\\n    --interval=1m \\\n    --export &gt;&gt; clusters/$CLUSTER_NAME/flux-system/gotk-sync.yaml\n\n# Fetches our patches and saves them in the clusters/$CLUSTER_NAME/flux-system directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/flux-files/kustomization.yaml &gt; clusters/$CLUSTER_NAME/flux-system/kustomization.yaml\n</code></pre> <p>Commit and push the files to the repository.</p> <p>Warning</p> <p>If you created your SSH keys in the repository folder, make sure you do not push these to the repository.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#install","title":"Install","text":"<p>Simply install by applying the kustomization.</p> <p><code>kubectl apply -k clusters/$CLUSTER_NAME/flux-system</code></p> <p>Now that it has been installed, you are ready to use Flux on Welkin!</p> <p>Read the Further Reading, keep in mind the (updated) list of Known Issues, both of which are found below.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#further-reading","title":"Further reading","text":"<ul> <li> <p>Flux core concepts</p> </li> <li> <p>Flux multi-tenancy</p> </li> <li> <p>Controller options</p> </li> <li> <p>Flux components</p> </li> </ul>","boost":2},{"location":"user-guide/self-managed-services/flux/#known-issues","title":"Known Issues","text":"","boost":2},{"location":"user-guide/self-managed-services/flux/#role-and-rolebindings-does-not-apply-correctly","title":"Role and RoleBindings does not apply correctly","text":"<p>Error produced: <code>Error from server (NotFound): error when creating \"roles/\": roles.rbac.authorization.k8s.io \"role\" not found</code></p> <p>There is a known issue with Role and RoleBindings not being able to be applied together using Flux in a GitOps way. For example, if you apply a Role and a RoleBinding that uses the Role, then Flux will fail to apply. If the Role already exists in the Cluster then Flux will succeed.</p> <p>Flux uses the server-side apply, which requires the \u2018bind\u2019 permission to properly apply RoleBindings. And we cannot give you this due to privilege escalation issues with this permission.</p> <p>You can workaround this using the Flux Kustomization <code>dependsOn</code> functionality. By splitting the Roles and RoleBindings into separate folders and then creating two Kustomizations for them where the RoleBindings will depend on the Roles. Then the Roles will be applied before the RoleBindings and so the issue will not occur. Refer to the previous link for an example.</p>","boost":2},{"location":"user-guide/self-managed-services/flux/#notes","title":"Notes","text":"<p>For security reasons, Elastisys has not enabled the multi-tenancy model described in Flux documentation. Doing so requires giving Flux implicit ClusterAdmin permissions using impersonation, and to then trust that its use of it is correct and secure.</p> <p>When using Flux as a self-managed service, it is important to ensure that excessive permissions cannot be gained through exploiting privilege escalation with Flux.</p> <p>The multitenancy feature flags can still be used by you, however, to, for example, deny cross namespace references.</p>","boost":2},{"location":"user-guide/self-managed-services/jaeger/","title":"Jaeger (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>This page is a preview for self-managed cluster-wide resources</p> <p>Welkin restricts Application Developers to manage CustomResourceDefinitions (CRDs) and other cluster-wide resources for security purposes. This means that some applications that require such cluster-wide resources are not possible to install for you as an Application Developer. As a trade-off, we are launching this preview feature that allows for self-management of specific cluster-wide resources required for certain popular applications. It is disabled by default, so please ask your Platform Administrator to enable this preview feature.</p> <p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket. This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>Important</p> <p>This feature is only available on Welkin Apps versions <code>v0.50.0</code> and newer.</p> <p>This page will help you install Jaeger with OpenSearch as a storage backend, and generate some traces to verify that everything is working. This guide is meant as a complement to the official documentation of the Jaeger Operator, OpenSearch and OpenSearch Dashboards, which you can refer to for further configuration based on your needs.</p>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#enable-self-managed-jaeger","title":"Enable Self-Managed Jaeger","text":"<p>Following the guide requires the self-managed Cluster resources feature to be enabled. You can contact your Platform Administrator to have this feature enabled.</p>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#deploy","title":"Deploy","text":"<p>This section will guide you through deploying all components with a minimal working configuration. Everything must be deployed in a namespace called <code>jaeger</code>. You can create this namespace under your parent namespace using the following snippet:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: jaeger\n  namespace: &lt;parent-namespace&gt;\nEOF\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#configure-and-deploy-opensearch","title":"Configure and Deploy OpenSearch","text":"<p>Supported versions</p> <p>This installation guide has been tested with OpenSearch version 2.34.0.</p> <p>Start by generating a strong password that will be used as the initial admin password for OpenSearch. The password needs to be a minimum of 8 characters long and must contain at least one uppercase letter, one lowercase letter, one digit, and one special character that is strong.</p> <p>Then create a Secret containing this password:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: opensearch-credentials\n  namespace: jaeger\ntype: Opaque\nstringData:\n  ES_PASSWORD: &lt;your-strong-password&gt;\n  ES_USERNAME: admin\nEOF\n</code></pre> <p>Add the OpenSearch Helm repository:</p> <pre><code>helm repo add opensearch https://opensearch-project.github.io/helm-charts/\nhelm repo update\n</code></pre> <p>Below is a sample <code>opensearch-values.yaml</code> file for OpenSearch that you can save to your working directory:</p> <pre><code>config:\n  opensearch.yml: |-\n    network.host: 0.0.0.0\n    action:\n      auto_create_index: \"jaeger*,.opensearch*,.opendistro-*,security-auditlog-*\"\nresources:\n  requests:\n    cpu: 500m\n    memory: 1Gi\npersistence:\n  imageTag: stable\n  enableInitChown: false\nextraEnvs:\n  - name: OPENSEARCH_INITIAL_ADMIN_PASSWORD\n    valueFrom:\n      secretKeyRef:\n        name: opensearch-credentials\n        key: ES_PASSWORD\n  - name: plugins.security.ssl.http.enabled\n    value: \"false\"\n  - name: plugins.security.allow_default_init_securityindex\n    value: \"true\"\n</code></pre> <p>Once you are happy with the configuration, you are ready to deploy OpenSearch:</p> <pre><code>helm upgrade --install opensearch opensearch/opensearch --version 2.34.0 \\\n  -n jaeger \\\n  -f opensearch-values.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#configure-and-deploy-opensearch-dashboards","title":"Configure and Deploy OpenSearch Dashboards","text":"<p>Supported versions</p> <p>This installation guide has been tested with OpenSearch Dashboards version 2.30.0.</p> <p>Once OpenSearch is up and running, you are ready to deploy OpenSearch Dashboards. Below is a sample <code>opensearch-dashboards-values.yaml</code>:</p> <pre><code>opensearchHosts: \"http://opensearch-cluster-master.jaeger.svc.cluster.local:9200\"\nresources:\n  requests:\n    cpu: 200m\n    memory: 300Mi\n  limits:\n    cpu: 400m\n    memory: 600Mi\nrbac:\n  create: false\n</code></pre> <p>Deploy OpenSearch Dashboards:</p> <pre><code>helm upgrade --install opensearch-dashboards opensearch/opensearch-dashboards \\\n   --version 2.30.0 \\\n  -n jaeger \\\n  -f opensearch-dashboards-values.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#configure-and-deploy-jaeger-operator","title":"Configure and Deploy Jaeger Operator","text":"<p>Supported versions</p> <p>This installation guide has been tested with Jaeger Operator version 2.57.0.</p> <p>Start by adding the Jaeger Helm repository:</p> <pre><code>helm repo add jaegertracing https://jaegertracing.github.io/helm-charts\nhelm repo update\n</code></pre> <p>Below is a sample <code>jaeger-operator-values.yaml</code>:</p> <pre><code>extraEnv:\n  - name: WATCH_NAMESPACE\n    value: \"jaeger\"\n  - name: ES_TAGS_AS_FIELDS_ALL\n    value: \"true\"\nwebhooks:\n  mutatingWebhook:\n    create: false\n  validatingWebhook:\n    create: false\nrbac:\n  create: false\n  clusterRole: false\nserviceAccount:\n  name: jaeger-operator\nresources:\n  requests:\n    cpu: 100m\n    memory: 128Mi\n  limits:\n    cpu: 250m\n    memory: 256Mi\njaeger:\n  create: true\n  spec:\n    ingress:\n      enabled: false\n    strategy: production\n    collector:\n      resources:\n        requests:\n          cpu: 100m\n          memory: 200Mi\n        limits:\n          cpu: 500m\n          memory: 512Mi\n    agent:\n      resources:\n        requests:\n          cpu: 100m\n          memory: 200Mi\n        limits:\n          cpu: 500m\n          memory: 512Mi\n    query:\n      resources:\n        requests:\n          cpu: 100m\n          memory: 200Mi\n        limits:\n          cpu: 500m\n          memory: 512Mi\n    storage:\n      dependencies:\n        enabled: false\n      type: elasticsearch\n      secretName: opensearch-credentials\n      options:\n        es:\n          create-index-templates: false\n          server-urls: http://opensearch-cluster-master:9200\n        es-provision: false\n      esIndexCleaner:\n        enabled: false\n</code></pre> <p>Install the Jaeger Operator:</p> <pre><code>helm upgrade --install jaeger-operator jaegertracing/jaeger-operator \\\n--version 2.57.0 \\\n-n jaeger \\\n-f jaeger-operator-values.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#example-application","title":"Example Application","text":"<p>Once you have deployed everything, you likely want to test that everything is working, and in order to do so you will want to generates some traces. This section will guide you through deploying a demo application to generate some traces, as well as viewing them in OpenSearch Dashboards and the Jaeger UI. Jaeger provides a demo microservice application which we will be using. The snippet below can be used to deploy the demo application:</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: hotrod\n  namespace: jaeger\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: hotrod\n  template:\n    metadata:\n      labels:\n        app: hotrod\n    spec:\n      containers:\n        - name: hotrod\n          image: jaegertracing/example-hotrod:1.53\n          env:\n            - name: OTEL_EXPORTER_OTLP_ENDPOINT\n              value: \"http://jaeger-operator-jaeger-collector.jaeger.svc.cluster.local:4318\"\n          args:\n            - \"all\"\n          ports:\n            - containerPort: 8080\n          resources:\n            requests:\n              cpu: \"100m\"\n              memory: \"128Mi\"\n            limits:\n              cpu: \"250m\"\n              memory: \"256Mi\"\n          securityContext:\n            runAsUser: 1000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: hotrod-svc\n  namespace: jaeger\nspec:\n  selector:\n    app: hotrod\n  type: ClusterIP\n  ports:\n    - name: http-hotrod\n      port: 8080\n      targetPort: 8080\nEOF\n</code></pre> <p>Once the demo application is up and running, you can start forwarding a local port to it:</p> <pre><code>kubectl port-forward -n jaeger svc/hotrod-svc 8080:8080\n</code></pre> <p>Also forward a local port to the Jaeger UI in a separate terminal:</p> <pre><code>kubectl port-forward -n jaeger svc/jaeger-operator-jaeger-query 16686:16686\n</code></pre> <p>You can now navigate to the demo application UI in your browser at <code>http://localhost:8080/</code>, where you can click on the different buttons to generate traces. See the screenshot below:</p> <p></p> <p>When generating a trace, the UI will output a <code>open trace</code> link which will take you to the trace in the Jaeger UI, where you can view details about it. See the screenshot below:</p> <p></p> <p>Afterwards you can close the port-forwards to the demo application and the Jaeger UI.</p> <p>Now that there have been traces generated, you can also view them in OpenSearch Dashboards. To do so forward a local port to the OpenSearch Dashboards Pod:</p> <pre><code>kubectl port-forward -n jaeger svc/opensearch-dashboards 5601:5601\n</code></pre> <p>Go to OpenSearch Dashboards in your browser by visiting <code>http://localhost:5601</code>. You can log in with the <code>admin</code> user and the password you generated earlier, which is available in the <code>opensearch-credentials</code> secret:</p> <pre><code>kubectl get secret -n jaeger opensearch-credentials -o jsonpath='{.data.ES_PASSWORD}' | base64 -d\n</code></pre> <p>Once you have logged in, navigate to <code>Dashboards Management-&gt;Index patterns</code> and create the index patterns <code>jaeger-service*</code> and <code>jaeger-span*</code>. See the screenshot below:</p> <p></p> <p>Afterwards, you can go to the <code>Discover</code> page and select the <code>jaeger-span*</code> index pattern to view the traces. See the screenshot below:</p> <p></p> <p>When you have finished testing things out, you can remove the demo application:</p> <pre><code>kubectl delete deployment -n jaeger hotrod\nkubectl delete service -n jaeger hotrod-svc\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jaeger/#further-reading","title":"Further Reading","text":"<ul> <li>OpenSearch Documentation<ul> <li>Configuring OpenSearch Security</li> <li>Best Practices</li> <li>OpenSearch Index State Management</li> </ul> </li> <li>Jaeger Operator Documentation</li> </ul>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/","title":"JupyterHub (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>JupyterHub brings Jupyter Notebooks to the cloud. It gives the users access to computational environments and resources without burdening users with installation and maintenance tasks. This documents shows a guide on how to setup JupyterHub in a Welkin environment.</p> <p></p>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#configure-and-deploy-jupyterhub","title":"Configure and Deploy JupyterHub","text":"<p>We chose to work with official Helm charts provided by JupyterHub. They can be downloaded via the webpage or added to your local repository by running:</p> <pre><code>helm repo add jupyterhub https://hub.jupyter.org/helm-chart/\nhelm repo update\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#configuring-jupyterhub","title":"Configuring JupyterHub","text":"<p>Below is a sample <code>values.yaml</code> file that can be used to deploy JupyterHub, please read the notes and change what is necessary. This sample uses Google OAuth for authentication and authorization.</p> <p>Note</p> <p>Requested resources should be evaluated and reconsidered for production. Information about resource allocation and also the enabling of GPU usage can be found here</p> <pre><code>imagePullSecrets:\n  - pull-secret # create this secret in the next section\n\nhub:\n  revisionHistoryLimit:\n  config:\n    GoogleOAuthenticator: #(5)\n      client_id: $YOUR_CLIENT_ID # replace this\n      client_secret: $YOUR_CLIENT_SECRET # (6) replace this\n      oauth_callback_url: https://$PROJECT_DOMAIN/hub/oauth_callback # replace this\n      hosted_domain:\n        - example.com # replace this the email domain which are allowed to log in to JupyterHub\n      login_service: Google\n      allow_all: true\n    JupyterHub:\n      admin_access: true\n      authenticator_class: google\n      admin_users:\n        - email@example.com # replace this\n  image:\n    name: k8s-hub\n\n  resources: &amp;resourceDefaults # (1)\n    requests:\n      memory: 512Mi\n      cpu: 10m\n    limits:\n      memory: 1Gi\n      cpu: 1\n  containerSecurityContext: &amp;SCDefaults # (2)\n    capabilities:\n      drop: [\"ALL\"]\n    runAsNonRoot: true\n    seccompProfile:\n      type: \"RuntimeDefault\"\n\nproxy:\n  service:\n    type: ClusterIP\n  chp:\n    containerSecurityContext: *SCDefaults\n    image:\n      name: configurable-http-proxy\n    resources: *resourceDefaults\n  traefik:\n    containerSecurityContext: *SCDefaults\n    image:\n      name: traefik\n    resources: *resourceDefaults\n  secretSync:\n    containerSecurityContext: *SCDefaults\n    image:\n      name: k8s-secret-sync\n    resources: *resourceDefaults\n  https:\n    hosts:\n      - $PROJECT_DOMAIN # replace this\n    type: letsencrypt\n    letsencrypt:\n      contactEmail: email@email.com\n\nsingleuser:\n  networkTools:\n    image:\n      name: k8s-network-tools\n    resources: *resourceDefaults\n  cloudMetadata:\n    blockWithIptables: false # (3)\n  storage: # (4)\n    type: none\n  image:\n    name: k8s-singleuser-sample\n  cpu:\n    limit: 1\n    guarantee: 0.1\n  memory:\n    limit: 2G\n    guarantee: 1G\n\nscheduling:\n  userScheduler:\n    enabled: false\n  userPlaceholder:\n    resources: *resourceDefaults\n    image:\n      name: pause\n    containerSecurityContext: *SCDefaults\n\nprePuller:\n  resources: *resourceDefaults\n  containerSecurityContext: *SCDefaults\n  hook:\n    image:\n      name: k8s-image-awaiter\n    containerSecurityContext: *SCDefaults\n    resources: *resourceDefaults\n  pause:\n    image:\n      name: pause\n    containerSecurityContext: *SCDefaults\n\ningress:\n  enabled: true\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  ingressClassName: \"nginx\"\n  hosts:\n    - $PROJECT_DOMAIN # replace this\n  tls:\n    - hosts:\n        - $PROJECT_DOMAIN # replace this\n      secretName: jupyter-secret\n</code></pre> <ol> <li>The following resources are reused using *resourceDefaults later in this file</li> <li>The following containerSecurityContext is reused using *SCDefaults later in this file</li> <li>Block set to true will append a privileged Init Container using <code>iptables</code> to block the sensitive metadata server at the provided IP. Privileged containers are not allowed in Welkin.</li> <li>\"type: none\" disables persistent storage for the user labs. Consolidate with Platform Administrator before enabling this feature. for reference</li> <li>Use this guide to get your <code>client_id</code> and <code>client_secret</code> through the Google API Console.</li> <li>This should not be treated as a secret. See risk analysis here and here.</li> </ol>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#pushing-the-jupyterhub-images-to-harbor","title":"Pushing the JupyterHub Images to Harbor","text":"<p>This sections shows how to pull the required images for JupyterHub and push them to another registry. If you are using the managed Harbor as your container registry, please follow these instructions on how to authenticate, create a new project, and how to create a robot account and using it in a pull-secret to be able to pull an image from Harbor to your Cluster safely.</p> <p>Note</p> <p>Run the following commands in the same directory as the location of your <code>values.yaml</code> file, since it will automatically update it with the correct images. If not, images will need to be manually set in the <code>values.yaml</code>.</p> <pre><code>DOMAIN=example.com # Replace this\nREGISTRY=harbor.$DOMAIN\nREGISTRY_PROJECT=jupyterhub\n\nIMAGES=$(helm show chart jupyterhub/jupyterhub | grep image: | awk  '{print $3}')\n\nfor IMAGE in $IMAGES\ndo\n\ndocker pull $IMAGE\ndocker tag $IMAGE  $REGISTRY/$REGISTRY_PROJECT/${IMAGE#*/}\ndocker push $REGISTRY/$REGISTRY_PROJECT/${IMAGE#*/}\nIMAGE_NAME=${IMAGE#*/} # Remove repository information\nsed -i 's|name: '\"${IMAGE_NAME%:*}\"'|name: '\"$REGISTRY/$REGISTRY_PROJECT/${IMAGE_NAME%:*}\"'|g' values.yaml\n\ndone\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#deploying-jupyterhub","title":"Deploying JupyterHub","text":"<p>To deploy simply use this command in combination with the modified <code>values.yaml</code> as provided above.</p> <pre><code>helm upgrade --install jupyterhub jupyterhub/jupyterhub --values values.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#known-issues-limitations","title":"Known Issues / Limitations","text":"<ul> <li> <p>JupyterHub's custom user scheduler is disabled (which can help with efficient Node downscaling). For safety reasons, developers in Welkin do not have the rights required to deploy it.</p> </li> <li> <p>JupyterHub's Admin functionality is limited since some of the admin functions require container root access. Pre-installed Python packages for users can not be added through the admin interface. They can be added by modifying the Docker image, pushing it to your image registry and redeploying JupyterHub. NOTE Users can still install their own packages.</p> </li> <li> <p>Persistent storage is currently disabled for Jupyter lab instances but can be enabled. It is disabled since Jupyter will allocate new storage for each user and should be carefully considered before it is enabled. This means that the workspace will be reset when the Pod crashes or is scaled down from not being used. This includes code added to the lab and installed packages.</p> </li> <li> <p>GPU is not enabled currently but can be enabled depending on your infrastructure provider.</p> </li> </ul>","boost":2},{"location":"user-guide/self-managed-services/jupyterhub/#further-reading","title":"Further Reading","text":"<ul> <li>General Documentation on Setting Up JupyterHub on Kubernetes<ul> <li>Customizing User Management</li> <li>Customizing User Storage</li> <li>Customizing User Environment</li> <li>Customizing User Resources</li> </ul> </li> </ul>","boost":2},{"location":"user-guide/self-managed-services/kafka/","title":"Kafka\u00ae (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>This page is a preview for self-managed cluster-wide resources</p> <p>Welkin restricts Application Developers to manage CustomResourceDefinitions (CRDs) and other cluster-wide resources for security purposes. This means that some applications that require such cluster-wide resources are not possible to install for you as an Application Developer. As a trade-off, we are launching this preview feature that allows for self-management of specific cluster-wide resources required for certain popular applications. It is disabled by default, so please ask your Platform Administrator to enable this preview feature.</p> <p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket. This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>Apache Kafka\u00ae is an open-source distributed event streaming platform. To run an Apache Kafka\u00ae Cluster on Kubernetes you can use an operator. This guide uses the Strimzi Kafka Operator.</p> <p>Strimzi is a CNCF Sandbox project.</p> <p>This page will show you how to install Strimzi Kafka Operator on Welkin. You can configure the operator to watch a single or multiple namespaces.</p> <p>Supported versions</p> <p>This installation guide has been tested with Strimzi Kafka Operator version 0.44.0.</p>","boost":2},{"location":"user-guide/self-managed-services/kafka/#enable-self-managed-kafka","title":"Enable Self-Managed Kafka","text":"<p>This guide depends on the self-managed Cluster resources feature to be enabled. This is so Strimzi Kafka Operator gets the necessary CRDs and ClusterRoles installed.</p> <p>Strimzi Kafka Operator also requires the image repository <code>quay.io/strimzi</code> to be allowlisted. Ask your Platform Administrator to do this while enabling the self-managed Cluster resources feature.</p>","boost":2},{"location":"user-guide/self-managed-services/kafka/#setup-crds-and-rbac","title":"Setup CRDs and RBAC","text":"<p>In Kubernetes you will need to:</p> <ol> <li> <p>Install the required CRDs.</p> </li> <li> <p>Create a namespace for Strimzi Kafka Operator.</p> </li> <li> <p>Create Roles/RoleBindings for Strimzi Kafka Operator.</p> </li> <li> <p>Create ServiceAccount and ConfigMap for Strimzi Kafka Operator.</p> </li> </ol>","boost":2},{"location":"user-guide/self-managed-services/kafka/#crds","title":"CRDs","text":"<p>You need to apply the Custom Resource Definitions (CRDs) required by Strimzi Kafka Operator. This is typically not allowed in a Welkin Environment, but with Kafka enabled with the self-managed Cluster resources feature, this allows you to apply these yourself.</p> <pre><code>mkdir crds\n\n# Fetches Strimzi Kafka Operator CRDs for v0.44.0 and saves it in the crds directory\ncurl -L https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.44.0/strimzi-crds-0.44.0.yaml &gt; crds/kafka-crds.yaml\n\nkubectl apply -f crds/kafka-crds.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/kafka/#namespace","title":"Namespace","text":"<p>You need to create a namespace where Strimzi Kafka Operator will work. This namespace must be called <code>kafka</code>. Create this sub-namespace under your parent namespace, e.g. <code>production</code>.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: kafka\n  namespace: production\nEOF\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/kafka/#roles-and-rolebindings","title":"Roles and RoleBindings","text":"<p>You need to create the necessary Roles for Strimzi Kafka Operator to function. This needs to be done in every namespace that you want Strimzi Kafka Operator to work in.</p> <p>Since Welkin uses the Hierarchical Namespace Controller, the easiest way to achieve this is to place the Roles and RoleBindings in the parent namespace where <code>kafka</code> was created from. By doing so, all namespaces created under the same parent namespace will inherit the Roles and RoleBindings.</p> <p>If you have multiple namespaces that ought to be targets for Strimzi Kafka Operator, you can add the Roles and RoleBindings to more than one \"parent\" namespace. For instance, to <code>staging</code>, to get Strimzi Kafka Operator to work with the <code>staging</code> namespace and any namespace anchored to it.</p> <pre><code>mkdir roles\n\n# Fetches the necessary Roles and saves it in the roles directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/roles/kafka-role.yaml &gt; roles/kafka-role.yaml\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/roles/kafka-rolebinding.yaml &gt; roles/kafka-rolebinding.yaml\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/roles/kustomization.yaml &gt; roles/kustomization.yaml\n\n# If you created the namespace kafka from another namespace other than production, edit the namespace in roles/kustomization.yaml\n\nkubectl apply -k roles\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/kafka/#serviceaccount-and-configmap","title":"ServiceAccount and ConfigMap","text":"<p>You need to create the ServiceAccount and ConfigMap that Strimzi Kafka Operator will use.</p> <pre><code>mkdir sa-cm\n\n# Fetches the ServiceAccount and ConfigMap and saves it in the sa-cm directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/sa-cm/kafka-sa-cm.yaml &gt; sa-cm/kafka-sa-cm.yaml\n\nkubectl apply -f sa-cm/kafka-sa-cm.yaml\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/kafka/#install-strimzi-kafka-operator","title":"Install Strimzi Kafka Operator","text":"<p>With the initial prep done, you are now ready to deploy the operator.</p> <p>You can find the Deployment manifest here. Deploying this on Welkin does require a Security Context to be added.</p> <p>Edit the manifest and add this under <code>spec.template.spec.containers[0]</code>:</p> <pre><code>securityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop:\n      - ALL\n  readOnlyRootFilesystem: true\n  runAsNonRoot: true\n  seccompProfile:\n    type: RuntimeDefault\n</code></pre> <p>After that you can apply the manifest with <code>kubectl apply -f 060-Deployment-strimzi-cluster-operator.yaml -n kafka</code>.</p> <p>Alternatively you can fetch an already edited file:</p> <pre><code>mkdir deployment\n\n# Fetches the edited operator Deployment and saves it in the deployment directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/deployment/kafka-operator-deployment.yaml &gt; deployment/kafka-operator-deployment.yaml\n\nkubectl apply -f deployment/kafka-operator-deployment.yaml\n</code></pre> <p>To configure the Strimzi Kafka Operator to watch multiple namespaces (e.g. running Kafka Clusters in different namespaces other than the kafka namespace), refer to Further reading.</p>","boost":2},{"location":"user-guide/self-managed-services/kafka/#deploy-your-kafka-cluster","title":"Deploy your Kafka Cluster","text":"<p>You are now ready to deploy your Kafka Cluster!</p> <p>The example files provided by Strimzi here serves as a good starting point.</p> <p>Important</p> <p>Some Kafka features are not supported as they require Cluster wide permissions, these include but may not be limited to:</p> <ul> <li>Rack awareness, using the <code>rack</code> keyword</li> <li>Storage resizing</li> <li>NodePort access</li> </ul> <p>Welkin requires that resource requests are specified for all containers. By default, the Strimzi Cluster Operator does not specify CPU and memory resource requests and limits for its deployed operands.</p> <p>Refer to Further reading for more information about resources.</p> <p>You can fetch a modified persistent-single example that includes resource requests:</p> <pre><code>mkdir kafka-cluster\n\n# Fetches the edited kafka cluster example and saves it in the kafka-cluster directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/kafka-cluster/persistent-single.yaml &gt; kafka-cluster/persistent-single.yaml\n\nkubectl apply -f kafka-cluster/persistent-single.yaml\n</code></pre> <p>Note</p> <p>The example above has very low resource requests. It is recommended to adjust these according to your Cluster.</p> <p>Refer to Further reading to learn more about how you can configure your Kafka Cluster.</p>","boost":2},{"location":"user-guide/self-managed-services/kafka/#testing","title":"Testing","text":"<p>After you have deployed your Kafka Cluster, you can test sending and receiving messages to see if it works!</p> <p>To do this, you can use a producer and consumer as seen here, under the section \"Send and receive messages\". But since Welkin requires resource requests to be specified, just copy pasting those commands will not work.</p> <p>You need to create a Pod manifest using the image <code>quay.io/strimzi/kafka:0.44.0-kafka-3.8.0</code>, and then you need to add your resource requests to this manifest. You also need to have an initial sleep command in the Pod manifest, to sleep the container for a while, this is to avoid the Pod going into the \"Completed\" stage instantly.</p> <p>Alternatively you can download a ready to use producer and consumer Pod manifests:</p> <pre><code>mkdir kafka-testing\n\n# Fetches Pod manifests for a producer and consumer and saves it in the kafka-testing directory\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/kafka-testing/kafka-producer.yaml &gt; kafka-testing/kafka-producer.yaml\ncurl https://raw.githubusercontent.com/elastisys/welkin/main/docs/user-guide/self-managed-services/kafka-files/kafka-testing/kafka-consumer.yaml &gt; kafka-testing/kafka-consumer.yaml\n\nkubectl apply -f kafka-testing/kafka-producer.yaml\nkubectl apply -f kafka-testing/kafka-consumer.yaml\n</code></pre> <p>After the Pods have started you can send and receive messages with <code>kubectl exec</code>.</p> <pre><code>kubectl exec -it kafka-producer -- bin/kafka-console-producer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic\n\nkubectl exec -it kafka-consumer -- bin/kafka-console-consumer.sh --bootstrap-server my-cluster-kafka-bootstrap:9092 --topic my-topic --from-beginning\n</code></pre> <p>Note</p> <p>If you are running the producer and/or consumer in different namespace than where your Kafka Cluster is, make sure you specify the path to the bootstrap service. E.g. <code>my-cluster-kafka-bootstrap.kafka.svc:9092</code>, if the Kafka Cluster is in the <code>kafka</code> namespace.</p>","boost":2},{"location":"user-guide/self-managed-services/kafka/#further-reading","title":"Further reading","text":"<ul> <li> <p>Strimzi Overview</p> </li> <li> <p>Deploying and Upgrading</p> </li> <li> <p>API Reference</p> </li> <li> <p>Configure Operator to watch multiple namespaces</p> </li> <li> <p>About resources for Strimzi containers</p> </li> <li> <p>Configuring Kafka</p> </li> <li> <p>Configuring Kafka and ZooKeeper storage</p> </li> </ul>","boost":2},{"location":"user-guide/self-managed-services/keycloak/","title":"Keycloak\u2122 (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p></p> <p>This page will help you succeed in connecting your application to an identity and access management solution Keycloak, which meets your security and compliance requirements.</p> <p>Keycloak is a widely recognized open-source Identity and Access Management (IAM) solution that provides user authentication and authorization services for applications. It offers a comprehensive set of features, including Single Sign-On (SSO), user federation, and social login support, making it a popular choice for securing applications in a variety of industries.</p> <p>As of May 2023, Keycloak is a CNCF Incubating project.</p> <p>In this guide we outline the necessary steps to configure and deploy a Keycloak instance on a Welkin Cluster that is using the managed PostgreSQL service.</p> <p>This will provide you with a robust and secure IAM solution to manage user access and authorization for your applications running on Welkin.</p>","boost":2},{"location":"user-guide/self-managed-services/keycloak/#initial-preparation","title":"Initial preparation","text":"<p>Note: This guide assumes that you have managed PostgreSQL as an additional service.</p> <p>Setup an application database and user in PostgreSQL</p> <p>Take note of the following variables for the next section.</p> <ul> <li> <p>The application secret you have created.</p> </li> <li> <p>The following environment variables:</p> </li> </ul> <pre><code>echo $PGHOST\necho $APP_USERNAME\necho $APP_DATABASE\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/keycloak/#configure-keycloak-with-managed-postgresql","title":"Configure Keycloak with managed PostgreSQL","text":"<p>We chose Bitnami's Helm chart for Keycloak due to its open-source nature, ease of Deployment, security optimization, and active maintenance.</p> <p>Bitnami is a well known provider of pre-configured, open-source application stacks that simplify Deployment and management in various environments, such as Kubernetes. They offer a Helm chart for Keycloak, which streamlines Deployment while adhering to Kubernetes best practices for security.</p>","boost":2},{"location":"user-guide/self-managed-services/keycloak/#deploying-keycloak","title":"Deploying Keycloak","text":"<p>Below is a sample values.yaml file that can be used to deploy Keycloak, please read the notes and change what is necessary.</p> <pre><code>resources: # (1)\n  limits:\n    #cpu: 1000m\n    memory: 1000Mi\n  requests:\n    cpu: 1000m\n    memory: 400Mi\n\npostgresql:\n  enabled: false\n\nexternalDatabase: # (2)\n  host: \"&lt;PGHOST&gt;\"\n  port: 5432\n  user: &lt;APP_USERNAME&gt;\n  database: &lt;APP_DATABASE&gt;\n  existingSecret: \"&lt;secret-name&gt;\"\n  existingSecretPasswordKey: \"PGPASSWORD\"\n\ningress: # (3)\n  enabled: true\n  ingressClassName: \"nginx\"\n  hostname: &lt;URL&gt;\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\n  tls: true\n\ncontainerSecurityContext:\n  allowPrivilegeEscalation: false\n  capabilities:\n    drop: [\"ALL\"]\n  runAsNonRoot: true\n  seccompProfile:\n    type: \"RuntimeDefault\"\n\nproduction: true # (4)\nproxy: edge\n</code></pre> <ol> <li>The example provided serves as a starting point for configuring resource requests and limits for your Keycloak Deployment. Be sure to tailor these values to your specific requirements, and monitor your Deployment to optimize resource allocation for your unique use case.</li> <li>Insert the variables that you got from initial preparation.</li> <li>Configure the Ingress hostname and which issuer you will be using.</li> <li>Enabling production mode and TLS for HTTPS. Disclaimer: Enabling production mode does not mean that the configuration here is ready for production. Please see further reading on Production configuration.</li> </ol> <p>Failure</p> <p>The values file above was produced for Keycloak Chart version 16.1.5 (appVersion 22.0.3). The shape of the values files and the required configuration may have changed with newer versions of the Keycloak Chart. Prefer installing the latest version to make sure you get all security updates. If you understand the security risks and only want to \"kick-the-tires\" then add <code>--version 16.1.5</code> in the command below.</p> <p>With the above values you can deploy Keycloak with:</p> <pre><code>helm repo add bitnami https://charts.bitnami.com/bitnami\nhelm upgrade --install keycloak bitnami/keycloak --values values.yaml\n</code></pre> <p>It might take up to 2 minutes for Keycloak to start. You can check progress as follows:</p> <pre><code>kubectl get pods --watch\n</code></pre> <p>Make sure you see <code>READY 1/1</code> for the Keycloak Pod.</p>","boost":2},{"location":"user-guide/self-managed-services/keycloak/#accessing-keycloak","title":"Accessing Keycloak","text":"<p>When you have deployed Keycloak and can access the URL you can:</p> <p>Login as admin and configure realms, users, and clients.</p> <p>The default admin user is <code>user</code>.</p> <p>The initial admin password can be fetched using the command below (assuming default namespace):</p> <pre><code>kubectl get secret keycloak -o jsonpath=\"{.data.admin-password}\" | base64 -d\n</code></pre> <p>Note: If you uninstall and install Keycloak the initial admin password will be regenerated but the previous initial admin password may still be used unless you clear the PostgreSQL database.</p> <p>For more information about using Keycloak to secure/protect your applications, see \u201cSecuring your applications\u201d and \u201cReverse Proxy\u201d in the further reading section.</p>","boost":2},{"location":"user-guide/self-managed-services/keycloak/#further-reading","title":"Further Reading","text":"<ul> <li>Documentation</li> <li>Guides</li> <li>Production Configuration</li> <li>Reverse Proxy<ul> <li>Ingress NGINX</li> <li>Oauth2-Proxy<ul> <li>Note: the \u201coidc-issuer-url\u201d may be outdated in the guide. See this issue.</li> <li>Note: Keycloak realm steps may be different if you are using the new admin console. Instructions for that can be found on their GitHub.</li> </ul> </li> </ul> </li> <li>Securing your applications</li> </ul>","boost":2},{"location":"user-guide/self-managed-services/mongodb/","title":"MongoDB Community Operator (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>This page is a preview for self-managed cluster-wide resources</p> <p>Welkin restricts Application Developers to manage CustomResourceDefinitions (CRDs) and other cluster-wide resources for security purposes. This means that some applications that require such cluster-wide resources are not possible to install for you as an Application Developer. As a trade-off, we are launching this preview feature that allows for self-management of specific cluster-wide resources required for certain popular applications. It is disabled by default, so please ask your Platform Administrator to enable this preview feature.</p> <p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket. This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>This page will help you to install MongoDB Community Operator, so that you are allowed to install the Cluster-wide resources that are required by MongoDB.</p> <p>This guide is a complement to the MongoDB Community Operator's own documentation.</p>","boost":2},{"location":"user-guide/self-managed-services/mongodb/#preparation","title":"Preparation","text":"<p>The self-managed Cluster-wide resources feature adds specific Roles, ServiceAccounts, etc. for you. This enables you to install and manage the resources that the MongoDB Community Operator needs. These pre-installed resources are propagated via HNC from your root namespace (recall the documentation of this feature).</p> <p>First create a new namespace using HNC, using the snippet below. If you do not know which root namespace you should use, ask your Platform Administrator.</p> <pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: mongodb\n  namespace: &lt;root namespace&gt;\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/mongodb/#install-mongodb-community-operator","title":"Install MongoDB Community Operator","text":"<p>Supported versions</p> <p>This installation guide has been tested with MongoDB Community Operator version 0.8.3.</p> <p>Please follow the official documentation for the MongoDB Community Operator. Be sure to read through the documentation fully.</p> <p>If default configuration choices are to your liking, you should be able to install the MongoDB Community Operator as follows:</p> <pre><code>helm repo add mongodb https://mongodb.github.io/helm-charts\nhelm upgrade --install community-operator mongodb/community-operator --namespace mongodb --version 0.8.3\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/mongodb/#further-reading","title":"Further Reading","text":"<p>Please refer to the official documentation how to operate and connect to MongoDB.</p> <ul> <li>Documentation</li> <li>Operator Documentation</li> <li>Deploy Configuration</li> <li>Secure MongoDB Resources</li> </ul>","boost":2},{"location":"user-guide/self-managed-services/sealedsecrets/","title":"SealedSecrets (self-managed)","text":"<p>Warning</p> <p>This page provides a starting point for installing an application. The information is useful, but not kept up-to-date. If you struggle, contact Elastisys Consulting.</p> <p>This page describes how to install a Customer Application</p> <p>You are solely responsible for Customer Applications.</p> <p>If you are an Elastisys Managed Services customer, please review your responsibilities in ToS 5.2.</p> <p>Specifically, you are responsible for performing due diligence for the project discussed in this page. At the very least, you must:</p> <ul> <li>assess project ownership, governance and licensing;</li> <li>assess project roadmap and future suitability;</li> <li>assess project compatibility with your use-case;</li> <li>assess business continuity, i.e., what will you do if the project is abandoned;</li> <li>subscribe to security advisories related to the project;</li> <li>apply security patches and updates, as needed;</li> <li>regularly test disaster recovery.</li> </ul> <p>This page is a preview for self-managed cluster-wide resources</p> <p>Welkin restricts Application Developers to manage CustomResourceDefinitions (CRDs) and other cluster-wide resources for security purposes. This means that some applications that require such cluster-wide resources are not possible to install for you as an Application Developer. As a trade-off, we are launching this preview feature that allows for self-management of specific cluster-wide resources required for certain popular applications. It is disabled by default, so please ask your Platform Administrator to enable this preview feature.</p> <p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket. This is a preview feature. For more information, please read ToS 9.1 Preview Features.</p> <p>This page will help you to install Sealed Secrets, so that you are allowed to install the Cluster-wide resources that are required by Sealed Secrets.</p> <p>This guide is a complement to Sealed Secrets own documentation.</p>","boost":2},{"location":"user-guide/self-managed-services/sealedsecrets/#preparation","title":"Preparation","text":"<p>The self-managed Cluster-wide resources feature adds specific Roles, ServiceAccounts, etc. for you. This enables you to install and manage the resources that Sealed Secrets needs. These pre-installed resources are propagated via HNC from your root Namespace (recall the documentation of this feature).</p> <p>First create a new namespace using HNC, using the snippet below. If you do not know which root namespace you should use, ask your Platform Administrator.</p> <pre><code>apiVersion: hnc.x-k8s.io/v1alpha2\nkind: SubnamespaceAnchor\nmetadata:\n  name: sealed-secrets\n  namespace: &lt;root namespace&gt;\n</code></pre>","boost":2},{"location":"user-guide/self-managed-services/sealedsecrets/#install-sealed-secrets","title":"Install Sealed Secrets","text":"<p>Supported versions</p> <p>This installation guide has been tested with Sealed Secrets version 0.24.2 and Helm Chart version 2.13.1.</p> <p>Sealed Secrets have a section in their documentation about installing Sealed Secrets into a restricted environment, where they give a <code>config.yaml</code> that defines what should be installed.</p> <p>The following <code>config.yaml</code> is an example of what is required to install Sealed Secrets into Welkin.</p> <pre><code>serviceAccount:\n  create: true\n  name: sealed-secrets\nrbac:\n  create: false\n  clusterRole: false\n## Add your namespace(s) here\nadditionalNamespaces: []\nresources:\n  requests:\n    cpu: 150m\n    memory: 256Mi\n  limits:\n    cpu: 150m\n    memory: 256Mi\n</code></pre> <p>Important</p> <p>Add the namespaces that should support creation of SealedSecrets to the <code>additionalNamespaces</code> list. If this list is empty the SealedSecrets controller will output an error when attempting to create a SealedSecret as it attempts to get Secrets at cluster level.</p> <p>You are now ready to install Sealed Secrets:</p> <pre><code>helm repo add sealed-secrets https://bitnami-labs.github.io/sealed-secrets\nhelm upgrade --install sealed-secrets -n sealed-secrets --version 2.13.1 sealed-secrets/sealed-secrets -f config.yaml\n</code></pre> <p>Note about <code>kubeseal</code></p> <p>The Sealed Secrets cli tool <code>kubeseal</code> expects the controller to be installed in the namespace <code>kube-system</code>. However the controller is installed in the namespace <code>sealed-secrets</code>. As such you need to follow this guide to use <code>kubeseal</code></p>","boost":2},{"location":"user-guide/self-managed-services/sealedsecrets/#further-reading","title":"Further Reading","text":"<p>Please refer to the official documentation how to operate and use Sealed Secrets.</p> <ul> <li>Documentation</li> <li>Cryptographic documentation</li> <li>SealedSecrets with Elastisys Managed Argo CD</li> </ul>","boost":2},{"location":"user-guide/self-managed-services/sjunet/","title":"Sjunet (self-managed)","text":"<p>For Welkin Managed Customers</p> <p>You can ask for this feature to be enabled by filing a service ticket.</p> <p>The Swedish Sjunet is a private network, managed by Inera, designed for healthcare systems with strict requirements. It is used by regions (regioner), municipalities (kommuner) and private actors with in the healthcare system. As the healthcare system has high requirements when it comes to distributing sensitive information. Sjunet as a private network, means they can validate that the strict requirements, such as high availability, stability and bandwidth, are meet. Also, that only authorized and audited users are allowed to use it.</p>","boost":2},{"location":"user-guide/self-managed-services/sjunet/#who-is-sjunet-for","title":"Who is Sjunet for?","text":"<p>Sjunet is for actors that are working with the Swedish healthcare system that require a high available, stable and secure network. Today there are over 100 services, such as Nationell patient\u00f6versikt, Intygstj\u00e4nster and F\u00f6delseanm\u00e4lan, using Sjunet to be reliably available for hospital and healthcare centers.</p>","boost":2},{"location":"user-guide/self-managed-services/sjunet/#how-welkin-connects-to-sjunet","title":"How Welkin connects to Sjunet","text":"<p>The architecture diagram below shows how Welkin connects to Sjunet and who is responsible for what.</p> <p>The Platform Administrator configures NodeLocalDNS to use Sjunet's DNS to resolve Sjunet domains. Together with static routes, network traffic heading to Sjunet is routed to a virtual machine acting as a gateway. The gateway will be co-located within the same security group as the Cluster to ensure that traffic between the Cluster and the gateway is secure and closed off from the outside.</p> <p>Inera, which is the administrators of Sjunet, require you to use a VPN when connecting to Sjunet over the public network.</p> <p>The Application Developer is responsible to install a supported VPN client on the VM acting as the gateway and connect it to Sjunet's VPN. Together with the static networking route within the Cluster, this means that traffic intended for Sjunet will be routed correctly via the gateway.</p> <p></p> <p> <code>Platform Administrators</code> area of responsibility  <code>Application Developers</code> area of responsibility  <code>Ineras</code> area of responsibility</p> <p>For Welkin Managed Customers</p> <p>Inform your Platform Administrator by filling a service ticket if additional IPs are required to be routed via the gateway machine. Or if additional security group ports, UDP/TCP, are required to be opened to allow traffic to flow in or out from the machine.</p>","boost":2},{"location":"user-guide/self-managed-services/sjunet/#further-reading","title":"Further Reading","text":"<ul> <li>Sjunet</li> <li>Order</li> <li>Regulatory Framework</li> <li>Technical Information</li> <li>VPN<ul> <li>WireGuard</li> <li>OpenVPN</li> </ul> </li> </ul>","boost":2},{"location":"ciso-guide/controls/bsi-it-grundschutz/","title":"BSI IT-Grundschutz Controls","text":"<p>The BSI IT-Grundschutz framework, developed by Germany\u2019s Federal Office for Information Security (BSI), provides a structured, modular approach to implementing information security management.</p> <p>Its \"building blocks\" (Bausteine) address specific components, processes, and technologies, offering concrete safeguards that can be tailored to different protection needs.</p> <p>These modules are grouped into thematic layers\u2014such as Applications (APP), Systems (SYS), and Networks (NET)\u2014and link security objectives with implementation guidance and verification steps, forming a cohesive, auditable framework.</p> <p>Within the Applications layer, APP.4.4 \u2013 Kubernetes focuses on securing container orchestration environments. Introduced in the 2022 edition of the IT-Grundschutz Compendium, this module addresses risks specific to Kubernetes Clusters, from configuration management and access control to backup and recovery. APP.4.4 complements SYS.1.6 (Containerisation) by translating general container security principles into Kubernetes-specific measures, ensuring that both operational practices and technical configurations meet robust, verifiable security standards.</p> <p>Important</p> <p>Many requirements in APP.4.4 cannot be fulfilled by an application platform alone, because they depend on factors outside the product's scope\u2014such as how Welkin is deployed, integrated, and operated in a specific environment, as well as how the application on top is developed and deployed. While a platform can provide features and guardrails (e.g., RBAC, audit logs) to support these controls, full compliance depends on correct configuration, secure surrounding infrastructure, and disciplined operational processes.</p> <p>That is why this documentation does not present an \"all green checkboxes\" compliance table for APP.4.4. Instead, it maps each relevant requirement to the parts of the product documentation that explain how Welkin can support or enable it. This approach allows platform administrators to combine Welkin's capabilities with their own environment-specific configurations, policies, and processes, ensuring a realistic and verifiable assessment rather than a misleading implication of complete, out-of-the-box compliance.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a1","title":"BSI IT-Grundschutz APP.4.4.A1","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a2","title":"BSI IT-Grundschutz APP.4.4.A2","text":"<ul> <li>External CI/CD Integration</li> <li>Argo\u2122 CD (preview)</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a3","title":"BSI IT-Grundschutz APP.4.4.A3","text":"<ul> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a5","title":"BSI IT-Grundschutz APP.4.4.A5","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a7","title":"BSI IT-Grundschutz APP.4.4.A7","text":"<ul> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a10","title":"BSI IT-Grundschutz APP.4.4.A10","text":"<ul> <li>External CI/CD Integration</li> <li>Argo\u2122 CD (preview)</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a13","title":"BSI IT-Grundschutz APP.4.4.A13","text":"<ul> <li>Standard Template for on-prem Environment</li> <li>Guardrails</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a14","title":"BSI IT-Grundschutz APP.4.4.A14","text":"<ul> <li>Use Dedicated Nodes for Additional Services</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a15","title":"BSI IT-Grundschutz APP.4.4.A15","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a16","title":"BSI IT-Grundschutz APP.4.4.A16","text":"<ul> <li>Additional Services</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a18","title":"BSI IT-Grundschutz APP.4.4.A18","text":"<ul> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-app44a21","title":"BSI IT-Grundschutz APP.4.4.A21","text":"<ul> <li>Maintaining and Upgrading your Welkin environment</li> <li>Prepare Your Application</li> </ul>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#other-it-grundschutz-controls","title":"Other IT-Grundschutz Controls","text":""},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a17-attestierung-von-nodes-h","title":"APP.4.4.A17 Attestierung von Nodes (H)","text":"<p>The Kubespray layer in Welkin ensures that Data Plane Nodes and Control Plane Nodes are mutually authenticated via mutual TLS.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#bsi-it-grundschutz-controls-outside-the-scope-of-welkin","title":"BSI IT-Grundschutz Controls outside the scope of Welkin","text":"<p>Pending official translation into English, the controls are written in German.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a6-initialisierung-von-pods-s","title":"APP.4.4.A6 Initialisierung von Pods (S)","text":"<p>Application Developers must make sure that initialization happens in init containers.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a11-uberwachung-der-container-s","title":"APP.4.4.A11 \u00dcberwachung der Container (S)","text":"<p>Application Developers must ensure that their application has a liveliness and readiness probe, which are configured in the Deployment. This is illustrated by our user demo.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a12-absicherung-der-infrastruktur-anwendungen-s","title":"APP.4.4.A12 Absicherung der Infrastruktur-Anwendungen (S)","text":"<p>This requirement essentially states that the Welkin environments are only as secure as the infrastructure around them. Make sure you have a proper IT policy in place. Regularly review the systems where you store backups and configuration of Welkin.</p>"},{"location":"ciso-guide/controls/bsi-it-grundschutz/#app44a20-verschlusselte-datenhaltung-bei-pods-h","title":"APP.4.4.A20 Verschl\u00fcsselte Datenhaltung bei Pods (H)","text":"<p>Welkin recommends disk encryption to be provided at the infrastructure level. If you have this requirement, check for full-disk encryption via the provider audit.</p>"},{"location":"ciso-guide/controls/cra/","title":"EU Cyber Resilience Act (CRA)","text":"<p>As explained by the European Commission:</p> <p>The Cyber Resilience Act (CRA) aims to safeguard consumers and businesses buying software or hardware products with a digital component. The Cyber Resilience Act addresses the inadequate level of cybersecurity in many products, and the lack of timely security updates for products and software. It also tackles the challenges consumers and businesses currently face when trying to determining which products are cybersecure and in setting them up securely. The new requirements will make it easier to take cybersecurity into account when selecting and using products that contain digital elements. It will be more straightforward to identify hardware and software products with the proper cybersecurity features.</p>"},{"location":"ciso-guide/controls/cra/#welkin-and-cra","title":"Welkin and CRA","text":"<p>As a containerized application platform, Welkin is a CRA Important Product with Digital Elements in Class II, because it falls under \"Hypervisors and container runtime systems that support virtualised execution of operating systems and similar environments\" (see Annex III CRA).</p> <p>As a result, Welkin needs to:</p> <ul> <li>provide certain information and instructions to the user (Annex II CRA);</li> <li>maintain technical documentation (Annex VII CRA) to, among others, demonstrate fulfilling essential cybersecurity requirements (Annex I CRA).</li> </ul> <p>The following sections provide documentation as needed to fulfill requirements.</p>"},{"location":"ciso-guide/controls/cra/#information-and-instructions-to-the-user-annex-ii-cra","title":"Information and Instructions to the User (Annex II CRA)","text":"<ol> <li>Manufacturer contact information: See Elastisys Contact.</li> </ol> <ol> <li>Where to report and receive vulnerabilities: See Reporting security issues.</li> </ol> <ol> <li>Name and type of any additional information enabling the unique identification of Welkin: Both the Grafana and OpenSearch Service Endpoints feature a welcome dashboards which show the version of Welkin you are currently running.</li> </ol> <ol> <li>Intended purpose of Welkin: See Mission and Vision.</li> </ol> <ol> <li>Foreseeable misuse, which may lead to significant cybersecurity risks:<ul> <li>Welkin is a complex product and requires skilled people. See:<ul> <li>Application Developers: Understand the Basics;</li> <li>Platform Administrator: Understand the Basics;</li> <li>Platform Administrator: Understand Welkin.</li> </ul> </li> <li>Welkin needs a compliant and secure infrastructure. See:<ul> <li>Infrastructure Requirements;</li> <li>Provider Audit.</li> </ul> </li> <li>Welkin needs an application prepared to run on a containerized platform. See:<ul> <li>Prepare Your Application.</li> </ul> </li> <li>Welkin needs a correctly configured Identity Provider. See:<ul> <li>Prepare Your Identify Provider.</li> </ul> </li> </ul> </li> </ol> <ol> <li>EU Declaration of Conformity: We are waiting for the European Commission to publish a list of notified bodies, as laid out in Article 44 CRA.</li> </ol> <ol> <li>Type of technical security support:<ul> <li>Customers may migrate from one minor version of Welkin to the immediately next one, unless otherwise noted.</li> <li>A minor Welkin version receives security support, until all Elastisys customers have stopped using that version.</li> <li>For more information, see:<ul> <li>Maintenance;</li> <li>Self-Managed Welkin.</li> </ul> </li> </ul> </li> </ol> <ol> <li>Detailed instructions and information on:<ul> <li>(a) the necessary measures during initial commissioning and throughout the lifetime of the product with digital elements to ensure its secure use:     See point 5 above.</li> <li>(b) how changes to the product with digital elements can affect the security of data:     Welkin is designed to be secure-by-default.     Among others it includes guardrails to make it hard to do things which may reduce the security of data.     Such guardrails should only be disabled if the consequences are properly understood.</li> <li>(c) how security-relevant updates can be installed:     See Maintenance.</li> <li>(d) the secure decommissioning of the product with digital elements, including information on how user data can be securely removed:     User data is fully removed if the VMs, block storage volumes and object storage buckets are removed.     Note that configuration data may still persist in your git repository.     For details, see Architecture and Understand Welkin.</li> <li>(e) how the default setting enabling the automatic installation of security updates can be turned off: Two components deal with automatic installation of security updates: Kured and Tekton. Both are turned off by default. See Configuration Reference.</li> <li>(f) where the product with digital elements is intended for integration into other products with digital elements, the information necessary for the integrator to comply with the essential cybersecurity requirements set out in Annex I and the documentation requirements set out in Annex VII: See Infrastructure Requirements, Provider Audit and Prepare Identify Provider.</li> </ul> </li> </ol> <ol> <li>Software bill of materials:     An older version can be found here.     If you need a newer version, please contact Elastisys.</li> </ol>"},{"location":"ciso-guide/controls/cra/#technical-documentation-annex-vii-cra","title":"Technical Documentation (Annex VII CRA)","text":"<p>Elastisys maintains an internal document entitled \"Technical Documentation (Annex VII CRA)\", which lays out technical documentation, as required by Annex VII CRA. This document contains extensive evidence to demonstrate that Welkin complies with essential cybersecurity requirements (Annex I CRA).</p> <p>Feel free to get in touch with Elastisys and we'd be happy to walk you through it.</p>"},{"location":"ciso-guide/controls/cra/#further-reading","title":"Further Reading","text":"<ul> <li>EU Cyber Resilience Act (CRA)</li> </ul>"},{"location":"ciso-guide/controls/gdpr/","title":"GDPR (Regulation (EU) 2016/679)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Fully implementing GDPR entails a lot of work, like:</p> <ul> <li>Assigning a DPO;</li> <li>Documenting Records of Processing Activities;</li> <li>Writing Privacy Policies;</li> <li>Signing Data Protection Agreements with your suppliers.</li> </ul> <p>This page only points you to the GDPR concerns relevant for Welkin.</p> <p>If you process personal data in the EU/EEA, you need to follow GDPR.</p>","tags":["ISO 27001 Annex A 5.34 Privacy and Protection of PII"]},{"location":"ciso-guide/controls/gdpr/#gdpr-art-32-security-of-processing","title":"GDPR Art. 32 Security of Processing","text":"<p>When it comes to security, GDPR is rather broad and non-prescriptive. Pretty much everything we do in Welkin is done to secure data. This includes, for instance, that we perform vulnerability scanning both at rest and at runtime, process logs in a separate Cluster controlled with restrictive access controls to make them tamper-proof from hacked applications, and that we put guardrails in place to make developers enforce network segregation per application component. And much more. In fact, we could pretty much link every single page to GDPR Art. 32, but that would be rather noisy!</p> <p>Hence, if you need a more precise understanding on how Welkin protects personal data as required by GDPR Art. 32, please look at our ISO 27001 Controls, which links to both more technical controls, and continuous confidentiality, integrity, availability and resilience of processing processes, such as our go-live checklist.</p>","tags":["ISO 27001 Annex A 5.34 Privacy and Protection of PII"]},{"location":"ciso-guide/controls/gdpr/#gdpr-art-17-right-to-erasure-right-to-be-forgotten","title":"GDPR Art. 17 Right to erasure (\"right to be forgotten\")","text":"<ul> <li>How do I comply with GDPR Art. 17 Right to erasure (\"right to be forgotten\")?</li> <li>Backups</li> <li>Long-term log retention</li> </ul>","tags":["ISO 27001 Annex A 5.34 Privacy and Protection of PII"]},{"location":"ciso-guide/controls/gdpr/#gdpr-art-28-processor","title":"GDPR Art. 28 Processor","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>","tags":["ISO 27001 Annex A 5.34 Privacy and Protection of PII"]},{"location":"ciso-guide/controls/gdpr/#further-reading","title":"Further reading","text":"<ul> <li>What is personal data?</li> <li>Art. 28 GDPR Processor</li> <li>Art. 17 GDPR Right to erasure (\"right to be forgotten\")</li> <li>Art. 32 GDPR Security of processing</li> </ul>","tags":["ISO 27001 Annex A 5.34 Privacy and Protection of PII"]},{"location":"ciso-guide/controls/hipaa/","title":"HIPAA Controls","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s5-security-management-process-information-system-activity-review-164308a1iid","title":"HIPAA S5 - Security Management Process - Information System Activity Review - \u00a7 164.308(a)(1)(ii)(D)","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s12-information-access-management-isolating-healthcare-clearinghouse-functions-164308a4iia","title":"HIPAA S12 - Information Access Management - Isolating Healthcare Clearinghouse Functions - \u00a7 164.308(a)(4)(ii)(A)","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s13-information-access-management-access-authorization-164308a4iib","title":"HIPAA S13 - Information Access Management - Access Authorization - \u00a7 164.308(a)(4)(ii)(B)","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s14-information-access-management-access-establishment-and-modification-164308a4iic","title":"HIPAA S14 - Information Access Management - Access Establishment and Modification - \u00a7 164.308(a)(4)(ii)(C)","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s16-security-awareness-and-training-security-reminders-164308a5iia","title":"HIPAA S16 - Security Awareness and Training - Security Reminders - \u00a7 164.308(a)(5)(ii)(A)","text":"<ul> <li>Vulnerability Dashboard</li> <li>Maintaining and Upgrading your Welkin environment</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s17-security-awareness-training-and-tools-protection-from-malicious-software-164308a5iib","title":"HIPAA S17 - Security Awareness, Training, and Tools - Protection from Malicious Software - \u00a7 164.308(a)(5)(ii)(B)","text":"<ul> <li>Vulnerability Dashboard</li> <li>Harbor - private container registry</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s18-security-awareness-training-and-tools-log-in-monitoring-164308a5iic","title":"HIPAA S18 - Security Awareness, Training, and Tools - Log-in Monitoring - \u00a7 164.308(a)(5)(ii)(C)","text":"<ul> <li>Audit Logs</li> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s20-security-incident-procedures-164308a6","title":"HIPAA S20 - Security Incident Procedures - \u00a7 164.308(a)(6)","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s23-contingency-plan-data-backup-plan-164308a7iia","title":"HIPAA S23 - Contingency Plan - Data Backup Plan - \u00a7 164.308(a)(7)(ii)(A)","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s24-contingency-plan-disaster-recovery-plan-164308a7iib","title":"HIPAA S24 - Contingency Plan - Disaster Recovery Plan - \u00a7 164.308(a)(7)(ii)(B)","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s26-contingency-plan-testing-and-revision-procedure-164308a7iid","title":"HIPAA S26 - Contingency Plan - Testing and Revision Procedure - \u00a7 164.308(a)(7)(ii)(D)","text":"<ul> <li>Go-live Checklist</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s29-business-associate-contracts-and-other-arrangements-164308b1","title":"HIPAA S29 - Business Associate Contracts and Other Arrangements - \u00a7 164.308(b)(1)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s31-facility-access-controls-164310a1","title":"HIPAA S31 - Facility Access Controls - \u00a7 164.310(a)(1)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s32-facility-access-controls-contingency-operations-164310a2i","title":"HIPAA S32 - Facility Access Controls - Contingency Operations - \u00a7 164.310(a)(2)(i)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s33-facility-access-controls-facility-security-plan-164310a2ii","title":"HIPAA S33 - Facility Access Controls - Facility Security Plan - \u00a7 164.310(a)(2)(ii)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s34-facility-access-controls-access-control-and-validation-procedures-164310a2iii","title":"HIPAA S34 - Facility Access Controls - Access Control and Validation Procedures - \u00a7 164.310(a)(2)(iii)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s35-facility-access-controls-maintain-maintenance-records-164310a2iv","title":"HIPAA S35 - Facility Access Controls - Maintain Maintenance Records - \u00a7 164.310(a)(2)(iv)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s39-device-and-media-controls-disposal-164310d2i","title":"HIPAA S39 - Device and Media Controls - Disposal - \u00a7 164.310(d)(2)(i)","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s43-access-control-164312a1","title":"HIPAA S43 - Access Control - \u00a7 164.312(a)(1)","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s44-access-control-unique-user-identification-164312a2i","title":"HIPAA S44 - Access Control - Unique User Identification - \u00a7 164.312(a)(2)(i)","text":"<ul> <li>Access control</li> <li>Use of Credentials</li> <li>External CI/CD Integration</li> <li>How to Delegate?</li> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s45-access-control-emergency-access-procedure-164312a2ii","title":"HIPAA S45 - Access Control - Emergency Access Procedure - \u00a7 164.312(a)(2)(ii)","text":"<ul> <li>Break-glass</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s47-access-control-encryption-and-decryption-164312a2iv","title":"HIPAA S47 - Access Control - Encryption and Decryption - \u00a7 164.312(a)(2)(iv)","text":"<ul> <li>Use of Cryptography</li> <li>Infrastructure Provider Audit</li> <li>Application Developer FAQ</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s48-audit-controls-164312b","title":"HIPAA S48 - Audit Controls - \u00a7 164.312(b)","text":"<ul> <li>Audit Logs</li> <li>Long-term log retention</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s52-transmission-164312e1","title":"HIPAA S52 - Transmission - \u00a7 164.312(e)(1)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s53-transmission-security-integrity-controls-164312e2i","title":"HIPAA S53 - Transmission Security - Integrity Controls - \u00a7 164.312(e)(2)(i)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#hipaa-s54-transmission-security-encryption-164312e2ii","title":"HIPAA S54 - Transmission Security - Encryption - \u00a7 164.312(e)(2)(ii)","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hipaa/#other-hipaa-controls","title":"Other HIPAA Controls","text":"<p>HIPAA controls are taken from these documents:</p> <ul> <li>HIPAA Security Series - Security Standards: Administrative Safeguards</li> <li>HIPAA Security Series - Security Standards: Physical Safeguards</li> <li>HIPAA Security Series - Security Standards: Technical Safeguards</li> </ul> <p>The following controls are outside the scope of Welkin and need to be implemented by the organization operating Welkin. ISO-27001-certified Welkin operators, such as Elastisys already have the right processes in place.</p> <ul> <li>S1 - Security Management Process - \u00a7 164.308(a)(1)</li> <li>S2 - Security Management Process - Risk Analysis - \u00a7 164.308(a)(1)(ii)(A)</li> <li>S3 - Security Management Process - Risk Management - \u00a7 164.308(a)(1)(ii)(B)</li> <li>S4 - Security Management Process - Sanction Policy - \u00a7 164.308(a)(1)(ii)(C)</li> <li>S6 - Assigned Security Responsibility - \u00a7 164.308(a)(2)</li> <li>S7 - Workforce Security - \u00a7 164.308(a)(3)</li> <li>S8 - Workforce security - Authorization and/or Supervision - \u00a7 164.308(a)(3)(ii)(A)</li> <li>S9 - Workforce security - Workforce Clearance Procedure - \u00a7 164.308(a)(3)(ii)(B)</li> <li>S10 - Workforce security - Establish Termination Procedures - \u00a7 164.308(a)(3)(ii)(C)</li> <li>S11 - Information Access Management - \u00a7 164.308(a)(4)</li> <li>S15 - Security Awareness and Training - \u00a7 164.308(a)(5)</li> <li>S19 - Security Awareness, Training, and Tools - Password Management - \u00a7 164.308(a)(5)(ii)(D)</li> <li>S21 - Security Incident Procedures - Response and Reporting - \u00a7 164.308(a)(6)</li> <li>S22 - Contingency Plan - \u00a7 164.308(a)(7)</li> <li>S25 - Contingency Plan - Emergency Mode Operation Plan - \u00a7 164.308(a)(7)(ii)(C)</li> <li>S27 - Contingency Plan - Application and Data Criticality Analysis - \u00a7 164.308(a)(7)(ii)(E)</li> <li>S28 - Evaluation - \u00a7 164.308(a)(8)</li> <li>S30 - Business Associate Contracts and Other Arrangements - Written Contract or Other Arrangement - \u00a7 164.308(b)(4)</li> <li>S36 - Workstation Use - \u00a7 164.310(b)</li> <li>S37 - Workstation Security - \u00a7 164.310(c)</li> <li>S38 - Device and Media Controls - \u00a7 164.310(d)(1)</li> <li>S40 - Device and Media Controls - Media Re-use - \u00a7 164.310(d)(2)(ii)</li> <li>S41 - Device and Media Controls - Accountability - \u00a7 164.310(d)(2)(iii)</li> <li>S42 - Device and Media Controls - Data Backup and Storage Procedures - \u00a7 164.310(d)(2)(iv)</li> <li>S46 - Access Control - Automatic Logoff - \u00a7 164.312(a)(2)(iii)</li> </ul> <p>!!!important</p> <pre><code>  Welkin API access is configured so as to require a new OpenID flow every 12 hours.\n</code></pre> <ul> <li>S49 - Integrity - \u00a7 164.312(c)(1)</li> <li>S50 - Integrity - Mechanism to Authenticate ePHI - \u00a7 164.312(c)(2)</li> <li>S51 - Person or Entity Authentication - \u00a7 164.312(d)</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/","title":"Swedish Patient Data Act (HSLF-FS 2016:40)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Welkin. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>If you are a Swedish healthcare provider, you likely process patient data. Patient data includes GDPR personal data and patient records. HSLF-FS 2016:40 recommends following ISO 27001.</p> <p>Please look at the ISO 27001 controls to understand how Welkin helps you keep patient data private and secure.</p>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-9-upphandling-och-utveckling","title":"HSLF-FS 2016:40 3 kap. 9 \u00a7 Upphandling och utveckling","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-10-upphandling-och-utveckling","title":"HSLF-FS 2016:40 3 kap. 10 \u00a7 Upphandling och utveckling","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-12-sakerhetskopiering","title":"HSLF-FS 2016:40 3 kap. 12 \u00a7 S\u00e4kerhetskopiering","text":"<ul> <li>Backup Dashboard</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-13-sakerhetskopiering","title":"HSLF-FS 2016:40 3 kap. 13 \u00a7 S\u00e4kerhetskopiering","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-14-fysiskt-skydd-av-informationssystem","title":"HSLF-FS 2016:40 3 kap. 14 \u00a7 Fysiskt skydd av informationssystem","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-3-kap-15-behandling-av-personuppgifter-i-oppna-nat","title":"HSLF-FS 2016:40 3 kap. 15 \u00a7 Behandling av personuppgifter i \u00f6ppna n\u00e4t","text":"<ul> <li>Cryptography Dashboard</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-2-styrning-av-behorigheter","title":"HSLF-FS 2016:40 4 kap. 2 \u00a7 Styrning av beh\u00f6righeter","text":"<ul> <li>Use of Credentials</li> <li>Kubernetes API</li> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-3-styrning-av-behorigheter","title":"HSLF-FS 2016:40 4 kap. 3 \u00a7 Styrning av beh\u00f6righeter","text":"<ul> <li>Access control</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#hslf-fs-201640-4-kap-9-kontroll-av-atkomst-till-uppgifter","title":"HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter","text":"<ul> <li>Audit Logs</li> <li>Log Review</li> <li>How do I comply with HSLF-FS 2016:40 4 kap. 9 \u00a7 Kontroll av \u00e5tkomst till uppgifter?</li> <li>Long-term log retention</li> </ul>"},{"location":"ciso-guide/controls/hslf-fs-201640/#further-reading","title":"Further reading","text":"<ul> <li>HSLF-FS 2016:40 Socialstyrelsens f\u00f6reskrifter och allm\u00e4nna r\u00e5d om journalf\u00f6ring och behandling av personuppgifter i h\u00e4lso- och sjukv\u00e5rden</li> </ul> <ul> <li>IMY Care providers' protection of patient information</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/","title":"ISO 27001:2022 Controls","text":"<p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Welkin. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>ISO/IEC 27001:2022 is the latest version of the international standard for Information Security Management Systems (ISMS). It provides a systematic approach to managing sensitive company information, ensuring its confidentiality, integrity, and availability.</p> <p>ISO/IEC 27001:2022 is structured around a risk-based approach, where organizations must identify and mitigate security risks through a set of well-defined controls. These controls are detailed in Annex A and includes 93 controls categorized into four key themes:</p> <ol> <li>Organizational Controls (37 controls) \u2013 Covering governance, policies, roles, and responsibilities, such as information security roles, supplier relationships, and threat intelligence.</li> <li>People Controls (8 controls) \u2013 Focused on human factors, including security awareness training, screening, and disciplinary processes.</li> <li>Physical Controls (14 controls) \u2013 Addressing physical security measures like access controls, equipment security, and environmental protections.</li> <li>Technological Controls (34 controls) \u2013 Covering cybersecurity measures such as encryption, identity management, and network security.</li> </ol> <p>Welkin can help your organization implement some of these control.</p> <p>Important</p> <p>Many ISO 27001:2022 controls apply to your organization. Being a product, Welkin cannot help you implement all of them. For example, \"Annex A 6 People Controls\" is something your HR department should be tasked with and is outside the scope of Welkin. Controls which are not mentioned below are outside the scope of Welkin as an application platform.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-51-policies-for-information-security","title":"ISO 27001 Annex A 5.1 Policies for Information Security","text":"<ul> <li>Mission and Vision</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-53-segregation-of-duties","title":"ISO 27001 Annex A 5.3 Segregation of Duties","text":"<ul> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-515-access-control","title":"ISO 27001 Annex A 5.15 Access Control","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-516-identity-management","title":"ISO 27001 Annex A 5.16 Identity Management","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-519-information-security-in-supplier-relationships","title":"ISO 27001 Annex A 5.19 Information Security in Supplier Relationships","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-520-addressing-information-security-within-supplier-agreements","title":"ISO 27001 Annex A 5.20 Addressing Information Security Within Supplier Agreements","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-521-managing-information-security-in-the-ict-supply-chain","title":"ISO 27001 Annex A 5.21 Managing Information Security in the ICT Supply Chain","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-524-information-security-incident-management-planning-and-preparation","title":"ISO 27001 Annex A 5.24 Information Security Incident Management Planning and Preparation","text":"<ul> <li>Troubleshooting for Platform Administrators</li> <li>Alerts via Alertmanager</li> <li>OpenSearch Alert</li> <li>Troubleshooting for Application Developers</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-530-ict-readiness-for-business-continuity","title":"ISO 27001 Annex A 5.30 ICT Readiness for Business Continuity","text":"<ul> <li>We believe in community-driven open source</li> <li>Disaster Recovery</li> <li>Go-live Checklist</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-532-intellectual-property-rights","title":"ISO 27001 Annex A 5.32 Intellectual Property Rights","text":"<ul> <li>CISO FAQ</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-534-privacy-and-protection-of-pii","title":"ISO 27001 Annex A 5.34 Privacy and Protection of PII","text":"<ul> <li>GDPR (Regulation (EU) 2016/679)</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-536-compliance-with-policies-rules-and-standards-for-information-security","title":"ISO 27001 Annex A 5.36 Compliance With Policies, Rules and Standards for Information Security","text":"<ul> <li>Policy-as-Code Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-7-physical-controls","title":"ISO 27001 Annex A 7 Physical Controls","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-82-privileged-access-rights","title":"ISO 27001 Annex A 8.2 Privileged Access Rights","text":"<ul> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-85-secure-authentication","title":"ISO 27001 Annex A 8.5 Secure Authentication","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-86-capacity-management","title":"ISO 27001 Annex A 8.6 Capacity Management","text":"<ul> <li>Capacity Management (Kubernetes Status) Dashboard</li> <li>Capacity Management</li> <li>Cluster API</li> <li>Enforce Resources</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-87-protection-against-malware","title":"ISO 27001 Annex A 8.7 Protection Against Malware","text":"<ul> <li>Intrusion Detection Dashboard</li> <li>Enforce Trusted Registries</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-88-management-of-technical-vulnerabilities","title":"ISO 27001 Annex A 8.8 Management of Technical Vulnerabilities","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-89-configuration-management","title":"ISO 27001 Annex A 8.9 Configuration Management","text":"<ul> <li>Understand Welkin</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-810-information-deletion","title":"ISO 27001 Annex A 8.10 Information Deletion","text":"<ul> <li>How do I comply with GDPR Art. 17 Right to erasure (\"right to be forgotten\")?</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-812-data-leakage-prevention","title":"ISO 27001 Annex A 8.12 Data Leakage Prevention","text":"<ul> <li>Network Security Dashboard</li> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-813-information-backup","title":"ISO 27001 Annex A 8.13 Information Backup","text":"<ul> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-814-redundancy-of-information-processing-facilities","title":"ISO 27001 Annex A 8.14 Redundancy of Information Processing Facilities","text":"<ul> <li>Default Pod Topology Spread Constraints</li> <li>Avoid Downtime with Replicas</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-815-logging","title":"ISO 27001 Annex A 8.15 Logging","text":"<ul> <li>Logging</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-816-monitoring-activities","title":"ISO 27001 Annex A 8.16 Monitoring Activities","text":"<ul> <li>Audit Logs</li> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-817-clock-synchronization","title":"ISO 27001 Annex A 8.17 Clock Synchronization","text":"<ul> <li>Clock Synchronization</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-818-use-of-privileged-utility-programs","title":"ISO 27001 Annex A 8.18 Use of Privileged Utility Programs","text":"<ul> <li>Reduce blast radius: Preventing forgotten roots</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-820-networks-security","title":"ISO 27001 Annex A 8.20 Networks Security","text":"<ul> <li>Network Security Dashboard</li> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-821-security-of-network-services","title":"ISO 27001 Annex A 8.21 Security of Network Services","text":"<ul> <li>Network Security Dashboard</li> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-822-segregation-of-networks","title":"ISO 27001 Annex A 8.22 Segregation of Networks","text":"<ul> <li>Network Security Dashboard</li> <li>Network Model</li> <li>Reduce blast radius: NetworkPolicies</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-823-web-filtering","title":"ISO 27001 Annex A 8.23 Web filtering","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-824-use-of-cryptography","title":"ISO 27001 Annex A 8.24 Use of Cryptography","text":"<ul> <li>Cryptography Dashboard</li> <li>Use of Cryptography</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-825-secure-development-life-cycle","title":"ISO 27001 Annex A 8.25 Secure Development Life Cycle","text":"<ul> <li>Contributor guide</li> <li>Argo\u2122 CD (preview)</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-827-secure-system-architecture-and-engineering-principles","title":"ISO 27001 Annex A 8.27 Secure System Architecture and Engineering Principles","text":"<ul> <li>Mission and Vision</li> <li>Architectural Decision Log</li> <li>Namespaces</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-829-security-testing-in-development-and-acceptance","title":"ISO 27001 Annex A 8.29 Security Testing in Development and Acceptance","text":"<ul> <li>Quality Criteria</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-831-separation-of-development-test-and-production-environments","title":"ISO 27001 Annex A 8.31 Separation of Development, Test and Production Environments","text":"<ul> <li>How many environments?</li> <li>Namespaces</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-832-change-management","title":"ISO 27001 Annex A 8.32 Change Management","text":"<ul> <li>Understand Welkin</li> <li>Argo\u2122 CD (preview)</li> <li>Avoid unexpected changes: disallowed tags</li> </ul>"},{"location":"ciso-guide/controls/iso-27001/#iso-27001-annex-a-537-documented-operating-procedures","title":"ISO 27001 Annex A 5.37 Documented Operating Procedures","text":"<p>The whole Welkin documentation contributes to this control.</p>"},{"location":"ciso-guide/controls/kritis/","title":"KRITIS","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>KRITIS is the German transposition into law of the EU NIS/NIS2 directive. It essentially gives power to the German Federal Office for Information Security (Bundesamt f\u00fcr Sicherheit in der Informationstechnik -- BSI) to regulate critical entities.</p> <p>To learn more about how Welkin can help you implement KRITIS, please see BSI IT-Grundschutz.</p>"},{"location":"ciso-guide/controls/kritis/#further-reading","title":"Further Reading","text":"<ul> <li>BSI: General information on KRITIS</li> </ul>"},{"location":"ciso-guide/controls/mdr/","title":"MDR (Regulation (EU) 2017/745)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>CE certification of a medical device according to the EU MDR can lead a huge commercial benefit, but it is a significant project. To start the certification process thorough knowledge of the regulation is required.</p> <p>This page only points you to the MDR concerns relevant for Welkin.</p> <p>If you place or make a medical device available, or put them into service, on the European market, then you must comply with the Medical Device Regulation (MDR).</p> <p>As of 2023, there is at least one Medical Device Software running on Welkin that is CE certified according to MDR class IIa.</p>"},{"location":"ciso-guide/controls/mdr/#article-110-data-protection","title":"Article 110: Data protection","text":"<p>This article makes explicit reference to GDPR. See GDPR controls.</p>"},{"location":"ciso-guide/controls/mdr/#annex-i-general-safety-and-performance-requirements","title":"Annex I: General Safety and Performance Requirements","text":"<p>This annex makes reference to information security, for example in 17.2. You might want to check ISO 27001 controls, since that is one of the most recognized information security standards.</p>"},{"location":"ciso-guide/controls/mdr/#annex-vi-udi-related","title":"Annex VI: UDI-related","text":"<p>This annex makes explicit reference to change management, for example in 6.5.2 and 6.5.3.</p> <p>See how many environments to reduce the risk associated with updating the Welkin environments hosting your software medical device. While rather unlikely, you really want to make sure that your software medical device preserves its original performance with the new version of Kubernetes.</p>"},{"location":"ciso-guide/controls/mdr/#mdr-annex-vi-udi-related","title":"MDR Annex VI UDI-related","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/mdr/#further-reading","title":"Further reading","text":"<ul> <li>Regulation (EU) 2017/745 on Medical Devices</li> </ul> <ul> <li>Medicintekniska produkter on IVO</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20188/","title":"MSBFS 2018:8 Controls","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>The EU NIS2 directive defines critical entities, i.e., IT entities which are really important to society. If you are a critical entity or a supplier to critical entities, then you need to comply with the NIS2 directive.</p> <p>The EU NIS2 directive is implemented in Sweden by a number of regulations issued by the Swedish Civil Contingencies Agency (Myndigheten f\u00f6r samh\u00e4llsskydd och beredskap -- MSB).</p> <p>Long story short, you need to implement ISO 27001. See the ISO 27001 Control page to learn more about how Welkin helps.</p>"},{"location":"ciso-guide/controls/msbfs-20188/#further-reading","title":"Further Reading","text":"<ul> <li>Informationss\u00e4kerhet f\u00f6r NIS-leverant\u00f6rer</li> <li>MSBFS 2018:8 f\u00f6reskrifter och allm\u00e4nna r\u00e5d om informationss\u00e4kerhet f\u00f6r leverant\u00f6rer av samh\u00e4llsviktiga tj\u00e4nster</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/","title":"MSBFS 2020:7 Controls","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>Note</p> <p>Controls not covered below are controls which cannot be fulfilled by Welkin. These include requirements such as:</p> <ul> <li>Your management team needs to regularly perform various risk analysis.</li> <li>You need to do background checks when recruiting.</li> <li>You need to activate multi-factor authentication in your Identity Provider.</li> <li>You need to have a policy on how to safely use USB sticks.</li> <li>Requirements which fall under the scope of the application.</li> </ul> <p>If you are a Swedish government agency or a supplier you likely need to comply with MSBFS 2020:7.</p> <p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-2-kap-4","title":"MSBFS 2020:7 2 kap. 4 \u00a7","text":"<ul> <li>Architecture</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-3-kap-1","title":"MSBFS 2020:7 3 kap. 1 \u00a7","text":"<ul> <li>Infrastructure Provider Audit</li> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-3-kap-2","title":"MSBFS 2020:7 3 kap. 2 \u00a7","text":"<ul> <li>Infrastructure Provider Audit</li> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-1","title":"MSBFS 2020:7 4 kap. 1 \u00a7","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-2","title":"MSBFS 2020:7 4 kap. 2 \u00a7","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-3","title":"MSBFS 2020:7 4 kap. 3 \u00a7","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-4","title":"MSBFS 2020:7 4 kap. 4 \u00a7","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-5","title":"MSBFS 2020:7 4 kap. 5 \u00a7","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-7","title":"MSBFS 2020:7 4 kap. 7 \u00a7","text":"<ul> <li>Application Developer FAQ</li> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-9","title":"MSBFS 2020:7 4 kap. 9 \u00a7","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-12","title":"MSBFS 2020:7 4 kap. 12 \u00a7","text":"<ul> <li>Maintaining and Upgrading your Welkin environment</li> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-13","title":"MSBFS 2020:7 4 kap. 13 \u00a7","text":"<ul> <li>Clock Synchronization</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-14","title":"MSBFS 2020:7 4 kap. 14 \u00a7","text":"<ul> <li>Backup Dashboard</li> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-15","title":"MSBFS 2020:7 4 kap. 15 \u00a7","text":"<ul> <li>Backup Dashboard</li> <li>Disaster Recovery</li> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-16","title":"MSBFS 2020:7 4 kap. 16 \u00a7","text":"<ul> <li>Audit Logs</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-17","title":"MSBFS 2020:7 4 kap. 17 \u00a7","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-18","title":"MSBFS 2020:7 4 kap. 18 \u00a7","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-20","title":"MSBFS 2020:7 4 kap. 20 \u00a7","text":"<ul> <li>Vulnerability Dashboard</li> <li>Harbor - private container registry</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-21","title":"MSBFS 2020:7 4 kap. 21 \u00a7","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#msbfs-20207-4-kap-22","title":"MSBFS 2020:7 4 kap. 22 \u00a7","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/msbfs-20207/#further-reading","title":"Further Reading","text":"<ul> <li>Myndigheten f\u00f6r samh\u00e4llsskydd och beredskaps f\u00f6reskrifter om s\u00e4kerhets\u00e5tg\u00e4rder i informationssystem f\u00f6r statliga myndigheter</li> </ul>"},{"location":"ciso-guide/controls/nis2/","title":"NIS2 Overview","text":""},{"location":"ciso-guide/controls/nis2/#network-and-information-security-directive-2-nis2","title":"Network and Information Security Directive 2 (NIS2)","text":"<p>We are not lawyers, this is not legal advise.</p> <p>It is your responsibility to discover what law applies to you and how to best comply with it. In case of doubt, consult your Data Protection Officer (DPO) or equivalent.</p> <p>The NIS2 Directive stands as a comprehensive EU-wide cybersecurity legislation, aimed at elevating the overall state of cybersecurity across the European Union. Imposing legal measures, it serves to fortify the digital landscape in the region.</p> <p>Initiated in 2016, the EU's cybersecurity regulations underwent a substantial transformation with the enactment of the NIS2 Directive in 2023. This update was imperative to adapt to the expanding realm of digitization and the continuously evolving cybersecurity threats. The directive's enhancements extend the applicability of cybersecurity regulations to novel sectors and entities, thereby enhancing the resilience and response capabilities of public and private bodies, competent authorities, and the entire EU.</p> <p>The NIS2 Directive, officially titled the Directive on measures for a high common level of cybersecurity across the Union, imposes legal requisites to augment cybersecurity throughout the EU. Its key provisions encompass ensuring the preparedness of Member States, mandating the establishment of essential capabilities like a Computer Security Incident Response Team (CSIRT) and a competent national network and information systems (NIS) authority. Furthermore, it promotes cooperation among Member States through the establishment of a Cooperation Group, fostering strategic collaboration and information exchange.</p> <p>The directive seeks to instill a culture of security across critical sectors vital for the economy and society, heavily reliant on information and communication technologies (ICTs). These sectors include energy, transport, water, banking, financial market infrastructures, healthcare, and digital infrastructure.</p> <p>To uphold the directive's objectives, businesses identified by Member States as operators of essential services in the specified sectors must implement suitable security measures and promptly report significant incidents to relevant national authorities. Similarly, key digital service providers, such as search engines, cloud computing services, and online marketplaces, are obligated to adhere to the security and notification requirements outlined in the directive.</p> <p>The NIS2 Directive shares a strong connection with two additional initiatives: the Critical Entities Resilience (CER) Directive and the Regulation for Digital Operational Resilience in the Financial Sector, commonly known as the Digital Operational Resilience Act (DORA).</p>"},{"location":"ciso-guide/controls/nis2/#which-sectors-are-covered-by-the-nis2-directive","title":"Which sectors are covered by the NIS2 Directive?","text":"<p>A lot more sectors than in the previous iteration. Society has become more digital, and as a result, more vulnerable to cyberattacks. It is clear that many use-cases where Welkin has been successfully used in the past are in scope for NIS2, including sectors of high criticality, healthcare, banking and the financial market, and general public administration.</p> <p>The official FAQ lists the sectors in scope as follows:</p> <p>Sectors of high criticality: energy (electricity, district heating and cooling, oil, gas and hydrogen); transport (air, rail, water and road); banking; financial market infrastructures; health including manufacture of pharmaceutical products including vaccines; drinking water; waste water; digital infrastructure (internet exchange points; DNS service providers; TLD name registries; cloud computing service providers; data centre service providers; content delivery networks; trust service providers; providers of public electronic communications networks and publicly available electronic communications services); ICT service management (managed service providers and managed security service providers), public administration and space.</p> <p>Other critical sectors: postal and courier services; waste management; chemicals; food; manufacturing of medical devices, computers and electronics, machinery and equipment, motor vehicles, trailers and semi-trailers and other transport equipment; digital providers (online market places, online search engines, and social networking service platforms) and research organisations.</p>"},{"location":"ciso-guide/controls/nis2/#how-does-the-nis2-directive-relate-to-welkin","title":"How does the NIS2 Directive relate to Welkin?","text":"<p>NIS2 Article 21(2) lists 10 so-called minimum requirements. These minimum requirements need to be translated into policies for your organization, which can then be technically implemented. Below is a list of pages, which help you translate such policies into implementation on top of Welkin.</p>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-b-incident-handling","title":"NIS2 Minimum Requirement (b) Incident Handling","text":"<ul> <li>Troubleshooting for Platform Administrators</li> <li>Alerts via Alertmanager</li> <li>OpenSearch Alert</li> <li>Troubleshooting for Application Developers</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-c-backup-management","title":"NIS2 Minimum Requirement (c) Backup Management","text":"<ul> <li>Backups</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-c-disaster-recovery","title":"NIS2 Minimum Requirement (c) Disaster Recovery","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-d-security-of-direct-suppliers","title":"NIS2 Minimum Requirement (d) Security of direct suppliers","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-e-security-in-network","title":"NIS2 Minimum Requirement (e) Security in Network","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-e-vulnerability-handling","title":"NIS2 Minimum Requirement (e) Vulnerability Handling","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-h-cryptography","title":"NIS2 Minimum Requirement (h) Cryptography","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-h-encryption-in-transit","title":"NIS2 Minimum Requirement (h) Encryption-in-transit","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-i-access-control","title":"NIS2 Minimum Requirement (i) Access Control","text":"<ul> <li>Access control</li> <li>How to Delegate?</li> <li>Demarcation</li> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/nis2/#nis2-minimum-requirement-j-multi-factor-authentication","title":"NIS2 Minimum Requirement (j) Multi-Factor Authentication","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/nis2/#out-of-scope-nis2-requirements","title":"Out of Scope NIS2 Requirements","text":"<p>Note that, some requirements are out-of-scope for Welkin, as listed below:</p> NIS2 Minimum Requirement Justification for Exclusion (a) policies on risk analysis and information system security This is a requirement on the management team, which a container platform product, like Welkin, cannot fulfill. (f) policies and procedures to assess the effectiveness of cybersecurity risk-management measures This is a requirement on the management team, which a container platform product, like Welkin, cannot fulfill. (g) basic cyber hygiene practices and cybersecurity training Welkin is not a training solution. However, Elastisys can help. Check out our training."},{"location":"ciso-guide/controls/nis2/#country-and-sector-specific-requirements","title":"Country- and Sector-Specific Requirements","text":"<p>Please see the following pages, also linked in the side bar, for country- and sector-specific rules on top of the NIS2 minimum requirements. Note that these rules were enacted under NIS1, as NIS2 still needs to be implemented in some EU Member States:</p> <ul> <li>KRITIS (Germany)</li> <li>BSI IT Grundschutz (Germany)</li> <li>MSBFS 2018:8 (Sweden)</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/","title":"NIST SP 800-171 Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations","text":"<p>Important</p> <p>A software product -- such as Welkin -- cannot by itself be NIST SP 800-171 conform or certified. Instead, NIST SP 800-171 sets requirements on the organization and how it works with the software. Welkin can support fulfilling all NIST SP 800-171 requirements, provided that the organization has suitable policies and processes in place. For example, a tight integration needs to exist between onboard and offboarding personnel in HR and the Identity Provider which integrates with Welkin. Below we map NIST SP 800-171 requirements to Welkin features.</p> <p>Important</p> <p>This document was written based on NIST SP 800-171 Rev. 2. As of Jan 2024, Rev. 3 was in final public draft stage. Update: Rev. 3 is now available and we have created an issue to update this section accordingly when time permits.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#overview","title":"Overview","text":"Status Number of requirements % of all requirements Fully supported 54 49% Org-related 50 45% Application-related 5 5% Infra-related 1 1% Total 110 100%"},{"location":"ciso-guide/controls/nist-sp-800-171/#requirements","title":"Requirements","text":"<p>Click on the links below to navigate the documentation by control.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-311","title":"NIST SP 800-171 3.1.1","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-312","title":"NIST SP 800-171 3.1.2","text":"<ul> <li>How to Delegate?</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-314","title":"NIST SP 800-171 3.1.4","text":"<ul> <li>How to Delegate?</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-315","title":"NIST SP 800-171 3.1.5","text":"<ul> <li>How to Delegate?</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-316","title":"NIST SP 800-171 3.1.6","text":"<ul> <li>How to Delegate?</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-317","title":"NIST SP 800-171 3.1.7","text":"<ul> <li>Audit Logs</li> <li>Reduce blast radius: Preventing forgotten roots</li> <li>Reduce blast radius: Enforcing restricted privileges</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3111","title":"NIST SP 800-171 3.1.11","text":"<ul> <li>Architecture</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3113","title":"NIST SP 800-171 3.1.13","text":"<ul> <li>Kubernetes API</li> <li>Logging</li> <li>Metrics</li> <li>Harbor - private container registry</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3115","title":"NIST SP 800-171 3.1.15","text":"<ul> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3120","title":"NIST SP 800-171 3.1.20","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-331","title":"NIST SP 800-171 3.3.1","text":"<ul> <li>Audit Logs</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-332","title":"NIST SP 800-171 3.3.2","text":"<ul> <li>Prepare your Identity Provider (IdP)</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-333","title":"NIST SP 800-171 3.3.3","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-335","title":"NIST SP 800-171 3.3.5","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-336","title":"NIST SP 800-171 3.3.6","text":"<ul> <li>Log Review</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-337","title":"NIST SP 800-171 3.3.7","text":"<ul> <li>Clock Synchronization</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-344","title":"NIST SP 800-171 3.4.4","text":"<ul> <li>How many environments?</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-345","title":"NIST SP 800-171 3.4.5","text":"<ul> <li>Maintaining and Upgrading your Welkin environment</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-347","title":"NIST SP 800-171 3.4.7","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-348","title":"NIST SP 800-171 3.4.8","text":"<ul> <li>Enforce Trusted Registries</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-349","title":"NIST SP 800-171 3.4.9","text":"<ul> <li>Enforce Trusted Registries</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-351","title":"NIST SP 800-171 3.5.1","text":"<ul> <li>External CI/CD Integration</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-352","title":"NIST SP 800-171 3.5.2","text":"<ul> <li>External CI/CD Integration</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-354","title":"NIST SP 800-171 3.5.4","text":"<ul> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-355","title":"NIST SP 800-171 3.5.5","text":"<ul> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-356","title":"NIST SP 800-171 3.5.6","text":"<ul> <li>Kubernetes API</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-363","title":"NIST SP 800-171 3.6.3","text":"<ul> <li>Disaster Recovery</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-371","title":"NIST SP 800-171 3.7.1","text":"<ul> <li>Maintaining and Upgrading your Welkin environment</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-374","title":"NIST SP 800-171 3.7.4","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-375","title":"NIST SP 800-171 3.7.5","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3112","title":"NIST SP 800-171 3.11.2","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3123","title":"NIST SP 800-171 3.12.3","text":"<ul> <li>Logging</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3124","title":"NIST SP 800-171 3.12.4","text":"<ul> <li>Architecture</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3131","title":"NIST SP 800-171 3.13.1","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3132","title":"NIST SP 800-171 3.13.2","text":"<ul> <li>Architectural Decision Log</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3133","title":"NIST SP 800-171 3.13.3","text":"<ul> <li>Architectural Decision Log</li> <li>Use Dedicated Nodes for Additional Services</li> <li>Demarcation</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3136","title":"NIST SP 800-171 3.13.6","text":"<ul> <li>Network Model</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-31310","title":"NIST SP 800-171 3.13.10","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-31311","title":"NIST SP 800-171 3.13.11","text":"<ul> <li>Use of Cryptography</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-31316","title":"NIST SP 800-171 3.13.16","text":"<ul> <li>Infrastructure Provider Audit</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3144","title":"NIST SP 800-171 3.14.4","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3145","title":"NIST SP 800-171 3.14.5","text":"<ul> <li>Vulnerability Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3146","title":"NIST SP 800-171 3.14.6","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#nist-sp-800-171-3147","title":"NIST SP 800-171 3.14.7","text":"<ul> <li>Intrusion Detection Dashboard</li> </ul>"},{"location":"ciso-guide/controls/nist-sp-800-171/#notes-on-some-requirements","title":"Notes on Some Requirements","text":""},{"location":"ciso-guide/controls/nist-sp-800-171/#334","title":"3.3.4","text":"<p>Welkin alerts, e.g., if Fluentd is unable to deliver audit logs to OpenSearch.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#338-and-339","title":"3.3.8 and 3.3.9","text":"<p>Audit logs are stored in OpenSearch and are write-only.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#341-342-and-343","title":"3.4.1, 3.4.2 and 3.4.3","text":"<p>Welkin configuration is fully stored in Git and can benefits from Git merge requests, reviews, etc.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#3135","title":"3.13.5","text":"<p>All Welkin environments should run inside the organization's demilitarized zone (DMZ).</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#31315","title":"3.13.15","text":"<p>Welkin uses HTTPS for all its Service endpoints.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#31316","title":"3.13.16","text":"<p>Welkin recommends full-disk encryption at the infrastructure level.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#3141","title":"3.14.1","text":"<p>Please find relevant information in Elastisys ToS 3.6 Vulnerability Management.</p>"},{"location":"ciso-guide/controls/nist-sp-800-171/#further-reading","title":"Further Reading","text":"<ul> <li>NIST SP 800-171 Rev. 2 Protecting Controlled Unclassified Information in Nonfederal Systems and Organizations</li> </ul>"}]}